
Functions as a Service (FaaS) is an emerging and popular cloud computing model, where applications use cloud resources through user defined ``functions'' that execute application code~\cite{jonas2017occupy,jonas_cloud_2019,van_eyk_spec_2017}.
By handling all aspects of function execution, including resource allocation, cloud platforms can  provide a ``serverless'' computing model where users do not have to explicitly provision and manage cloud resources (i.e., virtualized servers).
FaaS employs a fine-grained pricing model allowing applications to pay for the resources they use, and provides many other advantages such as near-infinite horizontal scaling. 
FaaS services are being offered by all major cloud platforms (such as Amazon Lambda~\cite{aws-lambda}, Google Functions~\cite{google-functions}, and Azure Functions~\cite{azure-functions}), and are being used by diverse applications such as web services, API services,  parallel and scientific computing~\cite{funcx_hpdc_20,john_sweep_2019,fouladi_laptop_2019}, and in machine learning pipelines~\cite{carreira2018case, carreira_cirrus_2019}. 


The execution time of each function is typically short---in the range of a few milliseconds to a few seconds.
This tight latency requirement and the wide diversity in function characteristics raises new challenges for FaaS providers. 
Cloud platforms execute each function invocation in a virtualized execution environment such as a container or a virtual machine (VM). %, which allows functions to be executed in an independent and isolated manner.
Using virtualization techniques, a single physical server can execute many functions concurrently and safely~\cite{wang2018peeking}. 
Each function invocation entails creating and launching a container or a VM, and fetching and installing the necessary libraries and dependencies, before the function itself can be executed.
This ``initialization'' phase can take non-negligible time, and adds to the overall function execution latency observed by the user. 
Reducing this function ``startup overhead'' is a key challenge in serverless computing~\cite{oakes_sock_2018, hendrickson2016serverless, warm1, warm2}. 


To mitigate the startup overhead, a common technique is to keep the execution environment alive or ``warm'' for a small duration, so that future invocations of the same function can run in the already initialized environment. 
%do not incur the cold start overhead.
Keeping functions warm can reduce the cold start overheads and overall function latency by more than  $5\times$~\cite{manner_cold_2018}.  
%However, keeping the execution environment alive and running, instead of immediately terminating it, has some drawbacks. 
However, keeping a container or a VM alive consumes computing resources on the physical servers, and increases the resource requirements for hosting FaaS platforms. 
%, and reduces the  number of functions that can be concurrently executed. 
Thus, while keep-alive can reduce the effective function execution latency, it can also reduce the overall system utilization and efficiency.



\begin{comment}
In this paper, we explore this tradeoff, and argue for and develop principled keep-alive techniques. 
%
Current cloud platforms and FaaS systems employ simple keep-alive policies. 
%
For example, AWS Lambda will keep functions ``warm'' for $\sim 15-60$ minutes, and users have to resort to wasteful ``polling'' techniques to keep functions warm to avoid the cold start penalty~\cite{lambda-warm, lambda-limits, lambda-warm-hour}. 
%for the maximum duration of the function, which is currently 15 minutes~\cite{lambda-warm, lambda-limits, lambda-warm-hour}. 
% 
An optimized keep-alive policy must balance the overhead of keeping a function warm with the likelihood that the function will be called again in the near future, and the duration of keep-alive should depend on the function's characteristics. 
%
Designing keep-alive policies is challenging because of the diverse range of functions with different initialization overheads, resource footprints (i.e., CPU and memory consumption),  and request frequencies. 
%These simple keep-alive policies are not ideally suited for handling a diverse range of functions with different initialization overheads, resource footprints (i.e., CPU and memory consumption),  and request frequencies. 
%
\end{comment}


%
In this paper, we focus on how diverse FaaS workloads can be efficiently executed, by developing a new class of resource management techniques that balance the fundamental latency vs. utilization tradeoff.
We argue that keep-alive \emph{policies} can have a crucial impact on application performance, and thus must be integrated into resource allocation and provisioning. % of computing resources for serving FaaS workloads.
Current cloud platforms and FaaS systems employ simple keep-alive policies. 
For example, AWS Lambda will keep functions ``warm'' for $\sim 15-60$ minutes, and users have to resort to ad-hoc ``polling'' techniques to keep functions warm to avoid the cold start penalty~\cite{lambda-warm, lambda-limits, lambda-warm-hour}.
Similarly, OpenWhisk sets a constant time to live (10 minutes) for function containers.


However, optimized keep-alive policies must balance the overhead of keeping a function warm with the likelihood that it will be invoked in the near future. 
Ideally, the duration of keep-alive should depend on the function's characteristics. 
This is challenging because of the diverse range of functions with different initialization overheads, resource footprints (i.e., CPU and memory consumption),  and request frequencies. 


\emph{Our primary insight is that the resource management of functions is equivalent to object caching.}
%\emph{Our primary insight is that the problem of which function to keep alive/warm, is analogous to object caching.} 
Keeping a function warm is equivalent to caching an object, and a warm function execution is equivalent to a cache hit. 
Terminating a function's execution environment means that the next invocation will incur the cold start penalty, and is thus equivalent to evicting an object from a cache. 
The objective is to keep functions warm such that the effective function latency is reduced, which is equivalent to caching's goal of reducing object access time.  
\emph{By mapping keep-alive to the exhaustively studied field of caching, we can leverage its  principles and techniques, and apply them to serverless computing.} 


Specifically, we use and adapt the Greedy-Dual caching framework~\cite{gdsf}, and develop keep-alive policies based on it. 
Our policies are cognizant of the memory footprint, access frequency,  initialization cost, and execution latency of different functions. 
This caching-based approach can improve both the function latency and server utilization compared to the simple keep-alive policies found in current FaaS platforms. 


The caching analogy allows us to use the vast set of caching algorithms and  analytical models, and provides a new approach to resource provisioning for FaaS platforms.
We use hit-ratio curves to determine the ideal size of servers required for handling FaaS workloads, and develop a vertical auto-scaling approach that dynamically adapts server size based on the workload characteristics.
The dynamic scaling uses proportional control and hit-ratio curves to minimize both the required server resources, and cold start overheads. 
%These static and dynamic provisioning policies, when combined with our keep-alive policies, help reduce cold start overhead by more than 100\% compared to current approaches, and reduce the server resource requirements by more than 30\%.
%Not sure if result is required here.. 

The rise of serverless computing and the challenges posed by its heterogeneity, workload diversity, and latency requirements, will require a new class of approaches to FaaS resource management. 
We argue that the vast collection of algorithms, analytical models, practical optimizations, and hard lessons from one of the most well studied fields in computer science, caching, can be customized to address many of these challenges. 
While bespoke solutions to serverless resource management will continue to be developed, our intent is to show the equivalence of caching and FaaS, and to highlight how naturally and easily caching techniques can be adapted instead. 
This paper makes an initial exploration into the world of caching-based approaches for resource management in serverless computing, and makes the following contributions:

\begin{enumerate}
\item We show the equivalence between caching and function keep-alive, and develop a family of caching-based keep-alive policies for reducing  function cold start overhead. We use a Greedy-Dual based approach that is designed to work even with the diverse FaaS workloads. 

\item We implement our caching-based techniques in a our system, FaasCache, which is based on OpenWhisk. 
%We empirically evaluate our techniques on real-world FaaS workloads. 
  We conduct extensive trace-driven and empirical analysis of the tradeoffs of keep-alive techniques under different workload characteristics based on the Azure FaaS traces~\cite{shahrad_serverless_2020} and popular FaaS applications~\cite{kim_functionbench_2019}.
  
 
\item Our resource provisioning policies use hit-ratio curves to determine the ideal server configuration (such as memory size) required to handle different function workloads.
  Our proportional-control based dynamic vertical-scaling  can adjust server resources to reduce the cold start probability, and reduce the average server size by more than $30\%$. 

\item   Our experimental results indicate that caching-based keep-alive can reduce cold start overheads by $3\times$, 
  improve \\ application-latency by $6\times$,
  and reduce system load to serve $2\times$ more requests.
\end{enumerate}

%We implement our policy as part of a new FaaS framework based on OpenWhisk.
%
%We show that keep-alive can significantly improve function performance, especially for computationally heavy workloads such as deep-learning inference.
%
%We also propose a new ``two step'' function execution abstraction, where functions can be first explicitly initialized, which increases the effectiveness of keep-alive even more, since repetitive parts of the function execution can be reduced.








%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
