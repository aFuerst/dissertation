% LaTeX template for Artifact Evaluation V20201122
%
% Prepared by 
% * Grigori Fursin (cTuning foundation, France) 2014-2020
% * Bruce Childers (University of Pittsburgh, USA) 2014
%
% See examples of this Artifact Appendix in
%  * SC'17 paper: https://dl.acm.org/citation.cfm?id=3126948
%  * CGO'17 paper: https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf
%  * ACM ReQuEST-ASPLOS'18 paper: https://dl.acm.org/citation.cfm?doid=3229762.3229763
%
% (C)opyright 2014-2020
%
% CC BY 4.0 license
%

%\pagebreak[4]
% \clearpage
\section{Artifact Appendix}
\label{sec:artifact-appendix}
% \appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Artifact Check-List (Meta-Information)}

\begin{itemize}
  \item {\bf Program:} FaasCache
  \item {\bf Data set:}  see \ref{data-sets}
  \item {\bf Run-time environment:} Ubuntu 16.04.5
  \item {\bf Hardware:} 250 GB RAM, 48 cores
  \item {\bf Experiments:} Simulation \& OpenWhisk implementation
  \item {\bf How much disk space required (approximately)?} 10 GB
  \item {\bf How much time is needed to prepare workflow?}2 hours
  \item {\bf How much time is needed to complete experiments (approximately)?} 6 hours
  \item {\bf Publicly available?} Yes
  \item {\bf Archived (provide DOI)?} 10.5281/zenodo.4321766
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}

We have two main software artifacts.

The first is a discrete-event simulator for FaaS workloads written in Python.
This simulator implements various keep-alive policies, all of which are described in the full paper.
Its inputs are workload trace files that are publicly available at the Azure trace site, and serialized into a custom format by scripts in {\em code/split/gen}.
The simulator takes server memory size, keep-alive policy, input trace file as arguments, and outputs various statistics on warm and cold starts, memory usage, and other accounting information. 
These outputs are run through the data-graphing scripts in {\em code/split/plotting}, to produce figures such as Figure 5 in the paper.

The second artifact is a custom OpenWhisk (i.e., FaasCache).
It is a drop-in replacement of OpenWhisk with the same installation procedures.
It optimizes OpenWhisk scheduling with GD keep-alive policy.
FaasCache or vanilla OpenWhisk can both be used to generate the information from Table 1 in the paper.

Our artifact also includes some FaasCache performance tests in {\em faas-keepalive/code/wsk-actions/load-test/traces}.
These can be run with scripts in {\em faas-keepalive/code/wsk-actions/load-test} that invoke LookBusy work-simulating FaaS functions at regular intervals to load test the FaasCache design.

Full details on how to run these two artifacts are in Section \ref{experiments} below.
For brevity, portions talking about the simulator or OpenWhisk load generation will be referencing folder {\em faas-keepalive}.
Those sections on running the customized OpenWhisk server are based in folder {\em openwhisk-caching}.

\subsubsection{How to Access}

10.5281/zenodo.4321766

The code for both experiments is in the zenodo zip, but also available on GitHub. 

\noindent Simulator code is here: https://github.com/aFuerst/faascache-sim

\noindent FaasCache OpenWhisk implementation:

https://github.com/aFuerst/openwhisk-caching/

commit/38ff898d45da57726da38c00f735cb449e7f8595


\subsubsection{Hardware Dependencies}

To reproduce the FaasCache load test results, it is recommended to use sufficient RAM and CPU cores. We used 64 GB RAM, and 48 CPU cores. 
The simulator needs ~1 GB RAM per core, and is embarrassingly parallel and is mainly limited by total system memory. 
%will be made faster with higher number of parallel processes. 


\subsubsection{Software Dependencies}

\begin{enumerate}
  \item Python 3.7+
  \item Docker
  \item Java
\end{enumerate}


\subsubsection{Data Sets} \label{data-sets}


The original data is pulled from the dataset described in this markdown file:

\noindent https://github.com/Azure/AzurePublicDataset/

blob/master/AzureFunctionsDataset2019.md

The representative trace used in the paper is in the zip, called {\em 392-b.pckl}.
Pre-computed simulator results are also in the zip, in the folder {\em 392-pckls}. 
These are high-resolution results, memory-wise, as the simulation is compute-intensive (i.e. slow).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}

\subsubsection{Simulator Setup}


\noindent The original trace dataset can be found here:

https://github.com/Azure/AzurePublicDataset/

blob/master/AzureFunctionsDataset2019.md

\noindent The simulator code is available here, as well as in the zip: 

\noindent https://github.com/aFuerst/faascache-sim

The script {\em ./code/split/trace\_split\_funcs.py} will combine the first day's info into one file.
Edit {\em datapath} and {\em store} in the script to adjust input and output locations.


{\em ./code/split/gen\_representative\_trace.py} will create traces that are generally representative of the larger trace sample.
Edit {\em datapath} and {\em store} in the script to adjust input for trace CSVs and split pickles (made by {\em trace\_split\_funcs.py} above) respectively.
{\em save\_dir} points to the folder where the resulting combined trace(s) will be saved.
Change lines 131-133 if you want specific trace sizes.

{\em ./code/split/gen\_rare.py} will create traces using the rarest half and quarter of functions.
You can edit line 99 to adjust which quartiles it picks functions from.
	

\subsubsection{FaasCache OpenWhisk Setup}

FaasCache OpenWhisk implementation:

https://github.com/aFuerst/openwhisk-caching/

commit/38ff898d45da57726da38c00f735cb449e7f8595

\noindent Install the OpenWhisk CLI: 

https://github.com/apache/openwhisk\#quick-start

\noindent OpenWhisk testing code (a subset of FaasCache sim repo): 

https://github.com/aFuerst/faascache-sim/tree/master/code/wsk-actions


All the edits made to the OpenWhisk source for FaasCache are located in the file {\em core/containerpool/ContainerPool.scala}.

Build the dockerfile located at {\em code/wsk-actions/py/Dockerfile} with the name/tag alfuerst/wsk-py-pybuild.
This name is not required, but you will have to change the next script to use the name you pick.
{\em ./code/wsk-actions/py/build.sh} will create zip packages for all the actions that OpenWhisk can use.

To run the load tests on OpenWhisk you will also have to build the LookBusy Docker container in \\
{\em ./code/wsk-actions/load-test/lookbusy}.
Make sure the new Docker container is added as a runtime to the OpenWhisk {\em ansible/files/runtimes.json}.
The default AI container OpenWhisk uses still may be missing packages, if this is the case you will also have to build the Dockerfile at {\em wsk-actions/py/cust\_ai} and add it as a custom runtime.

Edit the {\em ./sample-app.conf} in the root and put it in {\em ./bin/}.
Rename it to {\em application.conf}.
OpenWhisk requires this configuration file to run and allow function creation.
You can adjust {\em container-pool.user-memory} depending on local resources.

{\em ./openwhisk-caching/blob/master/run.sh} will build and run the custom OpenWhisk.

Make sure the {\em .conf} {\em whisk.user} info matches between the scripts that talk to OpenWhisk.

{\em code/wsk-actions/load-test/wsk\_interact.py} contains helper functions that interact with the OpenWhisk CLI to set up functions and authentication.
If you use a different username or auth key then you will need to edit this file.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment Workflow} \label{experiments}

% Can you also make significant changes to the artifact appendix?
% Maybe see earlier ASPLOS and other papers with AE, and see what kind of content makes it into the paper.
% I read through it now and even I came away confused about what the simulator does and how to operate it.
% Needs a good overview, and maybe fixing the painful issues in AE reviews.
% The high-level output of each script and/or a *flow-chart* is needed (even if in text list form). 

\subsubsection{FaasCache Simulation}

The {\em ./code/run\_sim.sh} file will run a trace in the simulator and graph the results.

\begin{enumerate} [label=Step \arabic{enumi}.,ref=Step \arabic{enumi}, leftmargin=*]
  \item trace\_dir => folder where trace file needs to be
  \item trace\_output\_dir => folder where sim results will end up
  \item log\_dir => file where sim log data will end up
\end{enumerate}

The location of the trace output and log output {\bf must} be different.

Edit the number of functions in {\em ./code/run\_sim.sh} to match the number of functions in the trace you want to run (this number will be in the file name).
Make sure the trace file letter (-b-, etc.) matches line 65 of {\em ./code/split/many\_run.py}, this is set by the trace generation script.


%\paragraph{Detailed Breakdown}

\paragraph{Running the Simulator}

{\em many\_run.py} executes instances of the simulator (LambdaScheduler) in parallel and saves the results of each simulation to a pickle file.
The inputs and outputs of many\_run.py are documented in the file itself.
{\em LambdaScheduler.py} contains all the major simulator code, and the helper classes are located in:

\begin{enumerate} [label=Step \arabic{enumi}.,ref=Step \arabic{enumi}, leftmargin=*]
  \item {\em LambdaData.py} - represents a Serverless function, holding a name, memory size, and cold and warm runtime lengths
  \item {\em Container.py} - holds a LambdaData and tracks if it is warm or cold and when it was last accessed  
\end{enumerate}

Each function execution enters with {\em runActivation} which does the following (function names are emphasized)


\begin{enumerate} [label=Step \arabic{enumi}.,ref=Step \arabic{enumi}, leftmargin=*]
  \item {\em cleanup\_finished} - remove containers that have exceeded their TTL (if applicable)
  \item {\em PreWarmContainers} - prewarm containers when calculated by the histogram policy (if that policy is being simulated)
  \item {\em track\_activation} - book-keeping for the histogram policy (if that policy is being simulated)
  \item {\em find\_container} - search for a container that matches the function being invoked that currently isn't running another function
  \begin{enumerate}
    \item {\em cache\_miss}
    \begin{enumerate}
      \item create a new {\em Container} for the function
      \item evict low-priority containers to make room if necessary
      \item assign the function to it, and set the cold running time
    \end{enumerate}
    \item Cache hit - a requisite container was found, simply assigns the function to it and sets the warm running time
  \end{enumerate}
  \item {\em calc\_priority} - update the priority of the function that was just called and any functions of that type that are currently in-memory
\end{enumerate}

\paragraph{Simulation Analysis}

These two scripts compute the number of cold/warm starts, and the global increase in execution time.

{\em compute\_policy\_results.py}

{\em compute\_mem\_usage.py}

\paragraph{Plotting Results}

These two perform the plotting for figures 5 \& 6 respectively: {\em plot\_run\_across\_mem.py}, and {\em plot\_cold\_across\_mem.py}.

The folder contains several other plotting scripts for analyzing the simulation results, but we chose not include those plots in the paper.


\subsubsection{FaasCache OpenWhisk}

You can follow the items in run.sh to run individual actions or run 

{\em ./code/wsk-actions/load-test/testing/find\_avgs.py} to get average run times for all the different actions.

{\em ./code/wsk-actions/load-test/gen\_litmus.py} will generate the litmus test pckls for the full OpenWhisk tests.
Then run {\em code/wsk-actions/load-test/sub\_litmi.py} to invoke the litmus test.

Cold vs warm hit metrics are output to OpenWhisk log (stdout from sh/jar). 
Make sure to pipe the output to a file.
You can grep on {\em cold hits:} to look at current results.

If you run the any of the litmus test, stop the test after 2 hours. 
OpenWhisk may or may not complete all invoked actions, stopping it significantly late is ok, just wastes time.
Then grep for the first hit event, record the time, then grep for the event closest to 2 hours later.
This will match how the paper results were gathered.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and Expected Results}

Results should be similar to those in the paper. 

The simulator results will not be identical if a new trace sampling is used, and if a coarser grained memory step is used. 
We used 500 MB steps, but this dramatically increases needed simulation time. 

Timings and numbers for the FaasCache OpenWhisk implementation will vary marginally due to the stochastic nature of web request handling and the inner workings of OpenWhisk. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Methodology}

% Submission, reviewing and badging methodology:

% \begin{itemize}
%   \item \url{https://www.acm.org/publications/policies/artifact-review-badging}
%   \item \url{http://cTuning.org/ae/submission-20201122.html}
%   \item \url{http://cTuning.org/ae/reviewing-20201122.html}
% \end{itemize}