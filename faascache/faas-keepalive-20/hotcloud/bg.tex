%Background and related work. 
%background on serverless platforms and keep-alive

% This is an introductory paragraph talking about serverless computing and some context. Seems repetitive wrt introduction? 
Serverless computing is now being provided by all large public cloud providers: Amazon Lambda, Google Functions, and Azure Functions are becoming an increasingly popular way to deploy applications on the cloud.  
Functions as a Service (FaaS) can also be realized on private clouds and dedicated clusters through the use of frameworks such as OpenWhisk~\cite{openwhisk}, OpenFaas~\cite{openfaas}, OpenLambda~\cite{hendrickson2016serverless}, etc. 
%
In this new cloud paradigm, users provide functions in languages such as Python, Javascript, Go, and others. 
%
The functions are executed by the FaaS platform, greatly simplifying resource management for the application. 



% Need to explain how it all works. But first provide some context for why this is important.


%In order to provide FaaS, the way it is implemented by platforms, results in certain performance challenges.
However, the execution of FaaS functions entails performance overheads that we must be cognizant of. 
%
%FaaS functions cannot assume that state will persist across invocations, and functions need to be self contained in terms of their dependencies. 
%
FaaS functions cannot assume that state will persist across invocations, and function definitions must first import and load all code and data dependencies on each execution. 
%
Each functions is run inside a containers such as Docker~\cite{docker-main}, or a lightweight VM such as Firecracker~\cite{firecracker-nsdi20}. 
By encapsulating all of the function state and any side-effects, the virtual execution environment provides isolation among multiple functions, and also allows for concurrent invocations of the same function. 
%
Due to the overhead of starting a new virtual execution environment (i.e., container or VM), and initializing the function by importing libraries and other data dependencies, function execution thus incurs a significant ``cold start'' penalty. 
%
Thus, FaaS can result in significant performance (i.e., total function execution latency) overheads compared to conventional models of execution where applications can reuse state and do not face the high initialization overheads. 


Two main techniques are used to alleviate the cold start penalty. 
Once a container for a function is created and the function finishes execution, the container can be kept alive instead of immediately terminating it. 
Subsequent invocations of the function can then \emph{reuse} the already running container.
This \emph{keep-alive} mechanism can alleviate the cold start overhead due to container launching (which can be $\sim 100$ ms). %Might be confusing, keep-alive also helps in other initialization. 
The second technique for reducing the cold start overhead is to explicitly initialize functions before running them, and resolving most of the function's code and data dependencies during the initialization phase. 

An example of function initialization is shown in Figure~\ref{fig:lambda-example}, which shows a pseudo-code snippet of a function that performs machine learning inference on its input. 
For ML inference, the function downloads an ML model and initializes the TensorFlow ML framework (lines  5 and 6). 
If the function's container is kept alive, then invocations of the function do not need to run the expensive initialization code (lines 2--6). 
Thus, the execution latency of functions can be minimized with a combination of careful function  initialization and keeping the containers alive. 


% \footnotesize
\begin{figure}
\begin{lstlisting}[language=Python, numbers=left, frame=single, basicstyle=\sffamily, columns=fullflexible]
#Initialization code 
import numpy as np 
import tensorflow as tf
  
m = download_model('http://model_serve/img_classify.pb')
session = create_tensorflow_graph(m) 
  
def lambda_handler(event, context):
     #This is called on every function invocation 
     picture = event['data']
     prediction_output = run_inference_on_image(picture) 
     return prediction_output 
   \end{lstlisting}
   \vspace*{\myfigspace}
   \caption{Initializing functions by importing and downloading code and data dependencies can reduce function latency by hiding the cold start overhead.}
   \label{fig:lambda-example}
   \vspace*{\myfigspace}
\end{figure}


However, keep-alive is not a panacea for all FaaS latency problems. 
Keeping a container alive consumes valuable computing resources on the servers, and reduces the number of functions that can be executed concurrently. 
Specifically, a running container occupies memory, and ``warm'' containers being kept alive in anticipation of future function invocations can reduce the multiplexing and efficiency of the servers.

Thus, we require keep-alive \emph{policies} that reduce the cold start overhead while keeping the server utilization high. 
Designing keep-alive policies is not trivial due to the highly diverse and expanding range of applications that are using FaaS platforms. 
Conventionally, FaaS has been used for hosting web services, which is attractive because of the pay-per-use properties. 
Event handling functions for web responses typically have a small memory footprint but require low execution latency. 
On the other hand, FaaS is also being used for ``heavy'' workloads with high memory footprint and large initialization overheads such as highly parallel numerical computing (such as matrix operations~\cite{jonas2017occupy}, scientific computing~\cite{shankar2018numpywren}, and machine learning~\cite{akkus_sand_2018}.
%
The diversity of FaaS applications also results in a wide range of function memory footprints, running times, and initialization times, as seen in Table~\ref{tab:workloads}. 
%
Keep-alive policies must therefore balance the resource footprint of the containers with the benefits of keeping containers alive---and do so in manner that is applicable across a wide range of applications. 


\begin{table}
  \label{tab:workloads}
  \begin{tabular}{llll}
    \hline 
    Application & Mem size & Run time & Init. time \\
    \hline
    ML inference & 2 GB & 2 min & 1 min \\
    Video Encoding & 200 MB & 20 s & 200 ms \\
    Matrix Multiply & 80 MB & 770 ms & 110 ms \\
    Web-serving & 100 MB & 100 ms & 10 ms \\
%    Scientific computing & 500 MB & 1 min & 20 s \\

    \hline
  \end{tabular}
%  \vspace*{\myfigspace}
  \caption{FaaS workloads are highly diverse in their resource requirements and running times.} 
  \vspace*{\myfigspace}
  \vspace*{\myfigspace}
  \vspace*{\myfigspace}
\end{table}

% only pay per use. Idling periods are therefore not charged. 
% %
% Also parallel and Scientific computing, by scaling out and running 1000s of functions in parallel, especially useful for embarassingly parallel workloads such as video processing.
% %




%\textbf{END}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






% Sources of cold start overhead




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
