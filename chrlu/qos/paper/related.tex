\section{Related Work}

\noindent\textbf{FaaS Resource Management:}
Multiple recent projects have looked into ways for optimizing the resource management of serverless frameworks, including solutions that
reduce the overhead of the cloud functions~\cite{du2020catalyzer, firecracker-nsdi20, dukic2020photons, akkus_sand_2018, vhive-asplos21, carreira2021warm},
improve locality through keep-alive policies~\cite{Shahrad:ATC:2020:ServerlessInTheWild},
better caching algorithms for worker nodes~\cite{faascache-asplos21},
reducing function communication and startup costs~\cite{gunasekaran2020fifer, daw2021speedo, shen2021defuse}, among others. 
Our work leverages the caching-based Greedy-Dual policy~\cite{faascache-asplos21}, in a cluster with pools that support differentiated services for high- versuls low-priority functions.

\noindent\textbf{Latency-sensitive scheduling and load balancing in serverless:}
Nightcore~\cite{Jia:ASPLOS:2021:Nightcore} provides low end-to-end latency and variability using fast paths for internal calls, low-latency message channels, efficient threading and concurrency.
Atoll~\cite{Singhvi:SoCC:2021:Atoll} uses deadline-aware two-level scheduling as part of a low-latency serverless platform.
Our proposal can be added to those solutions to further implement differentiated services on top of more performant serverless platforms.
In general, there is a rich previous body of work in performance improvement methods~\cite{Oakes:ATC:2018:SOCK,Kaffes:SoCC:2019:Core-Granular,Hunhoff:WoSC:2020:freshen} that are complementary to our approach.
In addition, we extend the notion of locality-aware function routing~\cite{firecracker-nsdi20,package-cristina-19,Fuerst:2022:CH-RLU,Abdi:2023:EuroSys:Palette} and use it within cluster pools that ensure differentiated services when functions can be divided into priority classes. 

\noindent\textbf{Differentiated services in serverless:}
Sequoia~\cite{Tariq:SOCC:2020:Sequoia} is a serverless framework with a QoS scheduler based on a simple priority-based queue; however, the issue of starvation in the presence of a continuous arrival of high-priority functions is not considered.
Furthermore, the framework is based on a completely new design that does not support synchronous function calls.
In contrast, our solution was implemented on top of OpenWhisk and considers both synchronous and asynchronous functions.
Bilal et al.~\cite{Bilal:CoRR:2021:GreatFreedom} analyzed the trade-off space between performance and cost that arises from different CPU/RAM configurations and the resulting function performance.
This approach is orthogonal to ours and can be leveraged by the provider to offer differentiated services that span this configuration space.
Qiu et al.~\cite{Qiu:WOSC:2021:LatencyCritical} suggested that providers could implement resource over-commitment for FaaS workloads with loose latency objectives;
our approach ties over-commitment with current demand, with a dynamic mechanism that supports handling of bursts in high priority workloads at the expense of low priority ones.
\emph{Real-time serverless}~\cite{Nguyen:WoSC:2019:Real-Time} is a work-in-progress system that describes an interface for specifying invocation rate guarantees and proposes delivering them via admission control and predictive container management.

\noindent\textbf{Workload shifting in the cloud:}
Delaying of tasks has been proposed to make better use of renewable excess energy~\cite{Wiesner:Middleware:2021:TemporalShifting,Wiesner:EuroPar:2022:Cucumber}, to reduce
energy consumption for workflow execution~\cite{Versluis:HotCloudPerf:2022:TaskFlow}, among others.
While we have not implemented policies with energy management goals, our solution could be extended to consider energy information (peaks, variable costs, green energy availability).

\noindent\textbf{Multi-pool cluster scheduling:}
Virtual cluster pools have been used for dynamic resource management in datacenters~\cite{Chase:HPDC:2003:COD}, to handle increased workloads in data analytics clusters~\cite{Lee:HotCloud:2011:Heterogeneity-Aware}, for QoS multi-class admission control~\cite{Delimitrou:ICAC:2013:ARQ}, and to decrease the delays of scheduling decisions~\cite{Singhvi:SoCC:2021:Atoll}. 
We use this technique for service differentiation, and complement it with a novel control-based dynamic resizing mechanism to support burst absorption in  serverless workloads.
%Note: {Lee:HotCloud:2011:Heterogeneity-Aware} uses too pools: core nodes and accelerator nodes; the latter are temporarily added to the cluster to handle increased jobs (observed or anticipated).

%
% OLD, conference-paper related work
%
% \noindent \textbf{FaaS Resource Management.}
% The initialization overheads of serverless functions and their repeated invocations have spawned a great deal of research into optimizing their resource management.
% Recent surveys~\cite{faas-survey-jan-2022, raza2021sok, eismann2020serverless, hassan2021survey, mampage2021holistic} provide an overview of the challenges and solutions in this very active research area. 

% Reducing the overhead of serverless functions through various systems and virtualization-level mechanisms and  optimizations~\cite{du2020catalyzer, firecracker-nsdi20, dukic2020photons, akkus_sand_2018, vhive-asplos21, carreira2021warm}. 
% %
% Locality for FaaS resource management has been explored in the form of function keep-alive policies~\cite{shahrad_serverless_2020}. 
% Our work builds on and uses the caching-based Greedy-Dual policy from FaasCache~\cite{faascache-asplos21}. 
% %
% Single-server environments have been the focus of these mechanisms and policies: we have made an initial attempt to understand their interactions in a distributed cluster context.
% %
% Inter-function dependencies can also be used for predictive resource management and reducing function communication and startup costs~\cite{gunasekaran2020fifer, daw2021speedo, shen2021defuse}: incorporating these policies into our load-balancer is part of future work. 

% \noindent \textbf{Function Load Balancing:}
% Package-aware load balancing~\cite{package-cristina-19}  identifies and uses function code dependencies (software packages) as an important source of data locality.
% While this is an important factor, we focus on in-memory locality of kept-alive functions, since memory capacity is much smaller than permanent storage and caching functions in memory has a very large performance impact.
% %
% CPU contention and interference is a major source of performance bottlenecks for co-located functions, and adjusting CPU-shares using cgroups can provide significant benefits~\cite{suresh2019fnsched, suresh2021servermore, ensure-faas-acsos20}.
% %
% The load-locality tradeoff we explore is complementary to these CPU scheduling optimizations. 
% %
% The repetitive nature of functions and their workflows can also be used to improve resource utilization and latency~\cite{hunhoff2020proactive, yu2021faasrank, puru_xanadu_20, przybylski2021data}: our load-balancer is stateless for the sake of simplicity and can be enhanced with these techniques if necessary.


% The tradeoff between locality and performance has also been explored in the context of delay scheduling~\cite{zaharia2010delay} for data-parallel applications like MapReduce.
% Load-balancing is seen as a ``dispatch'' problem in queueing theory, and the FaaS cluster system most closely approximates G/G/PS, since the arrivals and service times are not markovian.
% Techniques such as ``join the shortest queue'', and ``least work left''~\cite{gupta2007analysis} have been shown to be effective.
% The online-greedy policy evaluated in the previous section closely approximates least-work-left.
% However, it is difficult to implement in practice since the running times of functions is hard to predict due to their volatile arrival distribution mixtures and high variances in running time due to various system interference effects.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
