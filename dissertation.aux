\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{v}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Abstract}{vi}{chapter*.1}\protected@file@percent }
\citation{shahrad_serverless_2020}
\@writefile{toc}{\contentsline {chapter}{Abbreviations}{xvi}{chapter*.6}\protected@file@percent }
\citation{xen}
\citation{docker-main}
\citation{kubernetes}
\@writefile{toc}{\contentsline {chapter}{\numberline {1.}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\newlabel{chap:introduction}{{1}{1}{Introduction}{chapter.1}{}}
\citation{lambda}
\citation{fuerst2020cloud}
\citation{fuerst2022memory}
\citation{wang2021smartharvest}
\citation{serverless-harvest-sosp21}
\citation{harvest-osdi20}
\citation{sahraei2023xfaas}
\citation{cicconetti2020decentralized}
\citation{russo2023serverless}
\citation{cheng2019fog}
\citation{wang2020supporting}
\citation{du2022serverless}
\citation{persson2017kappa}
\citation{trilles2020iot}
\citation{cheng2019fog}
\citation{wang2020supporting}
\citation{bacis2020blastfunction}
\citation{ringlein2021case}
\citation{sahraei2023xfaas}
\citation{funcx_hpdc_20}
\citation{lambda}
\citation{gcp-functions}
\citation{azure-functions}
\citation{alibaba-compute}
\citation{openwhisk}
\citation{openfaas}
\citation{nuclio}
\citation{knative}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The major components of the control plane, and the areas of each this thesis impacts.\relax }}{4}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:control-plane}{{1.1}{4}{The major components of the control plane, and the areas of each this thesis impacts.\relax }{figure.caption.7}{}}
\citation{chhatrapati2021towards}
\citation{docker-main}
\citation{gvisor}
\citation{shillaker2020faasm}
\citation{aytekin2019harnessing}
\citation{firecracker-nsdi20}
\citation{mohan2019agile}
\citation{du2020catalyzer}
\citation{warm2}
\citation{mvondo2021ofc}
\citation{romero2021faa}
\citation{eismann2021sizeless}
\citation{yu2021harvesting}
\citation{serverless-harvest-sosp21}
\citation{faascache-asplos21}
\citation{shahrad2020serverless}
\citation{zhao2021understanding}
\citation{enes2020real}
\citation{li2022kneescale}
\citation{shahrad2020serverless}
\citation{balaji2021fireplace}
\citation{kaffes2021practical}
\citation{abdi2023palette}
\citation{openwhisk}
\citation{aumala2019beyond}
\citation{leegreedy}
\citation{faaslb-hpdc22}
\citation{choi2020lambda}
\citation{pemberton2022kernel}
\citation{guleria2019emf}
\citation{bacis2020blastfunction}
\citation{du2022serverless}
\citation{romero2021llama}
\citation{yang2022infless}
\citation{ali2022optimizing}
\citation{zhang2019video}
\citation{risco2021gpu}
\citation{hung2019rapid}
\citation{shankar2020serverless}
\citation{yuan2022smpi}
\citation{copik2023fmi}
\citation{copik2022faaskeeper}
\citation{sreekanti2020fault}
\citation{sreekanti2020cloudburst}
\citation{giantsidi2023flexlog}
\citation{xu2021lambdadnn}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Thesis Outline}{6}{section.1.1}\protected@file@percent }
\citation{karger1997consistent}
\citation{openwhisk}
\citation{influx}
\citation{docker-main}
\citation{lambda}
\citation{gcp-functions}
\citation{azure-functions}
\citation{openwhisk}
\citation{alibaba-compute}
\citation{hendrickson2016serverless}
\citation{openfaas}
\@writefile{toc}{\contentsline {chapter}{\numberline {2.}Background: Serverless computing and Function as a Service}{10}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\newlabel{chap:serverless}{{2}{10}{Background: Serverless computing and Function as a Service}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}What exactly is Serverless Computing?}{10}{section.2.1}\protected@file@percent }
\newlabel{sec:serverless-computing}{{2.1}{10}{What exactly is Serverless Computing?}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A common architecture for serverless control planes. A controller distributes invocations to workers who run them inside containers.\relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:serverless-arch}{{2.1}{11}{A common architecture for serverless control planes. A controller distributes invocations to workers who run them inside containers.\relax }{figure.caption.8}{}}
\citation{lambda-pricing}
\citation{openwhisk}
\citation{openfaas}
\citation{hendrickson2016serverless}
\citation{docker-main}
\citation{firecracker-nsdi20}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A classic serverless function: performing ML inference on image data. In this example library and model initialization are done before execution starts.\relax }}{12}{figure.caption.9}\protected@file@percent }
\newlabel{fig:background-lambda-example}{{2.2}{12}{A classic serverless function: performing ML inference on image data. In this example library and model initialization are done before execution starts.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces FaaS workloads are highly diverse in their resource requirements and running times. The initialization time can be significant and is the cause of the cold-start overheads, and depends on the size of code and data dependencies.\relax }}{13}{table.caption.10}\protected@file@percent }
\newlabel{tab:bg-workloads}{{2.1}{13}{FaaS workloads are highly diverse in their resource requirements and running times. The initialization time can be significant and is the cause of the cold-start overheads, and depends on the size of code and data dependencies.\relax }{table.caption.10}{}}
\citation{shahrad_serverless_2020}
\citation{xen}
\citation{dong2008sr}
\citation{ben2010turtles}
\citation{waldspurger2002memory}
\citation{binpacking}
\citation{fuerst2022memory}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Serverless Containers}{14}{section.2.2}\protected@file@percent }
\newlabel{sec:virtualization}{{2.2}{14}{Serverless Containers}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Virtual Machines}{14}{subsection.2.2.1}\protected@file@percent }
\citation{oci}
\citation{docker-main}
\citation{kubernetes}
\newlabel{fig:vms}{{2.3a}{15}{Subfigure 2 2.3a}{subfigure.2.3.1}{}}
\newlabel{sub@fig:vms}{{(a)}{a}{Subfigure 2 2.3a\relax }{subfigure.2.3.1}{}}
\newlabel{fig:docker}{{2.3b}{15}{Subfigure 2 2.3b}{subfigure.2.3.2}{}}
\newlabel{sub@fig:docker}{{(b)}{b}{Subfigure 2 2.3b\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Different layers of abstraction between hardware and kernel based virtualization.\relax }}{15}{figure.caption.11}\protected@file@percent }
\newlabel{fig:docker-vs-vms}{{2.3}{15}{Different layers of abstraction between hardware and kernel based virtualization.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Virtual Machines}}}{15}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Containers}}}{15}{subfigure.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}VM Resource Management}{15}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Containers}{15}{subsection.2.2.3}\protected@file@percent }
\citation{docker-main}
\citation{firecracker-nsdi20}
\citation{shahrad_serverless_2020}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Serverless Control Plane Research}{16}{section.2.3}\protected@file@percent }
\newlabel{sec:platform-enhance}{{2.3}{16}{Serverless Control Plane Research}{section.2.3}{}}
\citation{functionbench}
\citation{shahrad_serverless_2020}
\citation{shahrad_serverless_2020}
\citation{shahrad_serverless_2020}
\citation{romero2021faa}
\citation{raza2021sok}
\citation{hossein2022survey}
\citation{eismann2020serverless}
\citation{du2020catalyzer}
\citation{vhive-asplos21}
\citation{du2020catalyzer}
\citation{wei2022booting}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A CDF of daily invocations for functions. Invocation frequencies range from sub-second to less than one per day. Figure from \cite  {shahrad_serverless_2020}\relax }}{17}{figure.caption.12}\protected@file@percent }
\newlabel{fig:wild-invokes}{{2.4}{17}{A CDF of daily invocations for functions. Invocation frequencies range from sub-second to less than one per day. Figure from \cite {shahrad_serverless_2020}\relax }{figure.caption.12}{}}
\citation{docker-main}
\citation{firecracker-nsdi20}
\citation{carreira2021warm}
\citation{vhive-asplos21}
\citation{shillaker2020faasm}
\citation{jia2021nightcore}
\citation{du2020catalyzer}
\citation{akkus_sand_2018}
\citation{dukic2020photons}
\citation{akhtar_cose_2020}
\citation{eismann2021sizeless}
\citation{mvondo2021ofc}
\citation{stojkovic2023mxfaas}
\citation{faas-survey-jan-2022}
\citation{raza2021sok}
\citation{eismann2020serverless}
\citation{hassan2021survey}
\citation{mampage2021holistic}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces FaaS workloads are highly diverse in their resource requirements and execution times. The initialization time can be significant and is the cause of the cold-start overheads, and depends on the size of code and data dependencies.\relax }}{18}{table.caption.13}\protected@file@percent }
\newlabel{tab:workloads}{{2.2}{18}{FaaS workloads are highly diverse in their resource requirements and execution times. The initialization time can be significant and is the cause of the cold-start overheads, and depends on the size of code and data dependencies.\relax }{table.caption.13}{}}
\citation{package-cristina-19}
\citation{leegreedy}
\citation{balaji2021fireplace}
\citation{shahrad2020serverless}
\citation{shen2021defuse}
\citation{abdi2023palette}
\citation{guo_decomposing_2022}
\citation{kotni2021faastlane}
\citation{shen_defuse_2021}
\citation{mahgoub_wisefuse_2022}
\citation{zhou_qos-aware_2022}
\citation{tian_owl_2022}
\citation{package-cristina-19}
\citation{suresh2019fnsched}
\citation{suresh2021servermore}
\citation{ensure-faas-acsos20}
\citation{hunhoff2020proactive}
\citation{yu2021faasrank}
\citation{puru_xanadu_20}
\citation{przybylski2021data}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Load Balancing}{19}{subsection.2.3.1}\protected@file@percent }
\citation{zaharia2010delay}
\citation{gupta2007analysis}
\citation{choi2020lambda}
\citation{du2022serverless}
\citation{pemberton2022kernel}
\citation{daw2021speedo}
\citation{roy2022icebreaker}
\citation{mvondo2021ofc}
\citation{romero2021faa}
\citation{giantsidi2023flexlog}
\citation{sreekanti2020fault}
\citation{abdi2023palette}
\citation{basu2023propack}
\citation{copik2023fmi}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Heterogeneous Hardware}{20}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Serverless Data-plane}{20}{subsection.2.3.3}\protected@file@percent }
\citation{jonas2017occupy}
\citation{fouladi2019laptop}
\citation{ao2018sprocket}
\citation{zhang2019video}
\citation{romero2021llama}
\citation{risco2021gpu}
\citation{konstantoudakis2022serverless}
\citation{wang2021wearmask}
\citation{elordi2021demand}
\citation{yan2016building}
\citation{anand2019low}
\citation{benedetti2021experimental}
\citation{trilles2020iot}
\citation{hu2020hivemind}
\citation{persson2017kappa}
\citation{cicconetti2020decentralized}
\citation{cheng2019fog}
\citation{wang2020supporting}
\citation{hussain2019serverless}
\citation{mete2021implementation}
\citation{zhang2021serverless}
\citation{donkervlietservo}
\citation{copik2022faaskeeper}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Application Mitigations of Control Plane Deficiencies}{21}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Serverless Applications}{21}{section.2.5}\protected@file@percent }
\newlabel{sec:serverless-apps}{{2.5}{21}{Serverless Applications}{section.2.5}{}}
\citation{balaji2021fireplace}
\citation{mvondo2021ofc}
\citation{eismann2021sizeless}
\citation{yang2022infless}
\citation{ali2022optimizing}
\citation{wang2019distributed}
\citation{gimeno2022mlless}
\citation{xu2021lambdadnn}
\citation{funcx_hpdc_20}
\citation{kumanov2018serverless}
\citation{hung2019rapid}
\citation{werner2018serverless}
\citation{shankar2020serverless}
\citation{aytekin2019harnessing}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}ML in Serverless}{22}{section.2.6}\protected@file@percent }
\newlabel{sec:serverless-ai}{{2.6}{22}{ML in Serverless}{section.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Scientific Serverless Computing}{22}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3.}Keeping Serverless Computing Alive with Greedy-Dual Caching}{23}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\newlabel{chap:faascache}{{3}{23}{Keeping Serverless Computing Alive with Greedy-Dual Caching}{chapter.3}{}}
\citation{oakes_sock_2018}
\citation{mohan_agile_2019}
\citation{akkus_sand_2018}
\citation{unikernels}
\citation{firecracker-nsdi20}
\citation{du2020catalyzer}
\citation{shahrad_serverless_2020}
\citation{lin_mitigating_2019}
\citation{warm2}
\citation{warm1}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Related Works}{24}{section.3.1}\protected@file@percent }
\newlabel{sec:faascache-related}{{3.1}{24}{Related Works}{section.3.1}{}}
\citation{carver_search_2019}
\citation{ghosh_caching_2019}
\citation{ensure_acsos20}
\citation{o1993lru}
\citation{cheng2000lru}
\citation{megiddo2003arc}
\citation{einziger2017tinylfu}
\citation{cao_irani_1997}
\citation{young_gd_orig_94}
\citation{gdsf}
\citation{gdfs_2001}
\citation{cherkasova2001role}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Caching Background}{25}{subsection.3.1.1}\protected@file@percent }
\citation{osca_atc20}
\citation{hu2016kinetic}
\citation{che2002hierarchical}
\citation{sundarrajan2017footprint}
\citation{shards}
\citation{counterstacks}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Keep-alive Tradeoffs}{26}{section.3.2}\protected@file@percent }
\newlabel{sec:tradeoffs}{{3.2}{26}{Keep-alive Tradeoffs}{section.3.2}{}}
\citation{goog-functions-tricks}
\citation{aws-warm-predictable}
\citation{azure-warmup-trigger}
\citation{openwhisk}
\citation{jonas2017occupy}
\citation{shankar2018numpywren}
\citation{akkus_sand_2018}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Timeline of function execution and sources of cold-start delay in OpenWhisk for an ML inference application.\relax }}{28}{figure.caption.14}\protected@file@percent }
\newlabel{fig:timeline}{{3.1}{28}{Timeline of function execution and sources of cold-start delay in OpenWhisk for an ML inference application.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Initializing functions by importing and downloading code and data dependencies can reduce function latency by hiding the cold-start overhead.\relax }}{28}{figure.caption.15}\protected@file@percent }
\newlabel{fig:lambda-example}{{3.2}{28}{Initializing functions by importing and downloading code and data dependencies can reduce function latency by hiding the cold-start overhead.\relax }{figure.caption.15}{}}
\citation{shahrad_serverless_2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Policy Goals and Considerations}{29}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Caching-based Keep-Alive Policies}{31}{section.3.3}\protected@file@percent }
\newlabel{sec:cache-keep-alive}{{3.3}{31}{Caching-based Keep-Alive Policies}{section.3.3}{}}
\citation{gdsf}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Greedy-Dual Keep-Alive Policy}{32}{subsection.3.3.1}\protected@file@percent }
\newlabel{subsec:gdsf}{{3.3.1}{32}{Greedy-Dual Keep-Alive Policy}{subsection.3.3.1}{}}
\citation{young_gd_orig_94}
\newlabel{eq:prio-prop}{{3.1}{33}{Greedy-Dual Keep-Alive Policy}{equation.3.3.1}{}}
\citation{young2002line}
\citation{young2002line}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Other Caching-Based Policies}{35}{subsection.3.3.2}\protected@file@percent }
\newlabel{subsec:variants}{{3.3.2}{35}{Other Caching-Based Policies}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Server Provisioning Policies}{36}{section.3.4}\protected@file@percent }
\newlabel{sec:provision}{{3.4}{36}{Server Provisioning Policies}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Static Provisioning}{37}{subsection.3.4.1}\protected@file@percent }
\newlabel{subsec:static}{{3.4.1}{37}{Static Provisioning}{subsection.3.4.1}{}}
\citation{shards}
\newlabel{eq:hrc-rdd}{{3.2}{38}{Static Provisioning}{equation.3.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations of the Caching Analogy.}{38}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Hit ratio curve using reuse distances show slight deviations from the observed hit ratios due to dropped requests at lower sizes, and concurrent executions at higher sizes.\relax }}{39}{figure.caption.16}\protected@file@percent }
\newlabel{fig:hrc}{{3.3}{39}{Hit ratio curve using reuse distances show slight deviations from the observed hit ratios due to dropped requests at lower sizes, and concurrent executions at higher sizes.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Elastic Dynamic Scaling}{39}{subsection.3.4.2}\protected@file@percent }
\newlabel{subsec:dynamic}{{3.4.2}{39}{Elastic Dynamic Scaling}{subsection.3.4.2}{}}
\citation{pid-wiki}
\citation{deflation-eurosys19}
\citation{gandhi2012autoscale}
\newlabel{eq:dyn}{{3.3}{40}{Elastic Dynamic Scaling}{equation.3.4.3}{}}
\citation{shahrad_serverless_2020}
\citation{zhang2020osca}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces FaasCache system components. We build on OpenWhisk and augment it with new keep-alive policies and a provisioning controller. \relax }}{41}{figure.caption.19}\protected@file@percent }
\newlabel{fig:sys}{{3.4}{41}{FaasCache system components. We build on OpenWhisk and augment it with new keep-alive policies and a provisioning controller. \relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Online adjustments.}{41}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Implementation}{41}{section.3.5}\protected@file@percent }
\citation{deflation-eurosys19}
\citation{shahrad_serverless_2020}
\newlabel{fig:rep-trace-exec}{{3.5a}{43}{Subfigure 3 3.5a}{subfigure.3.5.1}{}}
\newlabel{sub@fig:rep-trace-exec}{{(a)}{a}{Subfigure 3 3.5a\relax }{subfigure.3.5.1}{}}
\newlabel{fig:rare-trace-exec}{{3.5b}{43}{Subfigure 3 3.5b}{subfigure.3.5.2}{}}
\newlabel{sub@fig:rare-trace-exec}{{(b)}{b}{Subfigure 3 3.5b\relax }{subfigure.3.5.2}{}}
\newlabel{fig:random-trace-exec}{{3.5c}{43}{Subfigure 3 3.5c}{subfigure.3.5.3}{}}
\newlabel{sub@fig:random-trace-exec}{{(c)}{c}{Subfigure 3 3.5c\relax }{subfigure.3.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Increase in execution time due to cold-starts for different workloads derived from the Azure function trace.\relax }}{43}{figure.caption.20}\protected@file@percent }
\newlabel{fig:exec-overheads-all}{{3.5}{43}{Increase in execution time due to cold-starts for different workloads derived from the Azure function trace.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { Representative functions. }}}{43}{subfigure.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rare functions. }}}{43}{subfigure.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Random sampling. }}}{43}{subfigure.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Experimental Evaluation}{43}{section.3.6}\protected@file@percent }
\newlabel{sec:eval}{{3.6}{43}{Experimental Evaluation}{section.3.6}{}}
\citation{kim_functionbench_2019}
\citation{shahrad_serverless_2020}
\newlabel{fig:rep-trace-cold}{{3.6a}{44}{Subfigure 3 3.6a}{subfigure.3.6.1}{}}
\newlabel{sub@fig:rep-trace-cold}{{(a)}{a}{Subfigure 3 3.6a\relax }{subfigure.3.6.1}{}}
\newlabel{fig:rare-trace-cold}{{3.6b}{44}{Subfigure 3 3.6b}{subfigure.3.6.2}{}}
\newlabel{sub@fig:rare-trace-cold}{{(b)}{b}{Subfigure 3 3.6b\relax }{subfigure.3.6.2}{}}
\newlabel{fig:random-trace-cold}{{3.6c}{44}{Subfigure 3 3.6c}{subfigure.3.6.3}{}}
\newlabel{sub@fig:random-trace-cold}{{(c)}{c}{Subfigure 3 3.6c\relax }{subfigure.3.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Fraction of cold-starts is lower with caching-based keep-alive. \relax }}{44}{figure.caption.21}\protected@file@percent }
\newlabel{fig:cold-starts-all}{{3.6}{44}{Fraction of cold-starts is lower with caching-based keep-alive. \relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Representative functions.}}}{44}{subfigure.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rare functions. }}}{44}{subfigure.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Random sampling. }}}{44}{subfigure.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Size and inter-arrival time (IAT) details for the Azure Function workloads used in our evaluation.\relax }}{44}{table.caption.22}\protected@file@percent }
\newlabel{tab:trace-deets}{{3.1}{44}{Size and inter-arrival time (IAT) details for the Azure Function workloads used in our evaluation.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Adapting the Azure Functions Trace.}{45}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Trace-Driven Keep-Alive Evaluation}{45}{subsection.3.6.1}\protected@file@percent }
\citation{shahrad_serverless_2020}
\citation{welford}
\citation{shahrad_serverless_2020}
\citation{basu2017adaptive}
\citation{jiang2018convergence}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces FaasCache runs 50 to 100\% more cold and warm functions, for skewed workload traces.\relax }}{48}{figure.caption.24}\protected@file@percent }
\newlabel{fig:litmus_2}{{3.7}{48}{FaasCache runs 50 to 100\% more cold and warm functions, for skewed workload traces.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}OpenWhisk Evaluation}{48}{subsection.3.6.2}\protected@file@percent }
\newlabel{subsec:ow-eval}{{3.6.2}{48}{OpenWhisk Evaluation}{subsection.3.6.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces FaaS workloads are highly diverse in their resource requirements and running times. The initialization time can be significant and is the cause of the cold-start overheads, and depends on the size of code and data dependencies.\relax }}{49}{table.caption.25}\protected@file@percent }
\newlabel{tab:workloads}{{3.2}{49}{FaaS workloads are highly diverse in their resource requirements and running times. The initialization time can be significant and is the cause of the cold-start overheads, and depends on the size of code and data dependencies.\relax }{table.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces FaasCache increases warm-starts by more than $2\times $, which also reduces system load and dropped functions.\relax }}{50}{figure.caption.26}\protected@file@percent }
\newlabel{fig:faasbench}{{3.8}{50}{FaasCache increases warm-starts by more than $2\times $, which also reduces system load and dropped functions.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Effectiveness of Provisioning Policies}{50}{subsection.3.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces With dynamic cache size adjustment, the cold starts per second are kept close to the target (horizontal line), which reduces the average server size by 30\%. \relax }}{51}{figure.caption.27}\protected@file@percent }
\newlabel{fig:dynamic}{{3.9}{51}{With dynamic cache size adjustment, the cold starts per second are kept close to the target (horizontal line), which reduces the average server size by 30\%. \relax }{figure.caption.27}{}}
\citation{package-cristina-19}
\citation{suresh2019fnsched}
\citation{suresh2021servermore}
\citation{ensure-faas-acsos20}
\citation{hunhoff2020proactive}
\citation{yu2021faasrank}
\citation{puru_xanadu_20}
\citation{przybylski2021data}
\@writefile{toc}{\contentsline {chapter}{\numberline {4.}Load- and Locality-Aware Load Balancing}{52}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\newlabel{chap:chrlu}{{4}{52}{Load- and Locality-Aware Load Balancing}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Related Work}{52}{section.4.1}\protected@file@percent }
\citation{zaharia2010delay}
\citation{gupta2007analysis}
\citation{karger1999web}
\citation{decandia2007dynamo}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Background: Load Balancing}{53}{section.4.2}\protected@file@percent }
\citation{nygren2010akamai}
\citation{decandia2007dynamo}
\citation{karger1999web}
\citation{karger1997consistent}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Consistent hashing runs functions on the nearest clockwise server. Functions are forwarded along the ring if the server is overloaded.\relax }}{54}{figure.caption.28}\protected@file@percent }
\newlabel{fig:ch}{{4.1}{54}{Consistent hashing runs functions on the nearest clockwise server. Functions are forwarded along the ring if the server is overloaded.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Consistent Hashing}{54}{subsection.4.2.1}\protected@file@percent }
\newlabel{subsec:ch}{{4.2.1}{54}{Consistent Hashing}{subsection.4.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Load-aware Consistent-Hashing}{55}{section.4.3}\protected@file@percent }
\newlabel{sec:chrlu}{{4.3}{55}{Load-aware Consistent-Hashing}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Tradeoff between Locality and Load}{55}{subsection.4.3.1}\protected@file@percent }
\citation{mirrokni2018consistent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Key Principle: Load-based Forwarding}{56}{subsection.4.3.2}\protected@file@percent }
\citation{chrj-aaai21}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Server Load Information}{57}{subsection.4.3.3}\protected@file@percent }
\citation{mirrokni2018consistent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Why CH-BL Is Insufficient}{58}{subsection.4.3.4}\protected@file@percent }
\citation{mirrokni2018consistent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Incorporating Function Performance Characteristics}{59}{subsection.4.3.5}\protected@file@percent }
\newlabel{subsec:chch}{{4.3.5}{59}{Incorporating Function Performance Characteristics}{subsection.4.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Handling Bursts}{60}{subsection.4.3.6}\protected@file@percent }
\newlabel{subsec:bursty}{{4.3.6}{60}{Handling Bursts}{subsection.4.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Detecting Popular Functions with Spatial Sampling}{60}{section*.29}\protected@file@percent }
\citation{shards}
\@writefile{toc}{\contentsline {subsubsection}{Randomly Updating Stale Loads}{61}{section*.30}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces SHARDS-inspired popular function detection. Functions with the top p percentile of IATs are 'popular'.\relax }}{62}{algorithm.1}\protected@file@percent }
\newlabel{algo:shards-popular}{{1}{62}{SHARDS-inspired popular function detection. Functions with the top p percentile of IATs are 'popular'.\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Putting it all together: CH-RLU}{62}{subsection.4.3.7}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Random Load Update Forwarding Function\relax }}{63}{algorithm.2}\protected@file@percent }
\newlabel{algo:PopularRLUPolicy}{{2}{63}{Random Load Update Forwarding Function\relax }{algorithm.2}{}}
\citation{faascache-asplos21}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Implementation}{64}{section.4.4}\protected@file@percent }
\newlabel{sec:impl}{{4.4}{64}{Implementation}{section.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces System diagram of relevant OpenWhisk components and communication used to schedule and run function invocations.\relax }}{65}{figure.caption.31}\protected@file@percent }
\newlabel{fig:sys-diag}{{4.2}{65}{System diagram of relevant OpenWhisk components and communication used to schedule and run function invocations.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Performance Optimizations For OpenWhisk}{65}{subsection.4.4.1}\protected@file@percent }
\citation{faascache-asplos21}
\newlabel{fig:20-normalized-latencies}{{4.3a}{67}{Subfigure 4 4.3a}{subfigure.4.3.1}{}}
\newlabel{sub@fig:20-normalized-latencies}{{(a)}{a}{Subfigure 4 4.3a\relax }{subfigure.4.3.1}{}}
\newlabel{fig:20-invokes}{{4.3b}{67}{Subfigure 4 4.3b}{subfigure.4.3.2}{}}
\newlabel{sub@fig:20-invokes}{{(b)}{b}{Subfigure 4 4.3b\relax }{subfigure.4.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Latency and throughput under low-load. Locality-agnostic least-loaded policy has more cold starts and a higher impact on latency.\relax }}{67}{figure.caption.32}\protected@file@percent }
\newlabel{fig:low-load}{{4.3}{67}{Latency and throughput under low-load. Locality-agnostic least-loaded policy has more cold starts and a higher impact on latency.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Global Latency Impact}}}{67}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Invocation Throughput}}}{67}{subfigure.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Evaluation}{67}{section.4.5}\protected@file@percent }
\newlabel{sec:chrlu-eval}{{4.5}{67}{Evaluation}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Evaluation Environment}{68}{subsection.4.5.1}\protected@file@percent }
\citation{kim_functionbench_2019}
\citation{shahrad_serverless_2020}
\citation{locust}
\newlabel{fig:120-normalized-latencies}{{4.4a}{69}{Subfigure 4 4.4a}{subfigure.4.4.1}{}}
\newlabel{sub@fig:120-normalized-latencies}{{(a)}{a}{Subfigure 4 4.4a\relax }{subfigure.4.4.1}{}}
\newlabel{fig:120-invokes}{{4.4b}{69}{Subfigure 4 4.4b}{subfigure.4.4.2}{}}
\newlabel{sub@fig:120-invokes}{{(b)}{b}{Subfigure 4 4.4b\relax }{subfigure.4.4.2}{}}
\newlabel{fig:120-load-variance}{{4.4c}{69}{Subfigure 4 4.4c}{subfigure.4.4.3}{}}
\newlabel{sub@fig:120-load-variance}{{(c)}{c}{Subfigure 4 4.4c\relax }{subfigure.4.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces At high server loads, our RLU policy reduces average latency by 2.2x at higher throughput, compared to OpenWhisk's default policy. It does so by keeping cold-starts and load-variances low.\relax }}{69}{figure.caption.33}\protected@file@percent }
\newlabel{fig:high-load}{{4.4}{69}{At high server loads, our RLU policy reduces average latency by 2.2x at higher throughput, compared to OpenWhisk's default policy. It does so by keeping cold-starts and load-variances low.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Latency}}}{69}{subfigure.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Throughput}}}{69}{subfigure.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Server Load variance}}}{69}{subfigure.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Load-balancing Performance}{70}{subsection.4.5.2}\protected@file@percent }
\newlabel{sec:policy-comare}{{4.5.2}{70}{Load-balancing Performance}{subsection.4.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Handling Bursty Traffic}{70}{figure.caption.36}\protected@file@percent }
\newlabel{fig:bursty-latencies}{{4.5a}{71}{Subfigure 4 4.5a}{subfigure.4.5.1}{}}
\newlabel{sub@fig:bursty-latencies}{{(a)}{a}{Subfigure 4 4.5a\relax }{subfigure.4.5.1}{}}
\newlabel{fig:bursty-variance}{{4.5b}{71}{Subfigure 4 4.5b}{subfigure.4.5.2}{}}
\newlabel{sub@fig:bursty-variance}{{(b)}{b}{Subfigure 4 4.5b\relax }{subfigure.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces RLU improves latency by 10\% compared to OpenWhisk under bursty load conditions, while keeping a low worker load variance.\relax }}{71}{figure.caption.35}\protected@file@percent }
\newlabel{fig:bursty-closedload}{{4.5}{71}{RLU improves latency by 10\% compared to OpenWhisk under bursty load conditions, while keeping a low worker load variance.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Global Latency Impact}}}{71}{subfigure.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Worker Load Variance}}}{71}{subfigure.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Global latency impact under a 30-minute long rising burst load from an open-loop generator. RLU reduces latency by 17\% compared to OpenWhisk.\relax }}{71}{figure.caption.36}\protected@file@percent }
\newlabel{fig:bursty-openload}{{4.6}{71}{Global latency impact under a 30-minute long rising burst load from an open-loop generator. RLU reduces latency by 17\% compared to OpenWhisk.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The average normalized function latency over time for a dynamic workload. New invokers are launched at the dashed lines, keeping the latency in check.\relax }}{72}{figure.caption.38}\protected@file@percent }
\newlabel{fig:scaling-latency}{{4.7}{72}{The average normalized function latency over time for a dynamic workload. New invokers are launched at the dashed lines, keeping the latency in check.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scaling}{72}{figure.caption.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Load-balancer Overhead}{73}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5.}Il\'{u}vatar: A Low-Latency FaaS Research Control Plane}{74}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\newlabel{chap:iluvatar}{{5}{74}{\sysname : A Low-Latency FaaS Research Control Plane}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Why a new control plane?}{74}{section.5.1}\protected@file@percent }
\newlabel{sec:ilu-motivation}{{5.1}{74}{Why a new control plane?}{section.5.1}{}}
\citation{faascache-asplos21}
\citation{cortez2017resource}
\citation{mytkowicz2009producing}
\citation{akkus_sand_2018}
\citation{shahrad_serverless_2020}
\citation{faascache-asplos21}
\citation{faaslb-hpdc22}
\citation{zhou2022aquatope}
\citation{ensure-faas-acsos20}
\citation{alzayat_groundhog_2022}
\citation{openwhisk}
\citation{kafka}
\citation{faaslb-hpdc22}
\citation{hotcarbon22-faas}
\citation{zuk_call_2022}
\citation{kim2019functionbench}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The latency overhead of the control plane, as the number of concurrent invocations increases. OpenWhisk overhead is significant and has high variance, resulting in high tail latency. Il\'{u}vatar\nobreakspace  {} reduces this overhead by 100x. \relax }}{77}{figure.caption.40}\protected@file@percent }
\newlabel{fig:ow-scaling}{{5.1}{77}{The latency overhead of the control plane, as the number of concurrent invocations increases. OpenWhisk overhead is significant and has high variance, resulting in high tail latency. \sysname ~ reduces this overhead by 100x. \relax }{figure.caption.40}{}}
\citation{faascache-asplos21}
\citation{firecracker-nsdi20}
\citation{kaffes2019shinjuku}
\citation{prekas2017zygos}
\citation{cortez2017resource}
\citation{mytkowicz2009producing}
\citation{akkus_sand_2018}
\citation{shahrad_serverless_2020}
\citation{faascache-asplos21}
\citation{faaslb-hpdc22}
\citation{zhou2022aquatope}
\citation{ensure-faas-acsos20}
\citation{alzayat_groundhog_2022}
\citation{popa_http_2010}
\citation{zhou_qos-aware_2022}
\citation{sreekanti2020cloudburst}
\citation{faaslb-hpdc22}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Il\'{u}vatar\nobreakspace  {}Design}{80}{section.5.2}\protected@file@percent }
\newlabel{sec:design}{{5.2}{80}{\sysname ~Design}{section.5.2}{}}
\citation{singhvi2021atoll}
\citation{kaffes_centralized_2019}
\citation{kaffes_hermod_2022}
\citation{faaslb-hpdc22}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Il\'{u}vatar\nobreakspace  {}has a worker-centric architecture. A per-worker queue helps schedule functions, and regulate load and overcommitment. \relax }}{81}{figure.caption.41}\protected@file@percent }
\newlabel{fig:arch}{{5.2}{81}{\sysname ~has a worker-centric architecture. A per-worker queue helps schedule functions, and regulate load and overcommitment. \relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Architecture and Overview}{81}{subsection.5.2.1}\protected@file@percent }
\newlabel{sec:design:arch}{{5.2.1}{81}{Architecture and Overview}{subsection.5.2.1}{}}
\citation{akhtar_cose_2020}
\citation{guo_decomposing_2022}
\citation{tian_owl_2022}
\citation{eismann2021sizeless}
\citation{kotni2021faastlane}
\citation{ustiugov2021benchmarking}
\citation{ao2022faasnap}
\citation{roy2022icebreaker}
\citation{shahrad_serverless_2020}
\citation{silva_prebaking_2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Function Lifecycle}{83}{subsection.5.2.2}\protected@file@percent }
\newlabel{sec:design:lifecycle}{{5.2.2}{83}{Function Lifecycle}{subsection.5.2.2}{}}
\citation{jia2021nightcore}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The main components of the Il\'{u}vatar\nobreakspace  {} overheads.\relax }}{84}{figure.caption.42}\protected@file@percent }
\newlabel{fig:timeline-flow}{{5.3}{84}{The main components of the \sysname ~ overheads.\relax }{figure.caption.42}{}}
\citation{oakes_sock_2018}
\citation{faascache-asplos21}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Latency of different Il\'{u}vatar\nobreakspace  {} worker components for a single warm invocation.\relax }}{85}{table.caption.43}\protected@file@percent }
\newlabel{tab:overheads}{{5.1}{85}{Latency of different \sysname ~ worker components for a single warm invocation.\relax }{table.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Worker Performance Optimizations}{85}{subsection.5.2.3}\protected@file@percent }
\newlabel{sec:design:worker}{{5.2.3}{85}{Worker Performance Optimizations}{subsection.5.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Resource Caching}{85}{section*.44}\protected@file@percent }
\citation{oakes_sock_2018}
\citation{faascache-asplos21}
\@writefile{toc}{\contentsline {subsubsection}{Async function life-cycle handling}{86}{section*.45}\protected@file@percent }
\citation{firecracker-nsdi20}
\citation{shillaker2020faasm}
\citation{graalvm}
\citation{containerd}
\citation{crun}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Container Handling}{87}{subsection.5.2.4}\protected@file@percent }
\newlabel{sec:design:ctr}{{5.2.4}{87}{Container Handling}{subsection.5.2.4}{}}
\citation{oci}
\citation{faaslb-hpdc22}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Function Invocation Queuing}{89}{section.5.3}\protected@file@percent }
\newlabel{sec:q}{{5.3}{89}{Function Invocation Queuing}{section.5.3}{}}
\citation{faaslb-hpdc22}
\citation{ristov_colder_warmer}
\citation{yang2000general}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Queue Architecture}{91}{subsection.5.3.1}\protected@file@percent }
\newlabel{sec:q:arch}{{5.3.1}{91}{Queue Architecture}{subsection.5.3.1}{}}
\citation{bender1998flow}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Queuing Policies}{92}{subsection.5.3.2}\protected@file@percent }
\newlabel{sec:q:pol}{{5.3.2}{92}{Queuing Policies}{subsection.5.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Implementation}{93}{section.5.4}\protected@file@percent }
\newlabel{sec:impl}{{5.4}{93}{Implementation}{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Support for FaaS research}{94}{subsection.5.4.1}\protected@file@percent }
\newlabel{sec:impl:support}{{5.4.1}{94}{Support for FaaS research}{subsection.5.4.1}{}}
\citation{kim2019functionbench}
\citation{lookbusy}
\citation{shahrad_serverless_2020}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experimental Evaluation}{96}{section.5.5}\protected@file@percent }
\newlabel{sec:ilu:eval}{{5.5}{96}{Experimental Evaluation}{section.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Control Plane and Function Performance}{96}{subsection.5.5.1}\protected@file@percent }
\newlabel{sec:ilu:eval:ovhead}{{5.5.1}{96}{Control Plane and Function Performance}{subsection.5.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The latency overhead of the control plane, as the number of concurrent invocations increases. OpenWhisk overhead is significant and has high variance, resulting in high tail latency. Il\'{u}vatar\nobreakspace  {} reduces this overhead by 100x. \relax }}{97}{figure.caption.46}\protected@file@percent }
\newlabel{fig:ow-scaling}{{5.4}{97}{The latency overhead of the control plane, as the number of concurrent invocations increases. OpenWhisk overhead is significant and has high variance, resulting in high tail latency. \sysname ~ reduces this overhead by 100x. \relax }{figure.caption.46}{}}
\newlabel{fig:flow:pyaes}{{5.5a}{97}{Subfigure 5 5.5a}{subfigure.5.5.1}{}}
\newlabel{sub@fig:flow:pyaes}{{(a)}{a}{Subfigure 5 5.5a\relax }{subfigure.5.5.1}{}}
\newlabel{fig:flow:json}{{5.5b}{97}{Subfigure 5 5.5b}{subfigure.5.5.2}{}}
\newlabel{sub@fig:flow:json}{{(b)}{b}{Subfigure 5 5.5b\relax }{subfigure.5.5.2}{}}
\newlabel{fig:flow:video}{{5.5c}{97}{Subfigure 5 5.5c}{subfigure.5.5.3}{}}
\newlabel{sub@fig:flow:video}{{(c)}{c}{Subfigure 5 5.5c\relax }{subfigure.5.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces End-to-end latency and execution times for different functions as we increase the concurrency levels.\relax }}{97}{figure.caption.47}\protected@file@percent }
\newlabel{fig:flow-fn-all}{{5.5}{97}{End-to-end latency and execution times for different functions as we increase the concurrency levels.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PyAES }}}{97}{subfigure.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {JSON }}}{97}{subfigure.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Video }}}{97}{subfigure.5.3}\protected@file@percent }
\citation{shahrad_architectural_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Most functions benefit from using a lower-level containerization and OS object caching on cold starts.\relax }}{99}{figure.caption.48}\protected@file@percent }
\newlabel{fig:cold}{{5.6}{99}{Most functions benefit from using a lower-level containerization and OS object caching on cold starts.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Queuing Performance}{99}{subsection.5.5.2}\protected@file@percent }
\newlabel{sec:eval:q}{{5.5.2}{99}{Queuing Performance}{subsection.5.5.2}{}}
\newlabel{fig:q-base:wted}{{5.7a}{100}{Subfigure 5 5.7a}{subfigure.5.7.1}{}}
\newlabel{sub@fig:q-base:wted}{{(a)}{a}{Subfigure 5 5.7a\relax }{subfigure.5.7.1}{}}
\newlabel{fig:q-base:box}{{5.7b}{100}{Subfigure 5 5.7b}{subfigure.5.7.2}{}}
\newlabel{sub@fig:q-base:box}{{(b)}{b}{Subfigure 5 5.7b\relax }{subfigure.5.7.2}{}}
\newlabel{fig:q-base:breakdown}{{5.7c}{100}{Subfigure 5 5.7c}{subfigure.5.7.3}{}}
\newlabel{sub@fig:q-base:breakdown}{{(c)}{c}{Subfigure 5 5.7c\relax }{subfigure.5.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Queuing performance on the stationary Azure workload. Size-based policies can provide significant latency benefits.\relax }}{100}{figure.caption.49}\protected@file@percent }
\newlabel{fig:q-baseline}{{5.7}{100}{Queuing performance on the stationary Azure workload. Size-based policies can provide significant latency benefits.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Overcommit }}}{100}{subfigure.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Distribution of function latencies }}}{100}{subfigure.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Latency breakdown }}}{100}{subfigure.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The per-invocation function latencies for different system sizes (\# CPUs). We see a sharp inflection point at 16 CPUs, and use that in our queuing evaluation.\relax }}{101}{figure.caption.50}\protected@file@percent }
\newlabel{fig:weak}{{5.8}{101}{The per-invocation function latencies for different system sizes (\# CPUs). We see a sharp inflection point at 16 CPUs, and use that in our queuing evaluation.\relax }{figure.caption.50}{}}
\newlabel{fig:q-burst:wted}{{5.9a}{101}{Subfigure 5 5.9a}{subfigure.5.9.1}{}}
\newlabel{sub@fig:q-burst:wted}{{(a)}{a}{Subfigure 5 5.9a\relax }{subfigure.5.9.1}{}}
\newlabel{fig:q-burst:box}{{5.9b}{101}{Subfigure 5 5.9b}{subfigure.5.9.2}{}}
\newlabel{sub@fig:q-burst:box}{{(b)}{b}{Subfigure 5 5.9b\relax }{subfigure.5.9.2}{}}
\newlabel{fig:q-burst:breakdown}{{5.9c}{101}{Subfigure 5 5.9c}{subfigure.5.9.3}{}}
\newlabel{sub@fig:q-burst:breakdown}{{(c)}{c}{Subfigure 5 5.9c\relax }{subfigure.5.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Small and bursty functions can get disproportionately impacted due to queuing. A little overcommitment can go a long way to reduce latency.\relax }}{101}{figure.caption.51}\protected@file@percent }
\newlabel{fig:q-burst}{{5.9}{101}{Small and bursty functions can get disproportionately impacted due to queuing. A little overcommitment can go a long way to reduce latency.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Overcommit }}}{101}{subfigure.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Distribution of function latencies }}}{101}{subfigure.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Latency Breakdown }}}{101}{subfigure.9.3}\protected@file@percent }
\citation{openwhisk}
\citation{openfaas}
\citation{nuclio}
\citation{knative}
\citation{funcx_hpdc_20}
\citation{jia2021nightcore}
\citation{hendrickson2016serverless}
\citation{oakes_sock_2018}
\citation{singhvi2021atoll}
\citation{vhive-asplos21}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Il\'{u}vatar\nobreakspace  {}running in-silico closely models the in-situ performance. Making it a viable exploration opportunity supplementing real experiments.\relax }}{104}{figure.caption.52}\protected@file@percent }
\newlabel{fig:sim-vs-live-little}{{5.10}{104}{\sysname ~running in-silico closely models the in-situ performance. Making it a viable exploration opportunity supplementing real experiments.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Related Work}{104}{section.5.6}\protected@file@percent }
\newlabel{sec:related}{{5.6}{104}{Related Work}{section.5.6}{}}
\citation{oakes_sock_2018}
\citation{hendrickson2016serverless}
\citation{jia2021nightcore}
\citation{singhvi2021atoll}
\citation{openfaas}
\citation{nuclio}
\citation{knative}
\citation{quevedo_evaluating_2019}
\citation{scheuner_lets_2022}
\citation{alzayat_groundhog_2022}
\citation{faaslb-hpdc22}
\citation{faascache-asplos21}
\citation{kim_scheduling_2021}
\citation{faaslb-hpdc22}
\citation{palade-edge-22}
\citation{pfandzelter_tinyfaas_2020}
\citation{hall_execution_2019}
\citation{wang2021lass}
\citation{ustiugov_analyzing_2021}
\citation{zuk_call_2022}
\citation{zuk_scheduling_2020}
\citation{kaffes_centralized_2019}
\citation{kaffes_hermod_2022}
\citation{faaslb-hpdc22}
\citation{yu2021faasrank}
\citation{fu2022sfs}
\citation{ensure-faas-acsos20}
\citation{shen_defuse_2021}
\citation{mahgoub_wisefuse_2022}
\citation{zhou_qos-aware_2022}
\citation{lambda-limits}
\citation{shahrad2020serverless}
\@writefile{toc}{\contentsline {chapter}{\numberline {6.}Black-Box GPU Acceleration for Serverless}{107}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\newlabel{chap:gpu-sched}{{6}{107}{Black-Box GPU Acceleration for Serverless}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Serverless GPU Challenges}{107}{section.6.1}\protected@file@percent }
\newlabel{sec:motiv}{{6.1}{107}{Serverless GPU Challenges}{section.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Attaching a GPU adds significant time to container startup overhead. All times are in seconds.\relax }}{107}{table.caption.53}\protected@file@percent }
\newlabel{tab:gpu-attatch}{{6.1}{107}{Attaching a GPU adds significant time to container startup overhead. All times are in seconds.\relax }{table.caption.53}{}}
\citation{du2020catalyzer}
\citation{lin_mitigating_2019}
\citation{manner_cold_2018}
\citation{mohan_agile_2019}
\citation{shahrad2020serverless}
\citation{naranjo2020accelerated}
\citation{pemberton2022kernel}
\citation{gu2023fast}
\citation{ng2023paella}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}GPUs Containers}{108}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Serverless Scheduling}{108}{subsection.6.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Time spent in a cold start without (top) and with (bottom) a GPU attached to a container hosting TensorFlow inference code. GPU attachment adds over a second to initialization time, and user code setup of the GPU increases agent startup inside the container. \relax }}{109}{figure.caption.54}\protected@file@percent }
\newlabel{fig:cold-timeline}{{6.1}{109}{Time spent in a cold start without (top) and with (bottom) a GPU attached to a container hosting TensorFlow inference code. GPU attachment adds over a second to initialization time, and user code setup of the GPU increases agent startup inside the container. \relax }{figure.caption.54}{}}
\citation{faaslb-hpdc22}
\citation{kaffes_hermod_2022}
\citation{abdi2023palette}
\citation{package-cristina-19}
\citation{yu2019automatic}
\citation{hong2017gpu}
\citation{ali_batch_2020}
\citation{yang2022infless}
\citation{ali2022optimizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Balancing Workloads}{110}{subsection.6.1.3}\protected@file@percent }
\citation{kim2019functionbench}
\citation{pyhpc-bench}
\citation{che2009rodinia}
\citation{ffmpeg}
\citation{john_sweep_2019}
\citation{mocskos_faaster_2018}
\citation{werner2018serverless}
\citation{shankar2020serverless}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Average invocation latency for a trace is 2x better on a small GPU platform running our desigin compared to a CPU-only system.\relax }}{111}{figure.caption.55}\protected@file@percent }
\newlabel{fig:cpu-compare}{{6.2}{111}{Average invocation latency for a trace is 2x better on a small GPU platform running our desigin compared to a CPU-only system.\relax }{figure.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces The functions in Tables\nobreakspace  {}\ref  {tab:gpu-attatch} and\nobreakspace  {}\ref  {tab:gpu-cpu} come from several sources. They are a subset of the ones we ported to Il\'{u}vatar.\relax }}{111}{table.caption.56}\protected@file@percent }
\newlabel{tab:fun-list}{{6.2}{111}{The functions in Tables~\ref {tab:gpu-attatch} and~\ref {tab:gpu-cpu} come from several sources. They are a subset of the ones we ported to \sysname .\relax }{table.caption.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Background}{111}{section.6.2}\protected@file@percent }
\newlabel{sec:bg}{{6.2}{111}{Background}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}GPU Support for Serverless}{111}{subsection.6.2.1}\protected@file@percent }
\citation{nvidia-mig}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Functions' get great performance benefits from running on GPU over CPU. All times are in seconds.\relax }}{112}{table.caption.57}\protected@file@percent }
\newlabel{tab:gpu-cpu}{{6.3}{112}{Functions' get great performance benefits from running on GPU over CPU. All times are in seconds.\relax }{table.caption.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}GPU Sharing Mechanisms}{112}{subsection.6.2.2}\protected@file@percent }
\citation{nvidia-mps}
\citation{gu2023fast}
\citation{duato2010rcuda}
\citation{yu2019automatic}
\citation{hong2017gpu}
\citation{yu2017full}
\citation{yu2019automatic}
\citation{hong2017gpu}
\citation{strati2024orion}
\citation{chen2017effisha}
\citation{kim2020navigator}
\citation{gu2023fast}
\citation{ng2023paella}
\citation{pemberton2022kernel}
\citation{strati2024orion}
\citation{fingler2022dgsf}
\citation{duato2010rcuda}
\citation{du2020catalyzer}
\citation{vhive-asplos21}
\citation{unikernels}
\citation{firecracker-nsdi20}
\citation{shillaker2020faasm}
\citation{shahrad2020serverless}
\citation{vahidinia2022mitigating}
\citation{ebrahimi2024cold}
\citation{faascache-asplos21}
\citation{faaslb-hpdc22}
\citation{balaji2021fireplace}
\citation{abdi2023palette}
\citation{yan2020hermes}
\citation{naranjo2020accelerated}
\citation{duato2010rcuda}
\citation{fingler2022dgsf}
\citation{pemberton2022kernel}
\citation{ng2023paella}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Related Work}{114}{section.6.3}\protected@file@percent }
\newlabel{sec:related}{{6.3}{114}{Related Work}{section.6.3}{}}
\citation{gu2023fast}
\citation{kaffes_hermod_2022}
\citation{kim_scheduling_2021}
\citation{abdi2023palette}
\citation{package-cristina-19}
\citation{serverless-harvest-sosp21}
\citation{faaslb-hpdc22}
\citation{openwhisk}
\citation{kaffes2021practical}
\citation{ao2018sprocket}
\citation{zhang2019video}
\citation{romero2021llama}
\citation{risco2021gpu}
\citation{yang2022infless}
\citation{ali2022optimizing}
\citation{ali_batch_2020}
\citation{funcx_hpdc_20}
\citation{kumanov2018serverless}
\citation{hung2019rapid}
\citation{werner2018serverless}
\citation{shankar2020serverless}
\citation{aytekin2019harnessing}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}GPU Scheduling Design}{115}{section.6.4}\protected@file@percent }
\newlabel{sec:design}{{6.4}{115}{GPU Scheduling Design}{section.6.4}{}}
\citation{hedayati2019multi}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Overview}{116}{subsection.6.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces MQFQ-Sticky\nobreakspace  {}system design.\relax }}{117}{figure.caption.58}\protected@file@percent }
\newlabel{fig:sys-diag}{{6.3}{117}{\QName ~system design.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Multi-Queuing}{118}{subsection.6.4.2}\protected@file@percent }
\citation{hedayati2019multi}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces List of symbols in MQFQ-Sticky's design.\relax }}{119}{table.caption.59}\protected@file@percent }
\newlabel{tab:mq-symbols}{{6.4}{119}{List of symbols in \QName 's design.\relax }{table.caption.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}MQFQ-Sticky}{119}{subsection.6.4.3}\protected@file@percent }
\newlabel{sec:mq}{{6.4.3}{119}{\QNameFull }{subsection.6.4.3}{}}
\citation{shahrad2020serverless}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Dispatch Algorithm}{121}{subsection.6.4.4}\protected@file@percent }
\newlabel{sec:dispatch}{{6.4.4}{121}{Dispatch Algorithm}{subsection.6.4.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Flow Dispatching Algorithm\relax }}{122}{algorithm.3}\protected@file@percent }
\newlabel{algo:dispatch}{{3}{122}{Flow Dispatching Algorithm\relax }{algorithm.3}{}}
\newlabel{lst:line:update_vt}{{3}{122}{Flow Dispatching Algorithm\relax }{algorithm.3}{}}
\newlabel{lst:line:filter}{{3}{122}{Flow Dispatching Algorithm\relax }{algorithm.3}{}}
\newlabel{lst:line:in_flight}{{3}{122}{Flow Dispatching Algorithm\relax }{algorithm.3}{}}
\newlabel{lst:line:done}{{3}{122}{Flow Dispatching Algorithm\relax }{algorithm.3}{}}
\newlabel{lst:line:update_state}{{3}{122}{Flow Dispatching Algorithm\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Memory Overcommitment}{122}{subsection.6.4.5}\protected@file@percent }
\newlabel{sec:design-cont-shim}{{6.4.5}{122}{Memory Overcommitment}{subsection.6.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.6}GPU Load Management}{123}{subsection.6.4.6}\protected@file@percent }
\newlabel{sec:gpu-man}{{6.4.6}{123}{GPU Load Management}{subsection.6.4.6}{}}
\citation{fuerst2023iluvatar}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Implementation}{124}{section.6.5}\protected@file@percent }
\newlabel{sec:impl}{{6.5}{124}{Implementation}{section.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Function Queue}{124}{subsection.6.5.1}\protected@file@percent }
\newlabel{sec:q-impl}{{6.5.1}{124}{Function Queue}{subsection.6.5.1}{}}
\citation{hedayati2019multi}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}GPU Driver Shim}{125}{subsection.6.5.2}\protected@file@percent }
\newlabel{sec:shim-impl}{{6.5.2}{125}{GPU Driver Shim}{subsection.6.5.2}{}}
\citation{nvml}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Experimental Evaluation}{126}{section.6.6}\protected@file@percent }
\newlabel{sec:eval}{{6.6}{126}{Experimental Evaluation}{section.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}UVM Shim}{127}{subsection.6.6.1}\protected@file@percent }
\newlabel{sec:shim}{{6.6.1}{127}{UVM Shim}{subsection.6.6.1}{}}
\citation{nvidia-uvm}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces More intelligent memory management improves execution latency. \texttt  {Prefetch To} moves memory on-device before a function container executes. \texttt  {Prefetch} additionally moves it off again when the container will be idle.\relax }}{128}{figure.caption.60}\protected@file@percent }
\newlabel{fig:mem-prefetch}{{6.4}{128}{More intelligent memory management improves execution latency. \texttt {Prefetch To} moves memory on-device before a function container executes. \texttt {Prefetch} additionally moves it off again when the container will be idle.\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Functions see little to no impact from our interception and substitution of allocation calls. This matches performance promised by Nvidia for UVM applications.\relax }}{129}{figure.caption.61}\protected@file@percent }
\newlabel{fig:shim-overhead}{{6.5}{129}{Functions see little to no impact from our interception and substitution of allocation calls. This matches performance promised by Nvidia for UVM applications.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Queuing Knobs}{129}{subsection.6.6.2}\protected@file@percent }
\newlabel{sec:queue-knobs}{{6.6.2}{129}{Queuing Knobs}{subsection.6.6.2}{}}
\citation{fuerst2023iluvatar}
\citation{faaslb-hpdc22}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces MQFQ-Sticky\nobreakspace  {}greatly reduces cold hits compared to FCFS, and is improved with a large container pool size. More cold hits are also caused when \emph  {D}\nobreakspace  {}(concurrency) is raised, needing private containers to serve concurrent invocations.\relax }}{131}{figure.caption.62}\protected@file@percent }
\newlabel{fig:container-pool-cold-hits}{{6.6}{131}{\QName ~greatly reduces cold hits compared to FCFS, and is improved with a large container pool size. More cold hits are also caused when \D ~(concurrency) is raised, needing private containers to serve concurrent invocations.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Adjusting \emph  {T}\nobreakspace  {}allows flows to overrun one another, increasing data locality and therefore performance. The performance changes when a function's GPU wall time is used to change \emph  {VT}, or the increment is fixed.\relax }}{132}{figure.caption.63}\protected@file@percent }
\newlabel{fig:unfairness-queue}{{6.7}{132}{Adjusting \T ~allows flows to overrun one another, increasing data locality and therefore performance. The performance changes when a function's GPU wall time is used to change \VT , or the increment is fixed.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Enabling a time-to-live for flows prevents them from becoming inactive, to keep resources warm on-device. This improves both latency and on-device execution time for functions. Note the non-linear scale on the X axis.\relax }}{133}{figure.caption.64}\protected@file@percent }
\newlabel{fig:flow-ttl}{{6.8}{133}{Enabling a time-to-live for flows prevents them from becoming inactive, to keep resources warm on-device. This improves both latency and on-device execution time for functions. Note the non-linear scale on the X axis.\relax }{figure.caption.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.3}MQFQ-Sticky\nobreakspace  {}Performance}{134}{subsection.6.6.3}\protected@file@percent }
\newlabel{sec:queue-perf}{{6.6.3}{134}{\QName ~Performance}{subsection.6.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Execution overhead grows as concurrency is increased. The gray line uses a fixed device concurrency, and the remaining lines represent the GPU utilization below which we allow a new dispatch.\relax }}{135}{figure.caption.65}\protected@file@percent }
\newlabel{fig:concur-exec-overhead}{{6.9}{135}{Execution overhead grows as concurrency is increased. The gray line uses a fixed device concurrency, and the remaining lines represent the GPU utilization below which we allow a new dispatch.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Latency for invocations is affected by concurrency. Increasing \emph  {D}\nobreakspace  {}when utilization is low improves latency, but if the threshold is too high, significant queuing delays occur.\relax }}{135}{figure.caption.66}\protected@file@percent }
\newlabel{fig:concur-e2e}{{6.10}{135}{Latency for invocations is affected by concurrency. Increasing \D ~when utilization is low improves latency, but if the threshold is too high, significant queuing delays occur.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Latency of various queue policies compared.\relax }}{136}{figure.caption.67}\protected@file@percent }
\newlabel{fig:queue-e2e}{{6.11}{136}{Latency of various queue policies compared.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Per-function latency comparison between FCFS and MQFQ-Sticky.\relax }}{137}{figure.caption.68}\protected@file@percent }
\newlabel{fig:queue-fairness}{{6.12}{137}{Per-function latency comparison between FCFS and \QName .\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Dispatching to multiple GPUs greatly improves latency and execution overhead compared to a single GPU. \relax }}{138}{figure.caption.69}\protected@file@percent }
\newlabel{fig:multi-gpu}{{6.13}{138}{Dispatching to multiple GPUs greatly improves latency and execution overhead compared to a single GPU. \relax }{figure.caption.69}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Discussion}{138}{section.6.7}\protected@file@percent }
\citation{cuda-ctx-overhead}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Dynamically selecting what compute a function runs on can reduce GPU queuing and improve global latency.\relax }}{139}{figure.caption.70}\protected@file@percent }
\newlabel{fig:poly}{{6.14}{139}{Dynamically selecting what compute a function runs on can reduce GPU queuing and improve global latency.\relax }{figure.caption.70}{}}
\citation{lin2020efficient}
\citation{guo2010slaw}
\citation{blumofe1999scheduling}
\@writefile{toc}{\contentsline {chapter}{\numberline {7.}Conclusion and Future Work}{140}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\newlabel{chap:summary}{{7}{140}{Conclusion and Future Work}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Future Work}{140}{section.7.1}\protected@file@percent }
\newlabel{sec:future}{{7.1}{140}{Future Work}{section.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Work Stealing Scheduling}{140}{subsection.7.1.1}\protected@file@percent }
\newlabel{sec:work-steal}{{7.1.1}{140}{Work Stealing Scheduling}{subsection.7.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Polymorphic Functions}{140}{subsection.7.1.2}\protected@file@percent }
\newlabel{chap:new-poly}{{7.1.2}{140}{Polymorphic Functions}{subsection.7.1.2}{}}
\citation{ginzburg2023vectorvisor}
\citation{giantsidi2023flexlog}
\citation{sreekanti2020fault}
\citation{mvondo2021ofc}
\citation{romero2021faa}
\citation{abdi2023palette}
\citation{hadoop}
\citation{mpi}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Serverless for Distributed Computing}{142}{subsection.7.1.3}\protected@file@percent }
\newlabel{sec:new-mpi}{{7.1.3}{142}{Serverless for Distributed Computing}{subsection.7.1.3}{}}
\citation{trach2019clemmys}
\citation{kim2023cryonics}
\citation{zhao2023reusable}
\citation{arnautov2016scone}
\citation{wang2022virtee}
\citation{tsai2017graphene}
\citation{jia2022hyperenclave}
\citation{tsai2017graphene}
\citation{jia2022hyperenclave}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}FaaS Security}{143}{subsection.7.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Conclusion}{144}{section.7.2}\protected@file@percent }
\newlabel{sec:conclusion}{{7.2}{144}{Conclusion}{section.7.2}{}}
\bibstyle{plain}
\bibdata{refs.bib,chrlu/faaslb-osdi22/camready/related.bib,chrlu/faaslb-osdi22/camready/faas-asplos.bib,chrlu/faaslb-osdi22/camready/faas.bib,chrlu/faaslb-osdi22/camready/lb.bib,iluvatar/camera-ready/efaas.bib,iluvatar/camera-ready/faas.bib,iluvatar/camera-ready/faas-asplos.bib,faascache/faas-keepalive-20/asplos_camready/faas.bib,mqfq/gpu-q-faas.bib}
\bibcite{containerd}{1}
\bibcite{pyhpc-bench}{2}
\bibcite{warm1}{3}
\bibcite{nvml}{4}
\bibcite{pid-wiki}{5}
\bibcite{docker-main}{6}
\bibcite{kubernetes}{7}
\bibcite{cuda-ctx-overhead}{8}
\bibcite{nvidia-uvm}{9}
\bibcite{warm2}{10}
\bibcite{aws-warm-predictable}{11}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{146}{chapter*.71}\protected@file@percent }
\bibcite{azure-warmup-trigger}{12}
\bibcite{functionbench}{13}
\bibcite{kafka}{14}
\bibcite{openwhisk}{15}
\bibcite{crun}{16}
\bibcite{goog-functions-tricks}{17}
\bibcite{openfaas}{18}
\bibcite{nuclio}{19}
\bibcite{knative}{20}
\bibcite{nvidia-mps}{21}
\bibcite{nvidia-mig}{22}
\bibcite{hadoop}{23}
\bibcite{burber}{24}
\bibcite{ffmpeg}{25}
\bibcite{influx}{26}
\bibcite{mpi}{27}
\bibcite{gvisor}{28}
\bibcite{abdi2023palette}{29}
\bibcite{firecracker-nsdi20}{30}
\bibcite{akhtar_cose_2020}{31}
\bibcite{akkus_sand_2018}{32}
\bibcite{ali_batch_2020}{33}
\bibcite{ali2022optimizing}{34}
\bibcite{alzayat_groundhog_2022}{35}
\bibcite{lambda-limits}{36}
\bibcite{harvest-osdi20}{37}
\bibcite{anand2019low}{38}
\bibcite{ao2018sprocket}{39}
\bibcite{ao2022faasnap}{40}
\bibcite{arnautov2016scone}{41}
\bibcite{aumala2019beyond}{42}
\bibcite{package-cristina-19}{43}
\bibcite{aytekin2019harnessing}{44}
\bibcite{azure-functions}{45}
\bibcite{bacis2020blastfunction}{46}
\bibcite{balaji2021fireplace}{47}
\bibcite{xen}{48}
\bibcite{basu2017adaptive}{49}
\bibcite{basu2023propack}{50}
\bibcite{ben2010turtles}{51}
\bibcite{bender1998flow}{52}
\bibcite{benedetti2021experimental}{53}
\bibcite{blumofe1999scheduling}{54}
\bibcite{graalvm}{55}
\bibcite{cao_irani_1997}{56}
\bibcite{lookbusy}{57}
\bibcite{carreira2021warm}{58}
\bibcite{carver_search_2019}{59}
\bibcite{funcx_hpdc_20}{60}
\bibcite{che2002hierarchical}{61}
\bibcite{che2009rodinia}{62}
\bibcite{chen2017effisha}{63}
\bibcite{chrj-aaai21}{64}
\bibcite{cheng2019fog}{65}
\bibcite{cheng2000lru}{66}
\bibcite{gdsf}{67}
\bibcite{gdfs_2001}{68}
\bibcite{cherkasova2001role}{69}
\bibcite{chhatrapati2021towards}{70}
\bibcite{choi2020lambda}{71}
\bibcite{cicconetti2020decentralized}{72}
\bibcite{alibaba-compute}{73}
\bibcite{lambda}{74}
\bibcite{lambda-pricing}{75}
\bibcite{gcp-functions}{76}
\bibcite{binpacking}{77}
\bibcite{copik2023fmi}{78}
\bibcite{copik2022faaskeeper}{79}
\bibcite{cortez2017resource}{80}
\bibcite{puru_xanadu_20}{81}
\bibcite{daw2021speedo}{82}
\bibcite{decandia2007dynamo}{83}
\bibcite{dong2008sr}{84}
\bibcite{donkervlietservo}{85}
\bibcite{du2022serverless}{86}
\bibcite{du2020catalyzer}{87}
\bibcite{duato2010rcuda}{88}
\bibcite{dukic2020photons}{89}
\bibcite{ebrahimi2024cold}{90}
\bibcite{einziger2017tinylfu}{91}
\bibcite{eismann2021sizeless}{92}
\bibcite{eismann2020serverless}{93}
\bibcite{elordi2021demand}{94}
\bibcite{enes2020real}{95}
\bibcite{fingler2022dgsf}{96}
\bibcite{fouladi2019laptop}{97}
\bibcite{fu2022sfs}{98}
\bibcite{fuerst2020cloud}{99}
\bibcite{fuerst2022memory}{100}
\bibcite{fuerst2023iluvatar}{101}
\bibcite{faascache-asplos21}{102}
\bibcite{faaslb-hpdc22}{103}
\bibcite{gandhi2012autoscale}{104}
\bibcite{ghosh_caching_2019}{105}
\bibcite{giantsidi2023flexlog}{106}
\bibcite{gimeno2022mlless}{107}
\bibcite{ginzburg2023vectorvisor}{108}
\bibcite{gu2023fast}{109}
\bibcite{guleria2019emf}{110}
\bibcite{guo2010slaw}{111}
\bibcite{guo_decomposing_2022}{112}
\bibcite{gupta2007analysis}{113}
\bibcite{hall_execution_2019}{114}
\bibcite{hassan2021survey}{115}
\bibcite{hedayati2019multi}{116}
\bibcite{hendrickson2016serverless}{117}
\bibcite{hong2017gpu}{118}
\bibcite{hu2020hivemind}{119}
\bibcite{hu2016kinetic}{120}
\bibcite{hung2019rapid}{121}
\bibcite{hunhoff2020proactive}{122}
\bibcite{hussain2019serverless}{123}
\bibcite{jia2022hyperenclave}{124}
\bibcite{jia2021nightcore}{125}
\bibcite{jiang2018convergence}{126}
\bibcite{john_sweep_2019}{127}
\bibcite{jonas2017occupy}{128}
\bibcite{kaffes2019shinjuku}{129}
\bibcite{kaffes_centralized_2019}{130}
\bibcite{kaffes2021practical}{131}
\bibcite{kaffes_hermod_2022}{132}
\bibcite{karger1997consistent}{133}
\bibcite{karger1999web}{134}
\bibcite{kim_scheduling_2021}{135}
\bibcite{kim_functionbench_2019}{136}
\bibcite{kim2019functionbench}{137}
\bibcite{kim2020navigator}{138}
\bibcite{kim2023cryonics}{139}
\bibcite{konstantoudakis2022serverless}{140}
\bibcite{kotni2021faastlane}{141}
\bibcite{kumanov2018serverless}{142}
\bibcite{leegreedy}{143}
\bibcite{li2022kneescale}{144}
\bibcite{faas-survey-jan-2022}{145}
\bibcite{lin2020efficient}{146}
\bibcite{lin_mitigating_2019}{147}
\bibcite{locust}{148}
\bibcite{unikernels}{149}
\bibcite{mahgoub_wisefuse_2022}{150}
\bibcite{mampage2021holistic}{151}
\bibcite{manner_cold_2018}{152}
\bibcite{megiddo2003arc}{153}
\bibcite{mete2021implementation}{154}
\bibcite{mirrokni2018consistent}{155}
\bibcite{mohan2019agile}{156}
\bibcite{mohan_agile_2019}{157}
\bibcite{mvondo2021ofc}{158}
\bibcite{mytkowicz2009producing}{159}
\bibcite{naranjo2020accelerated}{160}
\bibcite{ng2023paella}{161}
\bibcite{nygren2010akamai}{162}
\bibcite{oakes_sock_2018}{163}
\bibcite{o1993lru}{164}
\bibcite{palade-edge-22}{165}
\bibcite{pemberton2022kernel}{166}
\bibcite{persson2017kappa}{167}
\bibcite{pfandzelter_tinyfaas_2020}{168}
\bibcite{popa_http_2010}{169}
\bibcite{prekas2017zygos}{170}
\bibcite{oci}{171}
\bibcite{przybylski2021data}{172}
\bibcite{quevedo_evaluating_2019}{173}
\bibcite{raza2021sok}{174}
\bibcite{ringlein2021case}{175}
\bibcite{risco2021gpu}{176}
\bibcite{ristov_colder_warmer}{177}
\bibcite{romero2021faa}{178}
\bibcite{romero2021llama}{179}
\bibcite{roy2022icebreaker}{180}
\bibcite{russo2023serverless}{181}
\bibcite{sahraei2023xfaas}{182}
\bibcite{scheuner_lets_2022}{183}
\bibcite{hossein2022survey}{184}
\bibcite{shahrad_architectural_2019}{185}
\bibcite{shahrad2020serverless}{186}
\bibcite{shahrad_serverless_2020}{187}
\bibcite{shankar2018numpywren}{188}
\bibcite{shankar2020serverless}{189}
\bibcite{hotcarbon22-faas}{190}
\bibcite{deflation-eurosys19}{191}
\bibcite{shen2021defuse}{192}
\bibcite{shen_defuse_2021}{193}
\bibcite{shillaker2020faasm}{194}
\bibcite{silva_prebaking_2020}{195}
\bibcite{singhvi2021atoll}{196}
\bibcite{mocskos_faaster_2018}{197}
\bibcite{sreekanti2020fault}{198}
\bibcite{sreekanti2020cloudburst}{199}
\bibcite{stojkovic2023mxfaas}{200}
\bibcite{strati2024orion}{201}
\bibcite{sundarrajan2017footprint}{202}
\bibcite{suresh2021servermore}{203}
\bibcite{ensure_acsos20}{204}
\bibcite{ensure-faas-acsos20}{205}
\bibcite{suresh2019fnsched}{206}
\bibcite{tian_owl_2022}{207}
\bibcite{trach2019clemmys}{208}
\bibcite{trilles2020iot}{209}
\bibcite{tsai2017graphene}{210}
\bibcite{ustiugov_analyzing_2021}{211}
\bibcite{vhive-asplos21}{212}
\bibcite{ustiugov2021benchmarking}{213}
\bibcite{vahidinia2022mitigating}{214}
\bibcite{waldspurger2002memory}{215}
\bibcite{shards}{216}
\bibcite{wang2021lass}{217}
\bibcite{wang2019distributed}{218}
\bibcite{wang2020supporting}{219}
\bibcite{wang2022virtee}{220}
\bibcite{wang2021smartharvest}{221}
\bibcite{wang2021wearmask}{222}
\bibcite{wei2022booting}{223}
\bibcite{welford}{224}
\bibcite{werner2018serverless}{225}
\bibcite{counterstacks}{226}
\bibcite{xu2021lambdadnn}{227}
\bibcite{yan2020hermes}{228}
\bibcite{yan2016building}{229}
\bibcite{yang2022infless}{230}
\bibcite{yang2000general}{231}
\bibcite{young_gd_orig_94}{232}
\bibcite{young2002line}{233}
\bibcite{yu2021faasrank}{234}
\bibcite{yu2021harvesting}{235}
\bibcite{yu2019automatic}{236}
\bibcite{yu2017full}{237}
\bibcite{yuan2022smpi}{238}
\bibcite{zaharia2010delay}{239}
\bibcite{zhang2019video}{240}
\bibcite{zhang2021serverless}{241}
\bibcite{serverless-harvest-sosp21}{242}
\bibcite{osca_atc20}{243}
\bibcite{zhang2020osca}{244}
\bibcite{zhao2021understanding}{245}
\bibcite{zhao2023reusable}{246}
\bibcite{zhou2022aquatope}{247}
\bibcite{zhou_qos-aware_2022}{248}
\bibcite{zuk_scheduling_2020}{249}
\bibcite{zuk_call_2022}{250}
\citation{burber}
\@writefile{toc}{\contentsline {chapter}{Curriculum Vitae}{}{chapter*.72}\protected@file@percent }
