@inproceedings{iyer2001anticipatory,
  title={Anticipatory scheduling: A disk scheduling framework to overcome deceptive idleness in synchronous I/O},
  author={Iyer, Sitaram and Druschel, Peter},
  booktitle={Proceedings of the eighteenth ACM symposium on Operating systems principles},
  pages={117--130},
  year={2001}
}

@misc{azure-gpu-function,
  howpublished = {https://github.com/puthurr/python-azure-function-gpu},
  title = {{Project: GPU-Enabled docker image to host a Python PyTorch Azure Function}}
  }
@misc{alibaba-gpu-function,
  howpublished = {https://www.alibabacloud.com/help/en/fc/use-cases/best-practices-for-gpu-accelerated-instances/},
  title = {{Best practices for GPU-accelerated instances}}
  }
@misc{alibaba-gpu-noshare,
  howpublished = {https://www.alibabacloud.com/help/en/fc/use-cases/quasi-real-time-inference-scenarios#section-rzz-zcb-w4e},
  title        = {Cold Starts},
  year         = {2024}
}
@misc{aws-netflix,
  howpublished = {https://aws.amazon.com/solutions/case-studies/netflix-and-aws-lambda/},
  title = {{Netflix & AWS Lambda Case Study}}
  }
@misc{jetson-no-mps,
howpublished = {https://forums.developer.nvidia.com/t/mps-support-of-jetson-xavier/62397},
title = {{MPS support of Jetson Xavier }}
}
@inproceedings{varghese2015acceleration,
  title={Acceleration-as-a-service: Exploiting virtualised GPUs for a financial application},
  author={Varghese, Blesson and Prades, Javier and Reano, Carlos and Silla, Federico},
  booktitle={2015 IEEE 11th International Conference on e-Science},
  pages={47--56},
  year={2015},
  organization={IEEE}
}
@inproceedings{yamagiwa2009performance,
  title={Performance study of interference on gpu and cpu resources with multiple applications},
  author={Yamagiwa, Shinichi and Wada, Koichi},
  booktitle={2009 IEEE International Symposium on Parallel \& Distributed Processing},
  pages={1--8},
  year={2009},
  organization={IEEE}
}
@inproceedings{phull2012interference,
  title={Interference-driven resource management for GPU-based heterogeneous clusters},
  author={Phull, Rajat and Li, Cheng-Hong and Rao, Kunal and Cadambi, Hari and Chakradhar, Srimat},
  booktitle={Proceedings of the 21st international symposium on High-Performance Parallel and Distributed Computing},
  pages={109--120},
  year={2012}
}
@inproceedings{han2022microsecond,
  title={Microsecond-scale preemption for concurrent $\{$GPU-accelerated$\}$$\{$DNN$\}$ inferences},
  author={Han, Mingcong and Zhang, Hanze and Chen, Rong and Chen, Haibo},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={539--558},
  year={2022}
}
@inproceedings{li2022miso,
  title={Miso: exploiting multi-instance gpu capability on multi-tenant gpu clusters},
  author={Li, Baolin and Patel, Tirthak and Samsi, Siddharth and Gadepally, Vijay and Tiwari, Devesh},
  booktitle={Proceedings of the 13th Symposium on Cloud Computing},
  pages={173--189},
  year={2022}
}


@inproceedings{chen2017effisha,
  title={Effisha: A software framework for enabling effficient preemptive scheduling of gpu},
  author={Chen, Guoyang and Zhao, Yue and Shen, Xipeng and Zhou, Huiyang},
  booktitle={Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={3--16},
  year={2017}
}
@inproceedings{kim2020navigator,
  title={Navigator: Dynamic multi-kernel scheduling to improve GPU performance},
  author={Kim, Jiho and Kim, John and Park, Yongjun},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}
@article{alexopoulos2023nvshare,
  title  = {nvshare: Practical GPU Sharing without Memory Size Constraints},
  author = {Alexopoulos, Georgios and Mitropoulos, Dimitris},
  year   = {2023}
}

@article{mitzenmacher2001power,
  title     = {The power of two choices in randomized load balancing},
  author    = {Mitzenmacher, Michael},
  journal   = {IEEE Transactions on Parallel and Distributed Systems},
  volume    = {12},
  number    = {10},
  pages     = {1094--1104},
  year      = {2001},
  publisher = {IEEE}
}

@inproceedings{werner2018serverless,
  title={Serverless big data processing using matrix multiplication as example},
  author={Werner, Sebastian and Kuhlenkamp, J{\"o}rn and Klems, Markus and M{\"u}ller, Johannes and Tai, Stefan},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)},
  pages={358--365},
  year={2018},
  organization={IEEE}
}
@inproceedings{shankar2020serverless,
  title={Serverless linear algebra},
  author={Shankar, Vaishaal and Krauth, Karl and Vodrahalli, Kailas and Pu, Qifan and Recht, Benjamin and Stoica, Ion and Ragan-Kelley, Jonathan and Jonas, Eric and Venkataraman, Shivaram},
  booktitle={Proceedings of the 11th ACM Symposium on Cloud Computing},
  pages={281--295},
  year={2020}
}

@article{ebrahimi2024cold,
  title     = {Cold Start Latency Mitigation Mechanisms in Serverless Computing: Taxonomy, Review, and Future Directions},
  author    = {Ebrahimi, Ana and Ghobaei-Arani, Mostafa and Saboohi, Hadi},
  journal   = {Journal of Systems Architecture},
  pages     = {103115},
  year      = {2024},
  publisher = {Elsevier}
}
@article{vahidinia2022mitigating,
  title     = {Mitigating cold start problem in serverless computing: A reinforcement learning approach},
  author    = {Vahidinia, Parichehr and Farahani, Bahar and Aliee, Fereidoon Shams},
  journal   = {IEEE Internet of Things Journal},
  volume    = {10},
  number    = {5},
  pages     = {3917--3927},
  year      = {2022},
  publisher = {IEEE}
}

@article{kaffes2021practical,
  title   = {Practical scheduling for real-world serverless computing},
  author  = {Kaffes, Kostis and Yadwadkar, Neeraja J and Kozyrakis, Christos},
  journal = {arXiv preprint arXiv:2111.07226},
  year    = {2021}
}

@inproceedings{mccoy2024gallatin,
  title     = {Gallatin: A General-Purpose GPU Memory Manager},
  author    = {Mccoy, Hunter and Pandey, Prashant},
  booktitle = {Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
  pages     = {364--376},
  year      = {2024}
}
@article{strati2024orion,
  title  = {Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications},
  author = {Strati, Foteini and Ma, Xianzhe and Klimovic, Ana},
  year   = {2024}
}
@misc{cvetkovic2024dirigent,
  title         = {Dirigent: Lightweight Serverless Orchestration},
  author        = {Lazar Cvetkovi\'{c} and François Costa and Mihajlo Djokic and Michal Friedman and Ana Klimovic},
  year          = {2024},
  eprint        = {2404.16393},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC}
}
@inproceedings{serverless-cluster-cost,
  author    = {Cvetkovi\'{c}, Lazar and Fonseca, Rodrigo and Klimovic, Ana},
  title     = {Understanding the Neglected Cost of Serverless Cluster Management},
  year      = {2023},
  isbn      = {9798400702501},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3605181.3626286},
  doi       = {10.1145/3605181.3626286},
  abstract  = {Serverless computing enables the cloud platform to optimize resource management under the hood to improve performance and resource-efficiency. However, today's serverless cluster managers are designed by simply retrofitting legacy workload orchestration systems, despite the unique characteristics of serverless workloads. We study Knative-on-K8s as a representative state-of-the-art cluster manager for serverless and show that it can cause second-scale delays and contribute to over 65\% of end-to-end latency for function invocations experiencing cold starts. These overheads occur when the cluster experiences high sandbox churn, which is common in production serverless deployments. We analyze the root cause of current cluster manager overheads for serverless workloads and propose a set of design principles to improve end-to-end latency and peak throughput by rethinking the cluster manager system architecture.},
  booktitle = {Proceedings of the 4th Workshop on Resource Disaggregation and Serverless},
  pages     = {22–28},
  numpages  = {7},
  keywords  = {cloud computing, serverless computing, cluster management},
  location  = {Koblenz, Germany},
  series    = {WORDS '23}
}
@inproceedings{romero2021llama,
  title={Llama: A heterogeneous \& serverless framework for auto-tuning video analytics pipelines},
  author={Romero, Francisco and Zhao, Mark and Yadwadkar, Neeraja J and Kozyrakis, Christos},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={1--17},
  year={2021}
}
@article{risco2021gpu,
  title={GPU-enabled serverless workflows for efficient multimedia processing},
  author={Risco, Sebasti{\'a}n and Molt{\'o}, Germ{\'a}n},
  journal={Applied Sciences},
  volume={11},
  number={4},
  pages={1438},
  year={2021},
  publisher={MDPI}
}
@inproceedings{yang2022infless,
  title     = {INFless: a native serverless system for low-latency, high-throughput inference},
  author    = {Yang, Yanan and Zhao, Laiping and Li, Yiming and Zhang, Huanyu and Li, Jie and Zhao, Mingyang and Chen, Xingzhen and Li, Keqiu},
  booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {768--781},
  year      = {2022}
}
@article{ali2022optimizing,
  title={Optimizing inference serving on serverless platforms},
  author={Ali, Ahsan and Pinciroli, Riccardo and Yan, Feng and Smirni, Evgenia},
  journal={Proceedings of the VLDB Endowment},
  volume={15},
  number={10},
  year={2022}
}

@article{balaji2021fireplace,
  title   = {FirePlace: Placing Firecraker Virtual Machines with Hindsight Imitation},
  author  = {Balaji, Bharathan and Kakovitch, Christopher and Narayanaswamy, Balakrishnan},
  journal = {Proceedings of Machine Learning and Systems},
  volume  = {3},
  pages   = {652--663},
  year    = {2021}
}

@misc{pyhpc-bench,
  title        = {HPC benchmarks for Python},
  howpublished = {\url{https://github.com/dionhaefner/pyhpc-benchmarks}},
}
@inproceedings{che2009rodinia,
  title        = {Rodinia: A benchmark suite for heterogeneous computing},
  author       = {Che, Shuai and Boyer, Michael and Meng, Jiayuan and Tarjan, David and Sheaffer, Jeremy W and Lee, Sang-Ha and Skadron, Kevin},
  booktitle    = {2009 IEEE international symposium on workload characterization (IISWC)},
  pages        = {44--54},
  year         = {2009},
  organization = {Ieee}
}
@article{kumanov2018serverless,
  title   = {Serverless computing provides on-demand high performance computing for biomedical research},
  author  = {Kumanov, Dimitar and Hung, Ling-Hong and Lloyd, Wes and Yeung, Ka Yee},
  journal = {arXiv preprint arXiv:1807.11659},
  year    = {2018}
}
@article{hung2019rapid,
  title     = {Rapid RNA sequencing data analysis using serverless computing},
  author    = {Hung, Ling-Hong and Kumanov, Dimitar and Niu, Xingzhi and Lloyd, Wes and Yeung, Ka Yee},
  journal   = {bioRxiv},
  pages     = {576199},
  year      = {2019},
  publisher = {Cold Spring Harbor Laboratory}
}
@inproceedings{zhang2019video,
  title     = {Video processing with serverless computing: A measurement study},
  author    = {Zhang, Miao and Zhu, Yifei and Zhang, Cong and Liu, Jiangchuan},
  booktitle = {Proceedings of the 29th ACM workshop on network and operating systems support for digital audio and video},
  pages     = {61--66},
  year      = {2019}
}
@article{aytekin2019harnessing,
  title   = {Harnessing the power of serverless runtimes for large-scale optimization},
  author  = {Aytekin, Arda and Johansson, Mikael},
  journal = {arXiv preprint arXiv:1901.03161},
  year    = {2019}
}
@inproceedings{ao2018sprocket,
  title     = {Sprocket: A serverless video processing framework},
  author    = {Ao, Lixiang and Izhikevich, Liz and Voelker, Geoffrey M and Porter, George},
  booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
  pages     = {263--274},
  year      = {2018}
}
@inproceedings{duato2010rcuda,
  title        = {rCUDA: Reducing the number of GPU-based accelerators in high performance clusters},
  author       = {Duato, Jos{\'e} and Pena, Antonio J and Silla, Federico and Mayo, Rafael and Quintana-Ort{\'\i}, Enrique S},
  booktitle    = {2010 International Conference on High Performance Computing \& Simulation},
  pages        = {224--231},
  year         = {2010},
  organization = {IEEE}
}

@inproceedings{yu2017full,
  title     = {Full virtualization for gpus reconsidered},
  author    = {Yu, Hangchen and Rossbach, Christopher J},
  booktitle = {Proceedings of the Annual Workshop on Duplicating, Deconstructing, and Debunking},
  year      = {2017}
}

@inproceedings{yu2019automatic,
  title     = {Automatic virtualization of accelerators},
  author    = {Yu, Hangchen and Peters, Arthur M and Akshintala, Amogh and Rossbach, Christopher J},
  booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
  pages     = {58--65},
  year      = {2019}
}

@article{hong2017gpu,
  title     = {GPU virtualization and scheduling methods: A comprehensive survey},
  author    = {Hong, Cheol-Ho and Spence, Ivor and Nikolopoulos, Dimitrios S},
  journal   = {ACM Computing Surveys (CSUR)},
  volume    = {50},
  number    = {3},
  pages     = {1--37},
  year      = {2017},
  publisher = {ACM New York, NY, USA}
}

@article{naranjo2020accelerated,
  title     = {Accelerated serverless computing based on GPU virtualization},
  author    = {Naranjo, Diana M and Risco, Sebasti{\'a}n and de Alfonso, Carlos and P{\'e}rez, Alfonso and Blanquer, Ignacio and Molt{\'o}, Germ{\'a}n},
  journal   = {Journal of Parallel and Distributed Computing},
  volume    = {139},
  pages     = {32--42},
  year      = {2020},
  publisher = {Elsevier}
}
@misc{cuda-ctx-overhead,
  title        = {{How much device memory per device context creation.}},
  year         = {2016},
  howpublished = {\url{https://forums.developer.nvidia.com/t/predictable-how-much-device-memory-per-device-context-creation/41983/4}}
}
@misc{nvidia-mps,
  title        = {{Nvidia MPS}},
  year         = {2023},
  author    = {Nvidia},
  howpublished = {\url{https://docs.nvidia.com/deploy/mps/index.html}}
}
@misc{nvidia-container,
title = {{https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html}},
year = {2020},
howpublished = {https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html},
}
@misc{nvidia-smi,
  title        = {{System Management Interface SMI}},
  year         = {2024},
  author       = {Nvidia},
  howpublished = {\url{https://developer.nvidia.com/nvidia-system-management-interface}}
}

@misc{nvidia-mig,
  title        = {{NVIDIA Multi-Instance GPU User Guide}},
  year         = {2023},
  author    = {Nvidia},
  howpublished = {\url{https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html}}
}

@misc{nvidia-passthrough,
  title        = {{Virtual GPU Software User Guide}},
  year         = {2024},
  author    = {Nvidia},
  howpublished = {\url{https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html}}
}

@article{gimeno2022mlless,
  title={MLLess: Achieving Cost Efficiency in Serverless Machine Learning Training},
  author={Gimeno Sarroca, Pablo and S{\'a}nchez-Artigas, Marc},
  journal={arXiv e-prints},
  pages={arXiv--2206},
  year={2022}
}
@article{xu2021lambdadnn,
  title={$\lambda$dnn: Achieving predictable distributed DNN training with serverless architectures},
  author={Xu, Fei and Qin, Yiling and Chen, Li and Zhou, Zhi and Liu, Fangming},
  journal={IEEE Transactions on Computers},
  volume={71},
  number={2},
  pages={450--463},
  year={2021},
  publisher={IEEE}
}
@misc{l4tcontainer,
  title        = {{L4T Container}},
  year         = {2024},
  author       = {Nvidia},
  howpublished = {\url{https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-ml}}
}

@misc{jetson-specs,
  title        = {{Jetson AGX Orin - Specifications}},
  year         = {2024},
  author       = {Nvidia},
  howpublished = {\url{https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin}}
}

@misc{tegra-uvm,
  title        = {{CUDA for Tegra: Memory Management}},
  year         = {2024},
  author       = {Nvidia},
  howpublished = {\url{https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html#memory-management}}
}

@article{pemberton2022kernel,
  title={Kernel-as-a-Service: A Serverless Interface to GPUs},
  author={Pemberton, Nathan and Zabreyko, Anton and Ding, Zhoujie and Katz, Randy and Gonzalez, Joseph},
  journal={arXiv preprint arXiv:2212.08146},
  year={2022}
}

@inproceedings{hedayati2019multi,
  title     = {$\{$Multi-Queue$\}$ Fair Queuing},
  author    = {Hedayati, Mohammad and Shen, Kai and Scott, Michael L and Marty, Mike},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  pages     = {301--314},
  year      = {2019}
}

@article{fuerst2023iluvatar,
  title  = {Il{\'u}vatar: A Fast Control Plane for Serverless Computing},
  author = {Fuerst, Alexander and Rehman, Abdul and Sharma, Prateek},
  year   = {2023}
}

@inproceedings{fingler2022dgsf,
  title={Dgsf: Disaggregated gpus for serverless functions},
  author={Fingler, Henrique and Zhu, Zhiting and Yoon, Esther and Jia, Zhipeng and Witchel, Emmett and Rossbach, Christopher J},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={739--750},
  year={2022},
  organization={IEEE}
}

@inproceedings{gu2023fast,
  title={FaST-GShare: Enabling efficient spatio-temporal GPU sharing in serverless computing for deep learning inference},
  author={Gu, Jianfeng and Zhu, Yichao and Wang, Puxuan and Chadha, Mohak and Gerndt, Michael},
  booktitle={Proceedings of the 52nd International Conference on Parallel Processing},
  pages={635--644},
  year={2023}
}

@inproceedings{ng2023paella,
  title={Paella: Low-latency Model Serving with Software-defined GPU Scheduling},
  author={Ng, Kelvin KW and Demoulin, Henri Maxime and Liu, Vincent},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={595--610},
  year={2023}
}

@inproceedings{copik2023fmi,
  title={FMI: Fast and cheap message passing for serverless functions},
  author={Copik, Marcin and B{\"o}hringer, Roman and Calotoiu, Alexandru and Hoefler, Torsten},
  booktitle={Proceedings of the 37th International Conference on Supercomputing},
  pages={373--385},
  year={2023}
}

@inproceedings{yuan2022smpi,
  title={SMPI: Scalable Serverless MPI Computing},
  author={Yuan, Yuxin and Shi, Xiao and Lei, Zhengyu and Wang, Xiaohong and Zhao, Xiaofang},
  booktitle={2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)},
  pages={275--282},
  year={2022},
  organization={IEEE}
}


@InProceedings{	  10.1145/1807128.1807152,
  author	= {Cooper, Brian F. and Silberstein, Adam and Tam, Erwin and
		  Ramakrishnan, Raghu and Sears, Russell},
  title		= {Benchmarking Cloud Serving Systems with YCSB},
  year		= {2010},
  isbn		= {9781450300360},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/1807128.1807152},
  doi		= {10.1145/1807128.1807152},
  booktitle	= {Proceedings of the 1st ACM Symposium on Cloud Computing},
  pages		= {143–154},
  numpages	= {12},
  keywords	= {cloud serving database, benchmarking},
  location	= {Indianapolis, Indiana, USA},
  series	= {SoCC ’10}
}

###InProceedings{ 10.1145/1807128.1807152,
  author	= {Cooper, Brian F. and Silberstein, Adam and Tam, Erwin and
		  Ramakrishnan, Raghu and Sears, Russell},
  title		= {Benchmarking Cloud Serving Systems with YCSB},
  year		= {2010},
  isbn		= {9781450300360},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/1807128.1807152},
  doi		= {10.1145/1807128.1807152},
  booktitle	= {Proceedings of the 1st ACM Symposium on Cloud Computing},
  pages		= {143–154},
  numpages	= {12},
  keywords	= {cloud serving database, benchmarking},
  location	= {Indianapolis, Indiana, USA},
  series	= {SoCC ’10}
}

@InProceedings{	  abadi2016tensorflow,
  title		= {Tensorflow: A system for large-scale machine learning},
  author	= {Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and
		  Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin,
		  Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and
		  Isard, Michael and others},
  booktitle	= {12th {USENIX} symposium on operating systems design and
		  implementation ({OSDI}) 16)},
  pages		= {265--283},
  year		= {2016}
}

@Misc{		  acun_carbon_2022,
  title		= {Carbon {Explorer}: {A} {Holistic} {Approach} for
		  {Designing} {Carbon} {Aware} {Datacenters}},
  shorttitle	= {Carbon {Explorer}},
  url		= {http://arxiv.org/abs/2201.10036},
  abstract	= {Technology companies have been leading the way to a
		  renewable energy transformation, by investing in renewable
		  energy sources to reduce the carbon footprint of their
		  datacenters. In addition to helping build new solar and
		  wind farms, companies make power purchase agreements or
		  purchase carbon offsets, rather than relying on renewable
		  energy every hour of the day, every day of the week (24/7).
		  Relying on renewable energy 24/7 is challenging due to the
		  intermittent nature of wind and solar energy. Inherent
		  variations in solar and wind energy production causes
		  excess or lack of supply at different times. To cope with
		  the ﬂuctuations of renewable energy generation, multiple
		  solutions must be applied. These include: capacity sizing
		  with a mix of solar and wind power, energy storage options,
		  and carbon aware workload scheduling. However, depending on
		  the region and datacenter workload characteristics, the
		  carbon-optimal solution varies. Existing work in this space
		  does not give a holistic view of the trade-offs of each
		  solution and often ignore the embodied carbon cost of the
		  solutions.},
  language	= {en},
  urldate	= {2022-09-17},
  publisher	= {arXiv},
  author	= {Acun, Bilge and Lee, Benjamin and Kazhamiaka, Fiodar and
		  Maeng, Kiwan and Chakkaravarthy, Manoj and Gupta, Udit and
		  Brooks, David and Wu, Carole-Jean},
  month		= may,
  year		= {2022},
  note		= {arXiv:2201.10036 [cs, eess]},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Electrical Engineering and Systems Science -
		  Systems and Control, B.0, C.0},
  file		= {Acun et al. - 2022 - Carbon Explorer A Holistic Approach
		  for
		  Designing.pdf:/home/prateeks/Zotero/storage/IZEXQ8J4/Acun
		  et al. - 2022 - Carbon Explorer A Holistic Approach for
		  Designing.pdf:application/pdf}
}

@Article{	  acun_holistic_2022,
  title		= {A {Holistic} {Approach} for {Designing} {Carbon} {Aware}
		  {Datacenters}},
  url		= {http://arxiv.org/abs/2201.10036},
  abstract	= {Technology companies have been leading the way to a
		  renewable energy transformation, by investing in renewable
		  energy sources to reduce the carbon footprint of their
		  datacenters. In addition to helping build new solar and
		  wind farms, companies make power purchase agreements or
		  purchase carbon offsets, rather than relying on renewable
		  energy every hour of the day, every day of the week (24/7).
		  Relying on renewable energy 24/7 is challenging due to the
		  intermittent nature of wind and solar energy. Inherent
		  variations in solar and wind energy production causes
		  excess or lack of supply at different times. To cope with
		  the ﬂuctuations of renewable energy generation, multiple
		  solutions must be applied. These include: capacity sizing
		  with a mix of solar and wind power, energy storage options,
		  and carbon aware workload scheduling. However, depending on
		  the region and datacenter workload characteristics, the
		  carbonoptimal solution varies. Existing work in this space
		  does not give a holistic view of the trade-offs of each
		  solution and often ignore the embodied carbon cost of the
		  solutions.},
  language	= {en},
  urldate	= {2022-04-11},
  journal	= {arXiv:2201.10036 [cs, eess]},
  author	= {Acun, Bilge and Lee, Benjamin and Maeng, Kiwan and
		  Chakkaravarthy, Manoj and Gupta, Udit and Brooks, David and
		  Wu, Carole-Jean},
  month		= jan,
  year		= {2022},
  note		= {arXiv: 2201.10036},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Electrical Engineering and Systems Science -
		  Systems and Control, B.0, C.0},
  annote	= {- Facebook Carbon explorer: energy provisioning? Pareto
		  analysis. Hyperscale datacenters. Considers battery costs
		  etc - Renewables only, vs. R+Scheduling vs. R+batteries for
		  solar-rich locations. - Huge variance in renewable. As much
		  as 2.5x - Extra capacity + flexible workloads
		  -{\textgreater} most reduction in carbon },
  file		= {Acun et al. - 2022 - A Holistic Approach for Designing
		  Carbon Aware
		  Dat.pdf:/home/prateeks/Zotero/storage/72IUWCCL/Acun et al.
		  - 2022 - A Holistic Approach for Designing Carbon Aware
		  Dat.pdf:application/pdf}
}

@Article{	  aditya-rlcache,
  title		= {RL-Cache: Learning-based cache admission for content
		  delivery},
  author	= {Kirilin, Vadim and Sundarrajan, Aditya and Gorinsky,
		  Sergey and Sitaraman, Ramesh K},
  journal	= {IEEE Journal on Selected Areas in Communications},
  volume	= {38},
  number	= {10},
  pages		= {2372--2385},
  year		= {2020},
  publisher	= {IEEE}
}

@Article{	  aditya_fb_hotc22,
  title		= {Carbon Dependencies in Datacenter Design and Management},
  author	= {Bilge Acun, Benjamin C. Lee, Fiodar Kazhamiaka, Aditya
		  Sundarrajan, Manoj Chakkaravarthy, Kiwan Maeng, David
		  Brooks, Carole-Jean Wu},
  year		= {2002},
  journal	= {HotCarbon: Workshop on Sustainable Computer Systems Design
		  and Implementation}
}

@InProceedings{	  adzic2017serverless,
  title		= {Serverless computing: economic and architectural impact},
  author	= {Adzic, Gojko and Chatley, Robert},
  booktitle	= {Proceedings of the 2017 11th Joint Meeting on Foundations
		  of Software Engineering},
  pages		= {884--889},
  year		= {2017}
}

@Article{	  agarwal_redesigning_2021,
  title		= {Redesigning {Data} {Centers} for {Renewable} {Energy}},
  abstract	= {Renewable energy is becoming an important power source for
		  data centers, especially with the zero-carbon waste pledges
		  made by big cloud providers. However, one of the main
		  challenges of renewable energy sources is the high
		  variability of power produced. Traditional approaches such
		  as batteries or transmitting to the grid fall short on
		  scale, overhead, or "green-ness". We propose Virtual
		  Battery: instead of adapting the availability of power to
		  match the computation demand we shift computational demand
		  to meet the availability of power. Virtual batteries shift
		  demand by requiring applications to either be flexible and
		  delay-tolerant or proactively migrating to where power is
		  (going to be) available. We show that using multiple
		  virtual battery sites in combination can meet the needs of
		  modern applications. Moreover, we show how an intelligent
		  network and power aware co-scheduler can not only provide
		  availability despite variability but also help mitigate
		  migration related network overhead by over 30\% in total
		  and 4.2× at peak.},
  language	= {en},
  journal	= {HotNets},
  author	= {Agarwal, Anup and Sun, Jinghan and Noghabi, Shadi and
		  Iyengar, Srinivasan and Badam, Anirudh and Chandra, Ranveer
		  and Seshan, Srinivasan and Kalyanaraman, Shivkumar},
  year		= {2021},
  pages		= {8},
  file		= {Agarwal et al. - 2021 - Redesigning Data Centers for
		  Renewable
		  Energy.pdf:/home/prateeks/Zotero/storage/WNWC7WAY/Agarwal
		  et al. - 2021 - Redesigning Data Centers for Renewable
		  Energy.pdf:application/pdf}
}

###Article{	  agarwal_redesigning_2021,
  title		= {Redesigning {Data} {Centers} for {Renewable} {Energy}},
  abstract	= {Renewable energy is becoming an important power source for
		  data centers, especially with the zero-carbon waste pledges
		  made by big cloud providers. However, one of the main
		  challenges of renewable energy sources is the high
		  variability of power produced. Traditional approaches such
		  as batteries or transmitting to the grid fall short on
		  scale, overhead, or "green-ness". We propose Virtual
		  Battery: instead of adapting the availability of power to
		  match the computation demand we shift computational demand
		  to meet the availability of power. Virtual batteries shift
		  demand by requiring applications to either be flexible and
		  delay-tolerant or proactively migrating to where power is
		  (going to be) available. We show that using multiple
		  virtual battery sites in combination can meet the needs of
		  modern applications. Moreover, we show how an intelligent
		  network and power aware co-scheduler can not only provide
		  availability despite variability but also help mitigate
		  migration related network overhead by over 30\% in total
		  and 4.2× at peak.},
  language	= {en},
  journal	= {HotNets},
  author	= {Agarwal, Anup and Sun, Jinghan and Noghabi, Shadi and
		  Iyengar, Srinivasan and Badam, Anirudh and Chandra, Ranveer
		  and Seshan, Srinivasan and Kalyanaraman, Shivkumar},
  year		= {2021},
  pages		= {8},
  file		= {Agarwal et al. - 2021 - Redesigning Data Centers for
		  Renewable
		  Energy.pdf:/home/prateeks/Zotero/storage/WNWC7WAY/Agarwal
		  et al. - 2021 - Redesigning Data Centers for Renewable
		  Energy.pdf:application/pdf}
}

@Article{	  agmon2014rise,
  title		= {The rise of RaaS: the resource-as-a-service cloud},
  author	= {Agmon Ben-Yehuda, Orna and Ben-Yehuda, Muli and Schuster,
		  Assaf and Tsafrir, Dan},
  journal	= {Communications of the ACM},
  volume	= {57},
  number	= {7},
  pages		= {76--84},
  year		= {2014},
  publisher	= {ACM New York, NY, USA}
}

@InProceedings{	  akhtar_cose_2020,
  title		= {{COSE}: {Configuring} {Serverless} {Functions} using
		  {Statistical} {Learning}},
  shorttitle	= {{COSE}},
  doi		= {10.1109/INFOCOM41043.2020.9155363},
  abstract	= {Serverless computing has emerged as a new compelling
		  paradigm for the deployment of applications and services.
		  It represents an evolution of cloud computing with a
		  simplified programming model, that aims to abstract away
		  most operational concerns. Running serverless functions
		  requires users to configure multiple parameters, such as
		  memory, CPU, cloud provider, etc. While relatively simpler,
		  configuring such parameters correctly while minimizing cost
		  and meeting delay constraints is not trivial. In this
		  paper, we present COSE, a framework that uses Bayesian
		  Optimization to find the optimal configuration for
		  serverless functions. COSE uses statistical learning
		  techniques to intelligently collect samples and predict the
		  cost and execution time of a serverless function across
		  unseen configuration values. Our framework uses the
		  predicted cost and execution time, to select the "best"
		  configuration parameters for running a single or a chain of
		  functions, while satisfying customer objectives. In
		  addition, COSE has the ability to adapt to changes in the
		  execution time of a serverless function. We evaluate COSE
		  not only on a commercial cloud provider, where we
		  successfully found optimal/near-optimal configurations in
		  as few as five samples, but also over a wide range of
		  simulated distributed cloud environments that confirm the
		  efficacy of our approach.},
  booktitle	= {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer}
		  {Communications}},
  author	= {Akhtar, Nabeel and Raza, Ali and Ishakian, Vatche and
		  Matta, Ibrahim},
  month		= jul,
  year		= {2020},
  note		= {ISSN: 2641-9874},
  keywords	= {Cloud computing, Delays, Memory management, Performance
		  evaluation, Python, Statistical learning, Time factors},
  pages		= {129--138},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/T95B96W7/9155363.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/SXZG99I4/Akhtar et al. -
		  2020 - COSE Configuring Serverless Functions using
		  Stati.pdf:application/pdf}
}

@Article{	  akkus_sand_2018,
  title		= {{SAND}: {Towards} {High}-{Performance} {Serverless}
		  {Computing}},
  abstract	= {Serverless computing has emerged as a new cloud computing
		  paradigm, where an application consists of individual
		  functions that can be separately managed and executed.
		  However, existing serverless platforms normally isolate and
		  execute functions in separate containers, and do not
		  exploit the interactions among functions for performance.
		  These practices lead to high startup delays for function
		  executions and inefﬁcient resource usage.},
  language	= {en},
  journal	= {USENIX ATC},
  author	= {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and
		  Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya,
		  Paarijaat and Hilt, Volker},
  year		= {2018},
  pages		= {14},
  file		= {Akkus et al. - SAND Towards High-Performance Serverless
		  Computin.pdf:/home/prateeks/Zotero/storage/KBSRTIAB/Akkus
		  et al. - SAND Towards High-Performance Serverless
		  Computin.pdf:application/pdf}
}

###Article{	  akkus_sand_2018,
  title		= {{SAND}: {Towards} {High}-{Performance} {Serverless}
		  {Computing}},
  abstract	= {Serverless computing has emerged as a new cloud computing
		  paradigm, where an application consists of individual
		  functions that can be separately managed and executed.
		  However, existing serverless platforms normally isolate and
		  execute functions in separate containers, and do not
		  exploit the interactions among functions for performance.
		  These practices lead to high startup delays for function
		  executions and inefﬁcient resource usage.},
  language	= {en},
  journal	= {USENIX ATC},
  author	= {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and
		  Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya,
		  Paarijaat and Hilt, Volker},
  year		= {2018},
  pages		= {14},
  file		= {Akkus et al. - SAND Towards High-Performance Serverless
		  Computin.pdf:/home/prateeks/Zotero/storage/KBSRTIAB/Akkus
		  et al. - SAND Towards High-Performance Serverless
		  Computin.pdf:application/pdf}
}


@InProceedings{	  ali_batch_2020,
  address	= {Atlanta, GA, USA},
  title		= {{BATCH}: {Machine} {Learning} {Inference} {Serving} on
		  {Serverless} {Platforms} with {Adaptive} {Batching}},
  isbn		= {978-1-72819-998-6},
  shorttitle	= {{BATCH}},
  url		= {https://ieeexplore.ieee.org/document/9355312/},
  doi		= {10.1109/SC41405.2020.00073},
  abstract	= {Serverless computing is a new pay-per-use cloud service
		  paradigm that automates resource scaling for stateless
		  functions and can potentially facilitate bursty machine
		  learning serving. Batching is critical for latency
		  performance and costeffectiveness of machine learning
		  inference, but unfortunately it is not supported by
		  existing serverless platforms due to their stateless
		  design. Our experiments show that without batching, machine
		  learning serving cannot reap the beneﬁts of serverless
		  computing. In this paper, we present BATCH, a framework for
		  supporting efﬁcient machine learning serving on
		  serverless platforms. BATCH uses an optimizer to provide
		  inference tail latency guarantees and cost optimization and
		  to enable adaptive batching support. We prototype BATCH
		  atop of AWS Lambda and popular machine learning inference
		  systems. The evaluation veriﬁes the accuracy of the
		  analytic optimizer and demonstrates performance and cost
		  advantages over the state-of-the-art method MArk and the
		  state-of-the-practice tool SageMaker.},
  language	= {en},
  urldate	= {2021-07-19},
  booktitle	= {{SC20}: {International} {Conference} for {High}
		  {Performance} {Computing}, {Networking}, {Storage} and
		  {Analysis}},
  publisher	= {IEEE},
  author	= {Ali, Ahsan and Pinciroli, Riccardo and Yan, Feng and
		  Smirni, Evgenia},
  month		= nov,
  year		= {2020},
  pages		= {1--15},
  file		= {Ali et al. - 2020 - BATCH Machine Learning Inference
		  Serving on
		  Serve.pdf:/home/prateeks/Zotero/storage/4XIH9IF7/Ali et al.
		  - 2020 - BATCH Machine Learning Inference Serving on
		  Serve.pdf:application/pdf}
}
@Article{	  anand_hotcarbon22,
  title		= {{The Odd One Out: Energy is not like Other Metrics}},
  author	= {Anand, Vaastav and Xie, Zhiqiang and Stolet, Matheus and
		  De Viti, Roberta and Davidson, Thomas and Karimipour,
		  Reyhaneh and Alzayat, Safya and Mace, Jonathan},
  journal	= {HotCarbon 2022: 1st Workshop on Sustainable Computer
		  Systems Design and Implementation},
  year		= {2022},
  month		= {July}
}

@Article{	  anderson_treehouse_2022,
  title		= {Treehouse: {A} {Case} {For} {Carbon}-{Aware} {Datacenter}
		  {Software}},
  shorttitle	= {Treehouse},
  url		= {http://arxiv.org/abs/2201.02120},
  abstract	= {The end of Dennard scaling and the slowing of Moore’s
		  Law has put the energy use of datacenters on an
		  unsustainable path. Datacenters are already a signiﬁcant
		  fraction of worldwide electricity use, with application
		  demand scaling at a rapid rate. We argue that substantial
		  reductions in the carbon intensity of datacenter computing
		  are possible with a software-centric approach: by making
		  energy and carbon visible to application developers on a
		  ﬁne-grained basis, by modifying system APIs to make it
		  possible to make informed trade offs between performance
		  and carbon emissions, and by raising the level of
		  application programming to allow for ﬂexible use of more
		  energy efﬁcient means of compute and storage. We also lay
		  out a research agenda for systems software to reduce the
		  carbon footprint of datacenter computing.},
  language	= {en},
  urldate	= {2022-05-06},
  journal	= {arXiv:2201.02120 [cs]},
  author	= {Anderson, Thomas and Belay, Adam and Chowdhury, Mosharaf
		  and Cidon, Asaf and Zhang, Irene},
  month		= jan,
  year		= {2022},
  note		= {arXiv: 2201.02120},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Machine Learning, Computer
		  Science - Networking and Internet Architecture, Computer
		  Science - Computers and Society},
  file		= {Anderson et al. - 2022 - Treehouse A Case For Carbon-Aware
		  Datacenter
		  Soft.pdf:/home/prateeks/Zotero/storage/AXCFP2YU/Anderson et
		  al. - 2022 - Treehouse A Case For Carbon-Aware Datacenter
		  Soft.pdf:application/pdf}
}

@InProceedings{	  ao2022faasnap,
  title		= {FaaSnap: FaaS made fast using snapshot-based VMs},
  author	= {Ao, Lixiang and Porter, George and Voelker, Geoffrey M},
  booktitle	= {Proceedings of the Seventeenth European Conference on
		  Computer Systems},
  pages		= {730--746},
  year		= {2022}
}

###InProceedings{ ao2022faasnap,
  title		= {Faasnap: Faas made fast using snapshot-based vms},
  author	= {Ao, Lixiang and Porter, George and Voelker, Geoffrey M},
  booktitle	= {Proceedings of the Seventeenth European Conference on
		  Computer Systems},
  pages		= {730--746},
  year		= {2022}
}

@InProceedings{	  aquatope,
  author	= {Zhou, Zhuangzhuang and Zhang, Yanqi and Delimitrou,
		  Christina},
  title		= {AQUATOPE: QoS-and-Uncertainty-Aware Resource Management
		  for Multi-Stage Serverless Workflows},
  year		= {2022},
  isbn		= {9781450399159},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3567955.3567960},
  doi		= {10.1145/3567955.3567960},
  abstract	= {Multi-stage serverless applications, i.e., workflows with
		  many computation and I/O stages, are becoming increasingly
		  representative of FaaS platforms. Despite their advantages
		  in terms of fine-grained scalability and modular
		  development, these applications are subject to suboptimal
		  performance, resource inefficiency, and high costs to a
		  larger degree than previous simple serverless functions. We
		  present Aquatope, a QoS-and-uncertainty-aware resource
		  scheduler for end-to-end serverless workflows that takes
		  into account the inherent uncertainty present in FaaS
		  platforms, and improves performance predictability and
		  resource efficiency. Aquatope uses a set of scalable and
		  validated Bayesian models to create pre-warmed containers
		  ahead of function invocations, and to allocate appropriate
		  resources at function granularity to meet a complex
		  workflow's end-to-end QoS, while minimizing resource cost.
		  Across a diverse set of analytics and interactive
		  multi-stage serverless workloads, Aquatope significantly
		  outperforms prior systems, reducing QoS violations by 5X,
		  and cost by 34\% on average and up to 52\% compared to
		  other QoS-meeting methods.},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems, Volume 1},
  pages		= {1–14},
  numpages	= {14},
  keywords	= {function-as-a-service, serverless computing, resource
		  allocation, Cloud computing, resource efficiency, quality
		  of service, machine learning for systems, datacenter,
		  resource management},
  location	= {Vancouver, BC, Canada},
  series	= {ASPLOS 2023}
}

@Article{	  arora2012multiplicative,
  title		= {The multiplicative weights update method: a meta-algorithm
		  and applications},
  author	= {Arora, Sanjeev and Hazan, Elad and Kale, Satyen},
  journal	= {Theory of Computing},
  volume	= {8},
  number	= {1},
  pages		= {121--164},
  year		= {2012},
  publisher	= {Theory of Computing Exchange}
}

@InProceedings{	  atre2020caching,
  title		= {Caching with delayed hits},
  author	= {Atre, Nirav and Sherry, Justine and Wang, Weina and
		  Berger, Daniel S},
  booktitle	= {Proceedings of the Annual conference of the ACM Special
		  Interest Group on Data Communication on the applications,
		  technologies, architectures, and protocols for computer
		  communication},
  pages		= {495--513},
  year		= {2020}
}

@InProceedings{	  awamoto2020designing,
  title		= {Designing a Storage Software Stack for Accelerators},
  author	= {Awamoto, Shinichi and Focht, Erich and Honda, Michio},
  booktitle	= {12th USENIX Workshop on Hot Topics in Storage and File
		  Systems (HotStorage 20)},
  year		= {2020}
}

@Misc{		  aws-lambda,
  title		= {{AWS Lambda}},
  year		= {2020},
  howpublished	= {\url{https://aws.amazon.com/lambda/}}
}

@Misc{		  nvidia-uvm,
  title		= {{Unified Memory for CUDA Beginners}},
  year		= {2017},
  howpublished	= {\url{https://developer.nvidia.com/blog/unified-memory-cuda-beginners/}}
}

###Misc{	  aws-lambda,
  title		= {{AWS Lambda}},
  year		= {2020},
  howpublished	= {\url{https://aws.amazon.com/lambda/}}
}

@Misc{		  aws_carbon_tool,
  title		= {{Customer} {Carbon} {Footprint} {Tool} {\textbar} {AWS}
		  {News} {Blog}},
  url		= {https://aws.amazon.com/blogs/aws/new-customer-carbon-footprint-tool/},
  language	= {en-US},
  date		= {2022-10-12},
  month		= mar,
  year		= {2022},
  note		= {Section: Announcements},
  file		= {Snapshot:/home/prateeks/Zotero/storage/CW3EHK8K/new-customer-carbon-footprint-tool.html:text/html}
}

@InProceedings{	  azimi_powercoord_2018,
  address	= {Pittsburgh, PA, USA},
  title		= {{PowerCoord}: {A} {Coordinated} {Power} {Capping}
		  {Controller} for {Multi}-{CPU}/{GPU} {Servers}},
  isbn		= {978-1-5386-7466-6},
  shorttitle	= {{PowerCoord}},
  url		= {https://ieeexplore.ieee.org/document/8752132/},
  doi		= {10.1109/IGCC.2018.8752132},
  abstract	= {Modern supercomputers and cloud providers rely on server
		  nodes that are equipped with multiple CPU sockets and
		  general purpose GPUs (GPGPUs) to handle the high demand for
		  intensive computations. These servers consume much higher
		  power than commodity servers, and integrating them with
		  power capping systems used in modern clusters presents new
		  challenges. In this paper, we propose a new power capping
		  controller, PowerCoord, that is speciﬁcally designed for
		  servers with multiple CPU and GPU sockets that are running
		  multiple jobs at a time. PowerCoord coordinates among the
		  various power domains (e.g., CPU sockets and GPUs) inside a
		  server to meet target power caps, while seeking to maximize
		  throughput. Our approach also takes into consideration job
		  deadlines and priorities. Because performance modeling for
		  co-located jobs is error-prone, PowerCoord uses a learning
		  method to adapt to various workloads. PowerCoord has a
		  number of heuristic policies to allocate power among the
		  various CPUs and GPUs, and it uses reinforcement learning
		  to select the best policy during runtime. Based on the
		  observed state of the system, PowerCoord shifts the
		  distribution of selected policies. We implement our power
		  cap controller on a real multi-CPU/GPU server with low
		  overhead, and we demonstrate that it is able to meet target
		  power caps while maximizing the throughput, and balancing
		  other demands such as priorities and deadlines. Compared to
		  prior published techniques, our results show that
		  PowerCoord improves the throughput by an average of 14.4\%
		  under power caps.},
  language	= {en},
  urldate	= {2022-09-30},
  booktitle	= {2018 {Ninth} {International} {Green} and {Sustainable}
		  {Computing} {Conference} ({IGSC})},
  publisher	= {IEEE},
  author	= {Azimi, Reza and Jing, Chao and Reda, Sherief},
  month		= oct,
  year		= {2018},
  pages		= {1--9},
  file		= {Azimi et al. - 2018 - PowerCoord A Coordinated Power
		  Capping
		  Controller.pdf:/home/prateeks/Zotero/storage/QAWLSPZD/Azimi
		  et al. - 2018 - PowerCoord A Coordinated Power Capping
		  Controller.pdf:application/pdf}
}

@Misc{		  azure-functions,
  title		= {{Azure Functions}},
  year		= {2020},
  howpublished	= {\url{https://azure.microsoft.com/en-us/services/functions/
		  }}
}

###Misc{	  azure-functions,
  title		= {{Azure Functions}},
  year		= {2020},
  howpublished	= {\url{https://azure.microsoft.com/en-us/services/functions/
		  }}
}

@Misc{		  azure-iot-edge,
  title		= {Azure IoT Edge},
  howpublished	= {\url{https://azure.microsoft.com/en-us/services/iot-edge/}}
}

@Misc{		  azure_sustainability_calc,
  title		= {Microsoft {Sustainability} {Calculator} helps enterprises
		  analyze the carbon emissions of their {IT} infrastructure},
  howpublished	= {\url
		  {https://azure.microsoft.com/en-us/blog/microsoft-sustainability-calculator-helps-enterprises-analyze-the-carbon-emissions-of-their-it-infrastructure/}},
  abstract	= {For more than a decade, Microsoft has been investing to
		  reduce environmental impact while supporting the digital
		  transformation of organizations around the world through
		  cloud services.},
  language	= {en},
  date		= {2022-10-12},
  file		= {Snapshot:/home/prateeks/Zotero/storage/47UI3PX5/microsoft-sustainability-calculator-helps-enterprises-analyze-the-carbon-emissions-of-their-it-.html:text/html}
}

@InProceedings{	  babakol_calm_2020,
  address	= {Virtual Event USA},
  title		= {Calm energy accounting for multithreaded {Java}
		  applications},
  isbn		= {978-1-4503-7043-1},
  url		= {https://dl.acm.org/doi/10.1145/3368089.3409703},
  doi		= {10.1145/3368089.3409703},
  abstract	= {Energy accounting is a fundamental problem in energy
		  management, defined as attributing global energy
		  consumption to individual components of interest. In this
		  paper, we take on this problem at the application level,
		  where the components for accounting are application logical
		  units, such as methods, classes, and packages. Given a Java
		  application, our novel runtime system Chappie produces an
		  energy footprint, i.e., the relative energy consumption of
		  all programming abstraction units within the application.
		  The design of Chappie is unique in several dimensions.
		  First, relative to targeted energy profiling where the
		  profiler determines the energy consumption of a pre-defined
		  application logical unit, e.g., a specific method, Chappie
		  is total: the energy footprint encompasses all methods
		  within an application. Second, Chappie is
		  concurrency-aware: energy attribution is fully aware of the
		  multithreaded behavior of Java applications, including JVM
		  bookkeeping threads. Third, Chappie is an embodiment of a
		  novel philosophy for application-level energy accounting
		  and profiling, which states that the accounting run should
		  preserve the temporal phased power behavior of the
		  application, and the spatial power distribution among the
		  underlying hardware system. We term this important property
		  as calmness. Against state-of-the-art DaCapo benchmarks, we
		  show that the energy footprint generated by Chappie is
		  precise while incurring negligible overhead. In addition,
		  all results are produced with a high degree of calmness.},
  language	= {en},
  urldate	= {2022-07-08},
  booktitle	= {Proceedings of the 28th {ACM} {Joint} {Meeting} on
		  {European} {Software} {Engineering} {Conference} and
		  {Symposium} on the {Foundations} of {Software}
		  {Engineering}},
  publisher	= {ACM},
  author	= {Babakol, Timur and Canino, Anthony and Mahmoud, Khaled and
		  Saxena, Rachit and Liu, Yu David},
  month		= nov,
  year		= {2020},
  pages		= {976--988},
  file		= {Babakol et al. - 2020 - Calm energy accounting for
		  multithreaded Java
		  appl.pdf:/home/prateeks/Zotero/storage/CQVBAUCH/Babakol et
		  al. - 2020 - Calm energy accounting for multithreaded Java
		  appl.pdf:application/pdf}
}

###InProceedings{ babakol_calm_2020,
  address	= {Virtual Event USA},
  title		= {Calm energy accounting for multithreaded {Java}
		  applications},
  isbn		= {978-1-4503-7043-1},
  url		= {https://dl.acm.org/doi/10.1145/3368089.3409703},
  doi		= {10.1145/3368089.3409703},
  abstract	= {Energy accounting is a fundamental problem in energy
		  management, defined as attributing global energy
		  consumption to individual components of interest. In this
		  paper, we take on this problem at the application level,
		  where the components for accounting are application logical
		  units, such as methods, classes, and packages. Given a Java
		  application, our novel runtime system Chappie produces an
		  energy footprint, i.e., the relative energy consumption of
		  all programming abstraction units within the application.
		  The design of Chappie is unique in several dimensions.
		  First, relative to targeted energy profiling where the
		  profiler determines the energy consumption of a pre-defined
		  application logical unit, e.g., a specific method, Chappie
		  is total: the energy footprint encompasses all methods
		  within an application. Second, Chappie is
		  concurrency-aware: energy attribution is fully aware of the
		  multithreaded behavior of Java applications, including JVM
		  bookkeeping threads. Third, Chappie is an embodiment of a
		  novel philosophy for application-level energy accounting
		  and profiling, which states that the accounting run should
		  preserve the temporal phased power behavior of the
		  application, and the spatial power distribution among the
		  underlying hardware system. We term this important property
		  as calmness. Against state-of-the-art DaCapo benchmarks, we
		  show that the energy footprint generated by Chappie is
		  precise while incurring negligible overhead. In addition,
		  all results are produced with a high degree of calmness.},
  language	= {en},
  urldate	= {2022-09-11},
  booktitle	= {Proceedings of the 28th {ACM} {Joint} {Meeting} on
		  {European} {Software} {Engineering} {Conference} and
		  {Symposium} on the {Foundations} of {Software}
		  {Engineering}},
  publisher	= {ACM},
  author	= {Babakol, Timur and Canino, Anthony and Mahmoud, Khaled and
		  Saxena, Rachit and Liu, Yu David},
  month		= nov,
  year		= {2020},
  pages		= {976--988},
  file		= {Babakol et al. - 2020 - Calm energy accounting for
		  multithreaded Java
		  appl.pdf:/home/prateeks/Zotero/storage/FYUL58FD/Babakol et
		  al. - 2020 - Calm energy accounting for multithreaded Java
		  appl.pdf:application/pdf}
}

@InProceedings{	  babakol_eflect_2022,
  address	= {Pittsburgh Pennsylvania},
  title		= {Eflect: porting energy-aware applications to shared
		  environments},
  isbn		= {978-1-4503-9221-1},
  shorttitle	= {Eflect},
  url		= {https://dl.acm.org/doi/10.1145/3510003.3510145},
  doi		= {10.1145/3510003.3510145},
  abstract	= {Developing energy-aware applications is a well known
		  approach to software-based energy optimization. This
		  promising approach is however faced with a significant
		  hurdle when deployed to the environments shared among
		  multiple applications, where the energy consumption
		  effected by one application may erroneously be observed by
		  another application. We introduce Eflect, a novel software
		  framework for disentangling the energy consumption of
		  co-running applications. Our key idea, called energy
		  virtualization, enables each energy-aware application to be
		  only aware of the energy consumption effected by its
		  execution. Eflect is unique in its lightweight design: it
		  is a purely application-level solution that requires no
		  modification to the underlying hardware or system software.
		  Experiments show Eflect incurs low overhead with high
		  precision. Furthermore, it can seamlessly port existing
		  applicationlevel energy frameworks — one for
		  energy-adaptive approximation and the other for energy
		  profiling — to shared environments while retaining their
		  intended effectiveness.},
  language	= {en},
  urldate	= {2022-07-08},
  booktitle	= {Proceedings of the 44th {International} {Conference} on
		  {Software} {Engineering}},
  publisher	= {ACM},
  author	= {Babakol, Timur and Canino, Anthony and Liu, Yu David},
  month		= may,
  year		= {2022},
  pages		= {823--834},
  file		= {Babakol et al. - 2022 - Eflect porting energy-aware
		  applications to
		  share.pdf:/home/prateeks/Zotero/storage/2SW7T46H/Babakol et
		  al. - 2022 - Eflect porting energy-aware applications to
		  share.pdf:application/pdf}
}

@Article{	  banaei2021etas,
  title		= {ETAS: predictive scheduling of functions on worker nodes
		  of Apache OpenWhisk platform},
  author	= {Banaei, Ali and Sharifi, Mohsen},
  journal	= {The Journal of Supercomputing},
  pages		= {1--36},
  year		= {2021},
  publisher	= {Springer}
}

@InCollection{	  barolli_actual_2021,
  address	= {Cham},
  title		= {The {Actual} {Cost} of {Programmable} {SmartNICs}:
		  {Diving} into the {Existing} {Limits}},
  volume	= {225},
  isbn		= {978-3-030-75099-2 978-3-030-75100-5},
  shorttitle	= {The {Actual} {Cost} of {Programmable} {SmartNICs}},
  url		= {https://link.springer.com/10.1007/978-3-030-75100-5_17},
  abstract	= {Programmable Data Planes is a novel paradigm that enables
		  eﬃcient ofﬂoading of network applications. An important
		  enabler for this paradigm is the current available
		  SmartNICs, which should satisfy rigid network requirements
		  such as high throughput and low latency. Despite recent
		  research in this ﬁeld, not much attention was given to
		  understand the costs and limitations of oﬄoading network
		  applications into SmartNIC devices. Existing oﬄoading
		  approaches either neglect the existing limitations of
		  SmartNICs or assume that as an ongoing cost – leading,
		  therefore, to sub-optimal ofﬂoading approaches. In this
		  work, we conduct a comprehensive evaluation of SmartNICs in
		  order to quantify existing performance limitations. We
		  provide insights on network performance metrics such as
		  throughput and packet latency while considering diﬀerent
		  key building blocks of complex P4 programs (e.g.,
		  registers, cryptography functions, or packet
		  recirculation). Results show that line-rate throughput can
		  degrade up to 8x, while latency can increase as much as 80x
		  when performing memory-intensive operations in the data
		  plane.},
  language	= {en},
  urldate	= {2021-07-21},
  booktitle	= {Advanced {Information} {Networking} and {Applications}},
  publisher	= {Springer International Publishing},
  author	= {Viegas, Pablo B. and de Castro, Ariel G. and Lorenzon,
		  Arthur F. and Rossi, Fábio D. and Luizelli, Marcelo C.},
  editor	= {Barolli, Leonard and Woungang, Isaac and Enokido, Tomoya},
  year		= {2021},
  doi		= {10.1007/978-3-030-75100-5_17},
  note		= {Series Title: Lecture Notes in Networks and Systems},
  pages		= {181--194},
  file		= {Viegas et al. - 2021 - The Actual Cost of Programmable
		  SmartNICs Diving
		  .pdf:/home/prateeks/Zotero/storage/HDGWLTKY/Viegas et al. -
		  2021 - The Actual Cost of Programmable SmartNICs Diving
		  .pdf:application/pdf}
}

@Article{	  barroso2007case,
  title		= {The case for energy-proportional computing},
  author	= {Barroso, Luiz Andr{\'e} and H{\"o}lzle, Urs},
  journal	= {Computer},
  volume	= {40},
  number	= {12},
  pages		= {33--37},
  year		= {2007},
  publisher	= {IEEE}
}

@InProceedings{	  bashir_enabling_2021,
  address	= {Seattle WA USA},
  title		= {Enabling {Sustainable} {Clouds}: {The} {Case} for
		  {Virtualizing} the {Energy} {System}},
  isbn		= {978-1-4503-8638-8},
  shorttitle	= {Enabling {Sustainable} {Clouds}},
  url		= {https://dl.acm.org/doi/10.1145/3472883.3487009},
  doi		= {10.1145/3472883.3487009},
  abstract	= {Cloud platforms’ growing energy demand and carbon
		  emissions are raising concern about their environmental
		  sustainability. The current approach to enabling
		  sustainable clouds focuses on improving energy-e�ciency
		  and purchasing carbon o�sets. These approaches have
		  limits: many cloud data centers already operate near peak
		  e�ciency, and carbon o�sets cannot scale to near zero
		  carbon where there is little carbon left to o�set.
		  Instead, enabling sustainable clouds will require
		  applications to adapt to when and where unreliable
		  low-carbon energy is available. Applications cannot do this
		  today because their energy use and carbon emissions are not
		  visible to them, as the energy system provides the rigid
		  abstraction of a continuous, reliable energy supply. This
		  vision paper instead advocates for a “carbon �rst”
		  approach to cloud design that elevates carbon-e�ciency to
		  a �rst-class metric. To do so, we argue that cloud
		  platforms should virtualize the energy system by exposing
		  visibility into, and software-de�ned control of, it to
		  applications, enabling them to de�ne their own
		  abstractions for managing energy and carbon emissions based
		  on their own requirements.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the {ACM} {Symposium} on {Cloud}
		  {Computing}},
  publisher	= {ACM},
  author	= {Bashir, Noman and Guo, Tian and Hajiesmaili, Mohammad and
		  Irwin, David and Shenoy, Prashant and Sitaraman, Ramesh and
		  Souza, Abel and Wierman, Adam},
  month		= nov,
  year		= {2021},
  pages		= {350--358},
  annote	= {- Load shifting is key: Enabling sustainable clouds will
		  require applications to adapt to when and where unreliable
		  low-carbon energy is available. Move computation to energy.
		  Unreliability -{\textgreater} transient computing
		  techniques building block. - Jevons, efficiency, PUE=2 OK
		  if 0-carbon, - Exokernel approach to managing the complex
		  carbon/energy tradeoffs. Cmon, appliations cant do that.
		  The purpose of abstractions is to make it easy to do
		  things. Thus the first step should be a concrete
		  abstraction and /then/ break it if necessary. /HAS/ to be
		  within the OS! Visibility into the hw and apps! },
  file		= {Bashir et al. - 2021 - Enabling Sustainable Clouds The
		  Case for
		  Virtuali.pdf:/home/prateeks/Zotero/storage/U83GJM6J/Bashir
		  et al. - 2021 - Enabling Sustainable Clouds The Case for
		  Virtuali.pdf:application/pdf}
}

@InProceedings{	  basu2017adaptive,
  title		= {Adaptive TTL-based caching for content delivery},
  author	= {Basu, Soumya and Sundarrajan, Aditya and Ghaderi, Javad
		  and Shakkottai, Sanjay and Sitaraman, Ramesh},
  booktitle	= {Proceedings of the 2017 ACM SIGMETRICS/International
		  Conference on Measurement and Modeling of Computer
		  Systems},
  pages		= {45--46},
  year		= {2017}
}

@InProceedings{	  bellosa2000benefits,
  title		= {The benefits of event: driven energy accounting in
		  power-sensitive systems},
  author	= {Bellosa, Frank},
  booktitle	= {Proceedings of the 9th workshop on ACM SIGOPS European
		  workshop: beyond the PC: new challenges for the operating
		  system},
  pages		= {37--42},
  year		= {2000}
}

@InProceedings{	  bender1998flow,
  title		= {Flow and Stretch Metrics for Scheduling Continuous Job
		  Streams.},
  author	= {Bender, Michael A and Chakrabarti, Soumen and
		  Muthukrishnan, Sambavi},
  booktitle	= {SODA},
  volume	= {98},
  pages		= {270--279},
  year		= {1998}
}

@Article{	  bermbach_towards_2019,
  title		= {Towards {Auction}-{Based} {Function} {Placement} in
		  {Serverless} {Fog} {Platforms}},
  url		= {http://arxiv.org/abs/1912.06096},
  abstract	= {The Function-as-a-Service (FaaS) paradigm has a lot of
		  potential as a computing model for fog environments
		  comprising both cloud and edge nodes. When the request rate
		  exceeds capacity limits at the edge, some functions need to
		  be ofﬂoaded from the edge towards the cloud. In this
		  position paper, we propose an auction-based approach in
		  which application developers bid on resources. This allows
		  fog nodes to make a local decision about which functions to
		  ofﬂoad while maximizing revenue. For a ﬁrst evaluation
		  of our approach, we use simulation.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1912.06096 [cs]},
  author	= {Bermbach, David and Maghsudi, Setareh and Hasenburg,
		  Jonathan and Pfandzelter, Tobias},
  month		= dec,
  year		= {2019},
  note		= {arXiv: 1912.06096},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Networking and Internet
		  Architecture},
  annote	= {Comment: preprint},
  file		= {Bermbach et al. - 2019 - Towards Auction-Based Function
		  Placement in
		  Server.pdf:/home/prateeks/Zotero/storage/ZHS5KATE/Bermbach
		  et al. - 2019 - Towards Auction-Based Function Placement in
		  Server.pdf:application/pdf}
}

###Article{	  bermbach_towards_2019,
  title		= {Towards {Auction}-{Based} {Function} {Placement} in
		  {Serverless} {Fog} {Platforms}},
  url		= {http://arxiv.org/abs/1912.06096},
  abstract	= {The Function-as-a-Service (FaaS) paradigm has a lot of
		  potential as a computing model for fog environments
		  comprising both cloud and edge nodes. When the request rate
		  exceeds capacity limits at the edge, some functions need to
		  be ofﬂoaded from the edge towards the cloud. In this
		  position paper, we propose an auction-based approach in
		  which application developers bid on resources. This allows
		  fog nodes to make a local decision about which functions to
		  ofﬂoad while maximizing revenue. For a ﬁrst evaluation
		  of our approach, we use simulation.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1912.06096 [cs]},
  author	= {Bermbach, David and Maghsudi, Setareh and Hasenburg,
		  Jonathan and Pfandzelter, Tobias},
  month		= dec,
  year		= {2019},
  note		= {arXiv: 1912.06096},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Networking and Internet
		  Architecture},
  annote	= {Comment: preprint},
  file		= {Bermbach et al. - 2019 - Towards Auction-Based Function
		  Placement in
		  Server.pdf:/home/prateeks/Zotero/storage/ZHS5KATE/Bermbach
		  et al. - 2019 - Towards Auction-Based Function Placement in
		  Server.pdf:application/pdf}
}

@Article{	  bilal2021great,
  title		= {With Great Freedom Comes Great Opportunity: Rethinking
		  Resource Allocation for Serverless Functions},
  author	= {Bilal, Muhammad and Canini, Marco and Fonseca, Rodrigo and
		  Rodrigues, Rodrigo},
  journal	= {arXiv preprint arXiv:2105.14845},
  year		= {2021}
}

@InProceedings{	  brondolin_deep-mon_2018,
  title		= {{DEEP}-{Mon}: {Dynamic} and {Energy} {Efficient} {Power}
		  {Monitoring} for {Container}-{Based} {Infrastructures}},
  shorttitle	= {{DEEP}-{Mon}},
  doi		= {10.1109/IPDPSW.2018.00110},
  abstract	= {In the last few years energy efficiency of large scale
		  infrastructures gained a lot of attention, as power
		  consumption became one of the most impacting factors of the
		  operative costs of a data-center and of its Total Cost of
		  Ownership (TCO). Power consumption can be observed at
		  different layers of the data-center, from the overall power
		  grid, moving to each rack and arriving to each machine and
		  system. Given the rise of application containers both in
		  the cloud computing and High Performance Computing (HPC)
		  scenarios, it becomes more and more important to measure
		  power consumption also at the application level, where
		  power-aware schedulers and orchestrators can optimize the
		  execution of the workloads not only from a performance
		  perspective, but also considering performance/power
		  trade-offs. In this paper we propose DEEP-mon, a novel
		  monitoring tool able to measure power consumption and
		  attribute it for each thread and application container
		  running in the system. Moreover, we show how the proposed
		  approach has a negligible impact on the monitored system
		  and on the running workloads, overcoming the limitations of
		  the previous works in the field.},
  booktitle	= {2018 {IEEE} {International} {Parallel} and {Distributed}
		  {Processing} {Symposium} {Workshops} ({IPDPSW})},
  author	= {Brondolin, Rolando and Sardelli, Tommaso and Santambrogio,
		  Marco D.},
  month		= may,
  year		= {2018},
  keywords	= {Monitoring, Tools, Context, Data centers, Containers,
		  Power measurement, Power demand, application containers,
		  monitoring, power attribution, power awareness},
  pages		= {676--684},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/NGFBKVDX/8425477.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/2HC2ZZHC/Brondolin et al.
		  - 2018 - DEEP-Mon Dynamic and Energy Efficient Power
		  Monit.pdf:application/pdf}
}

@InProceedings{	  cadden_seuss_2020,
  address	= {Heraklion Greece},
  title		= {{SEUSS}: skip redundant paths to make serverless fast},
  isbn		= {978-1-4503-6882-7},
  shorttitle	= {{SEUSS}},
  url		= {https://dl.acm.org/doi/10.1145/3342195.3392698},
  doi		= {10.1145/3342195.3392698},
  abstract	= {This paper presents a system-level method for achieving
		  the rapid deployment and high-density caching of serverless
		  functions in a FaaS environment. For reduced start times,
		  functions are deployed from unikernel snapshots, bypassing
		  expensive initialization steps. To reduce the memory
		  footprint of snapshots we apply page-level sharing across
		  the entire software stack that is required to run a
		  function. We demonstrate the effects of our techniques by
		  replacing Linux on the compute node of a FaaS platform
		  architecture. With our prototype OS, the deployment time of
		  a function drops from 100s of milliseconds to under 10 ms.
		  Platform throughput improves by 51x on workload composed
		  entirely of new functions. We are able to cache over 50,
		  000 function instances in memory as opposed to 3, 000 using
		  standard OS techniques. In combination, these improvements
		  give the FaaS platform a new ability to handle large-scale
		  bursts of requests.},
  language	= {en},
  urldate	= {2020-08-23},
  booktitle	= {Proceedings of the {Fifteenth} {European} {Conference} on
		  {Computer} {Systems}},
  publisher	= {ACM},
  author	= {Cadden, James and Unger, Thomas and Awad, Yara and Dong,
		  Han and Krieger, Orran and Appavoo, Jonathan},
  month		= apr,
  year		= {2020},
  pages		= {1--15},
  file		= {Cadden et al. - 2020 - SEUSS skip redundant paths to make
		  serverless
		  fas.pdf:/home/prateeks/Zotero/storage/KTYTBCNU/Cadden et
		  al. - 2020 - SEUSS skip redundant paths to make serverless
		  fas.pdf:application/pdf}
}

@Article{	  cai_distilling_2022,
  title		= {Distilling the {Real} {Cost} of {Production} {Garbage}
		  {Collectors}},
  abstract	= {Despite the long history of garbage collection (GC) and
		  its prevalence in modern programming languages, there is
		  surprisingly little clarity about its true cost. Without
		  understanding their true cost, crucial tradeoffs made by
		  garbage collectors (GCs) often go unnoticed. This can lead
		  to misguided design constraints and evaluation criteria
		  used by GC researchers and users, hindering the development
		  of high-performance, low-cost GCs. In this paper, we
		  develop a methodology that allows us to empirically
		  estimate the absolute cost of GC for any given set of
		  metrics. This fundamental quantification has eluded the
		  research community, even when using modern,
		  well-established methodologies. By distilling out the
		  explicitly identifiable GC cost, we estimate the intrinsic
		  application execution cost using different GCs. The minimum
		  distilled cost forms a baseline. Subtracting this baseline
		  from the total execution costs, we can then place an
		  empirical lower bound on the absolute costs of different
		  GCs. Using this methodology, we study five production GCs
		  in OpenJDK 17, a high-performance Java runtime. We measure
		  the cost of these collectors, and expose their respective
		  key performance tradeoffs. We find that with a modestly
		  sized heap, production GCs incur substantial overheads
		  across a diverse suite of modern benchmarks, spending at
		  least 7–82 \% more wallclock time and 6–92 \% more CPU
		  cycles relative to the baseline cost. We show that these
		  costs can be masked by concurrency and generous
		  provisioning of memory/compute. In addition, we find that
		  newer low-pause GCs are significantly more expensive than
		  older GCs, and, surprisingly, sometimes deliver worse
		  application latency than stop-the-world GCs. Our findings
		  reaffirm that GC is by no means a solved problem and that a
		  low-cost, low-latency GC remains elusive. We recommend
		  adopting the distillation methodology together with a wider
		  range of cost metrics for future GC evaluations. This will
		  not only help the community more comprehensively understand
		  the performance characteristics of different GCs, but also
		  reveal opportunities for future GC optimizations.},
  language	= {en},
  journal	= {ISPASS},
  author	= {Cai, Zixian and Blackburn, Stephen M and Bond, Michael D
		  and Maas, Martin},
  year		= {2022},
  pages		= {12},
  file		= {Cai et al. - Distilling the Real Cost of Production
		  Garbage Col.pdf:/home/prateeks/Zotero/storage/LYLXMYYE/Cai
		  et al. - Distilling the Real Cost of Production Garbage
		  Col.pdf:application/pdf}
}

###Article{	  cai_distilling_2022,
  title		= {Distilling the {Real} {Cost} of {Production} {Garbage}
		  {Collectors}},
  abstract	= {Despite the long history of garbage collection (GC) and
		  its prevalence in modern programming languages, there is
		  surprisingly little clarity about its true cost. Without
		  understanding their true cost, crucial tradeoffs made by
		  garbage collectors (GCs) often go unnoticed. This can lead
		  to misguided design constraints and evaluation criteria
		  used by GC researchers and users, hindering the development
		  of high-performance, low-cost GCs. In this paper, we
		  develop a methodology that allows us to empirically
		  estimate the absolute cost of GC for any given set of
		  metrics. This fundamental quantification has eluded the
		  research community, even when using modern,
		  well-established methodologies. By distilling out the
		  explicitly identifiable GC cost, we estimate the intrinsic
		  application execution cost using different GCs. The minimum
		  distilled cost forms a baseline. Subtracting this baseline
		  from the total execution costs, we can then place an
		  empirical lower bound on the absolute costs of different
		  GCs. Using this methodology, we study five production GCs
		  in OpenJDK 17, a high-performance Java runtime. We measure
		  the cost of these collectors, and expose their respective
		  key performance tradeoffs. We find that with a modestly
		  sized heap, production GCs incur substantial overheads
		  across a diverse suite of modern benchmarks, spending at
		  least 7–82 \% more wallclock time and 6–92 \% more CPU
		  cycles relative to the baseline cost. We show that these
		  costs can be masked by concurrency and generous
		  provisioning of memory/compute. In addition, we find that
		  newer low-pause GCs are significantly more expensive than
		  older GCs, and, surprisingly, sometimes deliver worse
		  application latency than stop-the-world GCs. Our findings
		  reaffirm that GC is by no means a solved problem and that a
		  low-cost, low-latency GC remains elusive. We recommend
		  adopting the distillation methodology together with a wider
		  range of cost metrics for future GC evaluations. This will
		  not only help the community more comprehensively understand
		  the performance characteristics of different GCs, but also
		  reveal opportunities for future GC optimizations.},
  language	= {en},
  journal	= {ISPASS},
  author	= {Cai, Zixian and Blackburn, Stephen M and Bond, Michael D
		  and Maas, Martin},
  year		= {2022},
  pages		= {12},
  file		= {Cai et al. - Distilling the Real Cost of Production
		  Garbage Col.pdf:/home/prateeks/Zotero/storage/LYLXMYYE/Cai
		  et al. - Distilling the Real Cost of Production Garbage
		  Col.pdf:application/pdf}
}

@InProceedings{	  canino_proactive_2017,
  address	= {Barcelona Spain},
  title		= {Proactive and adaptive energy-aware programming with mixed
		  typechecking},
  isbn		= {978-1-4503-4988-8},
  url		= {https://dl.acm.org/doi/10.1145/3062341.3062356},
  doi		= {10.1145/3062341.3062356},
  abstract	= {Application-level energy management is an important
		  dimension of energy optimization. In this paper, we
		  introduce ENT, a novel programming language for enabling
		  proactive and adaptive mode-based energy management at the
		  application level. The proactive design allows programmers
		  to apply their application knowledge to energy management,
		  by characterizing the energy behavior of different program
		  fragments with modes. The adaptive design allows such
		  characterization to be delayed until run time, useful for
		  capturing dynamic program behavior dependent on program
		  states, conﬁguration settings, external battery levels,
		  or CPU temperatures. The key insight is both proactiveness
		  and adaptiveness can be uniﬁed under a type system
		  combined with static typing and dynamic typing. ENT has
		  been implemented as an extension to Java, and successfully
		  ported to three energy-conscious platforms: an Intel-based
		  laptop, a Raspberry Pi, and an Android phone. Evaluation
		  shows ENT improves the programmability, debuggability, and
		  energy efﬁciency of battery-aware and temperature-aware
		  programs.},
  language	= {en},
  urldate	= {2022-07-08},
  booktitle	= {Proceedings of the 38th {ACM} {SIGPLAN} {Conference} on
		  {Programming} {Language} {Design} and {Implementation}},
  publisher	= {ACM},
  author	= {Canino, Anthony and Liu, Yu David},
  month		= jun,
  year		= {2017},
  pages		= {217--232},
  file		= {Canino and Liu - 2017 - Proactive and adaptive
		  energy-aware programming
		  wi.pdf:/home/prateeks/Zotero/storage/N5BSTNXT/Canino and
		  Liu - 2017 - Proactive and adaptive energy-aware
		  programming wi.pdf:application/pdf}
}

@InProceedings{	  canino_toward_2019,
  address	= {Genova Italy},
  title		= {Toward a language design for energy prediction},
  isbn		= {978-1-4503-6257-3},
  url		= {https://dl.acm.org/doi/10.1145/3328433.3328445},
  doi		= {10.1145/3328433.3328445},
  abstract	= {Energy-aware programming languages and frameworks seek to
		  improve the energy efficiency of computer systems by taking
		  advantage of application-specific information to perform
		  energy optimizations. Effectively predicting an
		  application’s energy behavior enables more powerful
		  energy optimizations and additional energy management
		  techniques. However, application energy consumption is
		  fundamentally dynamic in nature, which limits the amount of
		  effective energy prediction that can be done ahead of time.
		  To address this challenge, we propose FJP, a
		  predication-aware semantics that takes the estimation
		  process online, and serves to partially predict a
		  program’s energy consumption. The key insight of FJP is
		  that by relaxing the restriction of statically and
		  completely predicting a programs energy consumption, we can
		  dynamically and partially predict energy consumption,
		  desirable for a host of hybrid energy management languages
		  and frameworks. FJP represents a first step toward
		  improving the expressiveness of energy-aware language and
		  framework techniques in its ability for online adaptive
		  energy prediction.},
  language	= {en},
  urldate	= {2022-07-08},
  booktitle	= {Proceedings of the {Conference} {Companion} of the 3rd
		  {International} {Conference} on {Art}, {Science}, and
		  {Engineering} of {Programming}},
  publisher	= {ACM},
  author	= {Canino, Anthony and Liu, Yu David},
  month		= apr,
  year		= {2019},
  pages		= {1--5},
  file		= {Canino and Liu - 2019 - Toward a language design for
		  energy
		  prediction.pdf:/home/prateeks/Zotero/storage/9BU6HWEU/Canino
		  and Liu - 2019 - Toward a language design for energy
		  prediction.pdf:application/pdf}
}

@InProceedings{	  cao_irani_1997,
  title		= {Cost-{Aware} {WWW} {Proxy} {Caching} {Algorithms}},
  abstract	= {Web caches can not only reduce network tra c and
		  downloading latency, but can also a ect the distribution of
		  web tra c over the network through costaware caching. This
		  paper introduces GreedyDualSize, which incorporates
		  locality with cost and size concerns in a simple and
		  non-parameterized fashion for high performance.
		  Trace-driven simulations show that with the appropriate
		  cost de nition, GreedyDualSize outperforms existing web
		  cache replacement algorithms in many aspects, including hit
		  ratios, latency reduction and network cost reduction. In
		  addition, GreedyDual-Size can potentially improve the
		  performance of main-memory caching of Web documents.},
  language	= {en},
  author	= {Cao, Pei and Irani, Sandy},
  pages		= {15},
  year		= {1997},
  booktitle	= {Proceedings of the USENIX Symposium on Internet
		  Technologies and Systems},
  file		= {Cao and Irani - Cost-Aware WWW Proxy Caching
		  Algorithms.pdf:/home/prateeks/Zotero/storage/Z9AZKPHJ/Cao
		  and Irani - Cost-Aware WWW Proxy Caching
		  Algorithms.pdf:application/pdf}
}

@Article{	  cardellini1999dynamic,
  title		= {Dynamic load balancing on web-server systems},
  author	= {Cardellini, Valeria and Colajanni, Michele and Yu, Philip
		  S},
  journal	= {IEEE Internet computing},
  volume	= {3},
  number	= {3},
  pages		= {28--39},
  year		= {1999},
  publisher	= {IEEE}
}

@InProceedings{	  carreira2018case,
  title		= {A case for serverless machine learning},
  author	= {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and
		  Zhang, Andrew and Katz, Randy},
  booktitle	= {Workshop on Systems for ML and Open Source Software at
		  NeurIPS},
  volume	= {2018},
  year		= {2018}
}

###InProceedings{ carreira2018case,
  title		= {A case for serverless machine learning},
  author	= {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and
		  Zhang, Andrew and Katz, Randy},
  booktitle	= {Workshop on Systems for ML and Open Source Software at
		  NeurIPS},
  volume	= {2018},
  year		= {2018}
}

@InProceedings{	  carreira2019cirrus,
  title		= {Cirrus: A serverless framework for end-to-end ml
		  workflows},
  author	= {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and
		  Zhang, Andrew and Katz, Randy},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {13--24},
  year		= {2019}
}

###InProceedings{ carreira2019cirrus,
  title		= {Cirrus: A serverless framework for end-to-end ml
		  workflows},
  author	= {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and
		  Zhang, Andrew and Katz, Randy},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {13--24},
  year		= {2019}
}

@InProceedings{	  carreira2021warm,
  title		= {From warm to hot starts: leveraging runtimes for the
		  serverless era},
  author	= {Carreira, Joao and Kohli, Sumer and Bruno, Rodrigo and
		  Fonseca, Pedro},
  booktitle	= {Proceedings of the Workshop on Hot Topics in Operating
		  Systems},
  pages		= {58--64},
  year		= {2021}
}

@InProceedings{	  carreira_cirrus_2019,
  address	= {Santa Cruz, CA, USA},
  title		= {Cirrus: a {Serverless} {Framework} for {End}-to-end {ML}
		  {Workflows}},
  isbn		= {978-1-4503-6973-2},
  shorttitle	= {Cirrus},
  url		= {http://dl.acm.org/citation.cfm?doid=3357223.3362711},
  doi		= {10.1145/3357223.3362711},
  abstract	= {Machine learning (ML) workflows are extremely complex. The
		  typical workflow consists of distinct stages of user
		  interaction, such as preprocessing, training, and tuning,
		  that are repeatedly executed by users but have
		  heterogeneous computational requirements. This complexity
		  makes it challenging for ML users to correctly provision
		  and manage resources and, in practice, constitutes a
		  significant burden that frequently causes over-provisioning
		  and impairs user productivity. Serverless computing is a
		  compelling model to address the resource management
		  problem, in general, but there are numerous challenges to
		  adopt it for existing ML frameworks due to significant
		  restrictions on local resources.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {ACM} {Symposium} on {Cloud}
		  {Computing} - {SoCC} '19},
  publisher	= {ACM Press},
  author	= {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and
		  Zhang, Andrew and Katz, Randy},
  year		= {2019},
  pages		= {13--24},
  file		= {Carreira et al. - 2019 - Cirrus a Serverless Framework for
		  End-to-end ML
		  W.pdf:/home/prateeks/Zotero/storage/BMERI3NP/Carreira et
		  al. - 2019 - Cirrus a Serverless Framework for End-to-end
		  ML W.pdf:application/pdf}
}

###InProceedings{ carreira_cirrus_2019,
  address	= {Santa Cruz, CA, USA},
  title		= {Cirrus: a {Serverless} {Framework} for {End}-to-end {ML}
		  {Workflows}},
  isbn		= {978-1-4503-6973-2},
  shorttitle	= {Cirrus},
  url		= {http://dl.acm.org/citation.cfm?doid=3357223.3362711},
  doi		= {10.1145/3357223.3362711},
  abstract	= {Machine learning (ML) workflows are extremely complex. The
		  typical workflow consists of distinct stages of user
		  interaction, such as preprocessing, training, and tuning,
		  that are repeatedly executed by users but have
		  heterogeneous computational requirements. This complexity
		  makes it challenging for ML users to correctly provision
		  and manage resources and, in practice, constitutes a
		  significant burden that frequently causes over-provisioning
		  and impairs user productivity. Serverless computing is a
		  compelling model to address the resource management
		  problem, in general, but there are numerous challenges to
		  adopt it for existing ML frameworks due to significant
		  restrictions on local resources.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {ACM} {Symposium} on {Cloud}
		  {Computing} - {SoCC} '19},
  publisher	= {ACM Press},
  author	= {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and
		  Zhang, Andrew and Katz, Randy},
  year		= {2019},
  pages		= {13--24},
  file		= {Carreira et al. - 2019 - Cirrus a Serverless Framework for
		  End-to-end ML
		  W.pdf:/home/prateeks/Zotero/storage/BMERI3NP/Carreira et
		  al. - 2019 - Cirrus a Serverless Framework for End-to-end
		  ML W.pdf:application/pdf}
}

###InProceedings{ carreira_cirrus_2019,
  address	= {Santa Cruz, CA, USA},
  title		= {Cirrus: a {Serverless} {Framework} for {End}-to-end {ML}
		  {Workflows}},
  isbn		= {978-1-4503-6973-2},
  shorttitle	= {Cirrus},
  url		= {http://dl.acm.org/citation.cfm?doid=3357223.3362711},
  doi		= {10.1145/3357223.3362711},
  abstract	= {Machine learning (ML) workflows are extremely complex. The
		  typical workflow consists of distinct stages of user
		  interaction, such as preprocessing, training, and tuning,
		  that are repeatedly executed by users but have
		  heterogeneous computational requirements. This complexity
		  makes it challenging for ML users to correctly provision
		  and manage resources and, in practice, constitutes a
		  significant burden that frequently causes over-provisioning
		  and impairs user productivity. Serverless computing is a
		  compelling model to address the resource management
		  problem, in general, but there are numerous challenges to
		  adopt it for existing ML frameworks due to significant
		  restrictions on local resources.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {ACM} {Symposium} on {Cloud}
		  {Computing} - {SoCC} '19},
  publisher	= {ACM Press},
  author	= {Carreira, Joao and Fonseca, Pedro and Tumanov, Alexey and
		  Zhang, Andrew and Katz, Randy},
  year		= {2019},
  pages		= {13--24},
  file		= {Carreira et al. - 2019 - Cirrus a Serverless Framework for
		  End-to-end ML
		  W.pdf:/home/prateeks/Zotero/storage/BMERI3NP/Carreira et
		  al. - 2019 - Cirrus a Serverless Framework for End-to-end
		  ML W.pdf:application/pdf}
}

@InProceedings{	  carroll2010analysis,
  title		= {An analysis of power consumption in a smartphone},
  author	= {Carroll, Aaron and Heiser, Gernot},
  booktitle	= {2010 USENIX Annual Technical Conference (USENIX ATC 10)},
  year		= {2010}
}

@InProceedings{	  cartas_reality_2019,
  address	= {Dresden, Germany},
  title		= {A {Reality} {Check} on {Inference} at {Mobile} {Networks}
		  {Edge}},
  isbn		= {978-1-4503-6275-7},
  url		= {http://dl.acm.org/citation.cfm?doid=3301418.3313946},
  doi		= {10.1145/3301418.3313946},
  abstract	= {Edge computing is considered a key enabler to deploy
		  Artificial Intelligence platforms to provide real-time
		  applications such as AR/VR or cognitive assistance.
		  Previous works show computing capabilities deployed very
		  close to the user can actually reduce the end-to-end
		  latency of such interactive applications. Nonetheless, the
		  main performance bottleneck remains in the machine learning
		  inference operation. In this paper, we question some
		  assumptions of these works, as the network location where
		  edge computing is deployed, and considered software
		  architectures within the framework of a couple of popular
		  machine learning tasks. Our experimental evaluation shows
		  that after performance tuning that leverages recent
		  advances in deep learning algorithms and hardware, network
		  latency is now the main bottleneck on end-to-end
		  application performance. We also report that deploying
		  computing capabilities at the first network node still
		  provides latency reduction but, overall, it is not required
		  by all applications. Based on our findings, we overview the
		  requirements and sketch the design of an adaptive
		  architecture for general machine learning inference across
		  edge locations.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 2nd {International} {Workshop} on
		  {Edge} {Systems}, {Analytics} and {Networking} - {EdgeSys}
		  '19},
  publisher	= {ACM Press},
  author	= {Cartas, Alejandro and Kocour, Martin and Raman, Aravindh
		  and Leontiadis, Ilias and Luque, Jordi and Sastry, Nishanth
		  and Nuñez-Martinez, Jose and Perino, Diego and Segura,
		  Carlos},
  year		= {2019},
  pages		= {54--59},
  file		= {Cartas et al. - 2019 - A Reality Check on Inference at
		  Mobile Networks
		  Ed.pdf:/home/prateeks/Zotero/storage/E4H3S9AE/Cartas et al.
		  - 2019 - A Reality Check on Inference at Mobile Networks
		  Ed.pdf:application/pdf}
}

###InProceedings{ cartas_reality_2019,
  address	= {Dresden, Germany},
  title		= {A {Reality} {Check} on {Inference} at {Mobile} {Networks}
		  {Edge}},
  isbn		= {978-1-4503-6275-7},
  url		= {http://dl.acm.org/citation.cfm?doid=3301418.3313946},
  doi		= {10.1145/3301418.3313946},
  abstract	= {Edge computing is considered a key enabler to deploy
		  Artificial Intelligence platforms to provide real-time
		  applications such as AR/VR or cognitive assistance.
		  Previous works show computing capabilities deployed very
		  close to the user can actually reduce the end-to-end
		  latency of such interactive applications. Nonetheless, the
		  main performance bottleneck remains in the machine learning
		  inference operation. In this paper, we question some
		  assumptions of these works, as the network location where
		  edge computing is deployed, and considered software
		  architectures within the framework of a couple of popular
		  machine learning tasks. Our experimental evaluation shows
		  that after performance tuning that leverages recent
		  advances in deep learning algorithms and hardware, network
		  latency is now the main bottleneck on end-to-end
		  application performance. We also report that deploying
		  computing capabilities at the first network node still
		  provides latency reduction but, overall, it is not required
		  by all applications. Based on our findings, we overview the
		  requirements and sketch the design of an adaptive
		  architecture for general machine learning inference across
		  edge locations.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 2nd {International} {Workshop} on
		  {Edge} {Systems}, {Analytics} and {Networking} - {EdgeSys}
		  '19},
  publisher	= {ACM Press},
  author	= {Cartas, Alejandro and Kocour, Martin and Raman, Aravindh
		  and Leontiadis, Ilias and Luque, Jordi and Sastry, Nishanth
		  and Nuñez-Martinez, Jose and Perino, Diego and Segura,
		  Carlos},
  year		= {2019},
  pages		= {54--59},
  file		= {Cartas et al. - 2019 - A Reality Check on Inference at
		  Mobile Networks
		  Ed.pdf:/home/prateeks/Zotero/storage/E4H3S9AE/Cartas et al.
		  - 2019 - A Reality Check on Inference at Mobile Networks
		  Ed.pdf:application/pdf}
}

@Article{	  carver_search_2019,
  title		= {In {Search} of a {Fast} and {Efficient} {Serverless} {DAG}
		  {Engine}},
  url		= {http://arxiv.org/abs/1910.05896},
  abstract	= {Python-written data analytics applications can be modeled
		  as and compiled into a directed acyclic graph (DAG) based
		  workﬂow, where the nodes are ﬁne-grained tasks and the
		  edges are task dependencies. Such analytics workﬂow jobs
		  are increasingly characterized by short, ﬁne-grained
		  tasks with large fan-outs. These characteristics make them
		  well-suited for a new cloud computing model called
		  serverless computing or Function-as-a-Service (FaaS), which
		  has become prevalent in recent years. The auto-scaling
		  property of serverless computing platforms accommodates
		  short tasks and bursty workloads, while the pay-per-use
		  billing model of serverless computing providers keeps the
		  cost of short tasks low.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1910.05896 [cs]},
  author	= {Carver, Benjamin and Zhang, Jingyuan and Wang, Ao and
		  Cheng, Yue},
  month		= oct,
  year		= {2019},
  note		= {arXiv: 1910.05896},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  annote	= {Comment: Appears at PDSW 2019},
  file		= {Carver et al. - 2019 - In Search of a Fast and Efficient
		  Serverless DAG
		  E.pdf:/home/prateeks/Zotero/storage/YPH9BW94/Carver et al.
		  - 2019 - In Search of a Fast and Efficient Serverless DAG
		  E.pdf:application/pdf}
}

###Article{	  carver_search_2019,
  title		= {In {Search} of a {Fast} and {Efficient} {Serverless} {DAG}
		  {Engine}},
  url		= {http://arxiv.org/abs/1910.05896},
  abstract	= {Python-written data analytics applications can be modeled
		  as and compiled into a directed acyclic graph (DAG) based
		  workﬂow, where the nodes are ﬁne-grained tasks and the
		  edges are task dependencies. Such analytics workﬂow jobs
		  are increasingly characterized by short, ﬁne-grained
		  tasks with large fan-outs. These characteristics make them
		  well-suited for a new cloud computing model called
		  serverless computing or Function-as-a-Service (FaaS), which
		  has become prevalent in recent years. The auto-scaling
		  property of serverless computing platforms accommodates
		  short tasks and bursty workloads, while the pay-per-use
		  billing model of serverless computing providers keeps the
		  cost of short tasks low.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1910.05896 [cs]},
  author	= {Carver, Benjamin and Zhang, Jingyuan and Wang, Ao and
		  Cheng, Yue},
  month		= oct,
  year		= {2019},
  note		= {arXiv: 1910.05896},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  annote	= {Comment: Appears at PDSW 2019},
  file		= {Carver et al. - 2019 - In Search of a Fast and Efficient
		  Serverless DAG
		  E.pdf:/home/prateeks/Zotero/storage/YPH9BW94/Carver et al.
		  - 2019 - In Search of a Fast and Efficient Serverless DAG
		  E.pdf:application/pdf}
}

@Article{	  castro2019rise,
  title		= {The rise of serverless computing},
  author	= {Castro, Paul and Ishakian, Vatche and Muthusamy, Vinod and
		  Slominski, Aleksander},
  journal	= {Communications of the ACM},
  volume	= {62},
  number	= {12},
  pages		= {44--54},
  year		= {2019},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  che2002hierarchical,
  title		= {Hierarchical web caching systems: Modeling, design and
		  experimental results},
  author	= {Che, Hao and Tung, Ye and Wang, Zhijun},
  journal	= {IEEE journal on Selected Areas in Communications},
  volume	= {20},
  number	= {7},
  pages		= {1305--1314},
  year		= {2002},
  publisher	= {IEEE}
}

###Article{	  che2002hierarchical,
  title		= {Hierarchical web caching systems: Modeling, design and
		  experimental results},
  author	= {Che, Hao and Tung, Ye and Wang, Zhijun},
  journal	= {IEEE journal on Selected Areas in Communications},
  volume	= {20},
  number	= {7},
  pages		= {1305--1314},
  year		= {2002},
  publisher	= {IEEE}
}

@Article{	  chen_cross_2020,
  title		= {Cross {Architectural} {Power} {Modelling}},
  url		= {http://arxiv.org/abs/2003.08305},
  abstract	= {Existing power modelling research focuses on the model
		  rather than the process for developing models. An automated
		  power modelling process that can be deployed on different
		  processors for developing power models with high accuracy
		  is developed. For this, (i) an automated hardware
		  performance counter selection method that selects counters
		  best correlated to power on both ARM and Intel processors,
		  (ii) a noise ﬁlter based on clustering that can reduce
		  the mean error in power models, and (iii) a two stage power
		  model that surmounts challenges in using existing power
		  models across multiple architectures are proposed and
		  developed. The key results are: (i) the automated hardware
		  performance counter selection method achieves comparable
		  selection to the manual method reported in the literature,
		  (ii) the noise ﬁlter reduces the mean error in power
		  models by up to 55\%, and (iii) the two stage power model
		  can predict dynamic power with less than 8\% error on both
		  ARM and Intel processors, which is an improvement over
		  classic models.},
  language	= {en},
  urldate	= {2022-04-11},
  journal	= {arXiv:2003.08305 [cs]},
  author	= {Chen, Kai and Kilpatrick, Peter and Nikolopoulos,
		  Dimitrios S. and Varghese, Blesson},
  month		= mar,
  year		= {2020},
  note		= {arXiv: 2003.08305},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  annote	= {Comment: 10 pages; IEEE/ACM CCGrid 2020. arXiv admin note:
		  text overlap with arXiv:1710.10325},
  file		= {Chen et al. - 2020 - Cross Architectural Power
		  Modelling.pdf:/home/prateeks/Zotero/storage/PRZFGCX2/Chen
		  et al. - 2020 - Cross Architectural Power
		  Modelling.pdf:application/pdf}
}

@InProceedings{	  cheng2000lru,
  title		= {LRU-SP: a size-adjusted and popularity-aware LRU
		  replacement algorithm for web caching},
  author	= {Cheng, Kai and Kambayashi, Yahiko},
  booktitle	= {Proceedings 24th Annual International Computer Software
		  and Applications Conference. COMPSAC2000},
  pages		= {48--53},
  year		= {2000},
  organization	= {IEEE}
}

@InProceedings{	  cherkasova2001role,
  title		= {Role of aging, frequency, and size in web cache
		  replacement policies},
  author	= {Cherkasova, Ludmila and Ciardo, Gianfranco},
  booktitle	= {International Conference on High-Performance Computing and
		  Networking},
  pages		= {114--123},
  year		= {2001},
  organization	= {Springer}
}

@Article{	  chien_driving_2021,
  title		= {Driving the cloud to true zero carbon},
  volume	= {64},
  issn		= {0001-0782, 1557-7317},
  url		= {https://dl.acm.org/doi/10.1145/3445037},
  doi		= {10.1145/3445037},
  language	= {en},
  number	= {2},
  urldate	= {2022-04-11},
  journal	= {Communications of the ACM},
  author	= {Chien, Andrew A.},
  month		= jan,
  year		= {2021},
  pages		= {5--5},
  file		= {Chien - 2021 - Driving the cloud to true zero
		  carbon.pdf:/home/prateeks/Zotero/storage/QA7YNYZ5/Chien -
		  2021 - Driving the cloud to true zero
		  carbon.pdf:application/pdf}
}

@InProceedings{	  choi2020lambda,
  title		= {$\lambda$-NIC: Interactive serverless compute on
		  programmable SmartNICs},
  author	= {Choi, Sean and Shahbaz, Muhammad and Prabhakar, Balaji and
		  Rosenblum, Mendel},
  booktitle	= {2020 IEEE 40th International Conference on Distributed
		  Computing Systems (ICDCS)},
  pages		= {67--77},
  year		= {2020},
  organization	= {IEEE}
}

@InProceedings{	  choi_roofline_2013,
  title		= {A {Roofline} {Model} of {Energy}},
  doi		= {10.1109/IPDPS.2013.77},
  abstract	= {We describe an energy-based analogue of the time-based
		  roofline model. We create this model from the perspective
		  of algorithm designers and performance tuners, with the
		  intent not of making exact predictions, but rather,
		  developing highlevel analytic insights into the possible
		  relationships among the time, energy, and power costs of an
		  algorithm. The model expresses algorithms in terms of
		  operations, concurrency, and memory traffic; and
		  characterizes the machine based on a small number of simple
		  cost parameters, namely, the time and energy costs per
		  operation or per word of communication. We confirm the
		  basic form of the model experimentally. From this model, we
		  suggest under what conditions we ought to expect an
		  algorithmic time-energy trade-off, and show how algorithm
		  properties may help inform power management.},
  booktitle	= {2013 {IEEE} 27th {International} {Symposium} on {Parallel}
		  and {Distributed} {Processing}},
  author	= {Choi, Jee Whan and Bedard, Daniel and Fowler, Robert and
		  Vuduc, Richard},
  month		= may,
  year		= {2013},
  note		= {ISSN: 1530-2075},
  keywords	= {Algorithm design and analysis, Analytical models,
		  computational intensity, Computational modeling, Current
		  measurement, Graphics processing units, machine balance,
		  performance analysis, power and energy modeling, Power
		  measurement, Prediction algorithms, roofline model},
  pages		= {661--672},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/W2HPJV43/6569852.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/C6WQR3A4/Choi et al. -
		  2013 - A Roofline Model of Energy.pdf:application/pdf}
}

@InProceedings{	  chou_dpm_2019,
  title		= {μ{DPM}: {Dynamic} {Power} {Management} for the
		  {Microsecond} {Era}},
  shorttitle	= {μ{DPM}},
  doi		= {10.1109/HPCA.2019.00032},
  abstract	= {The complex, distributed nature of data centers have
		  spawned the adoption of distributed, multi-tiered software
		  architectures, consisting of many inter-connected
		  microservices. These microservices exhibit extremely short
		  request service times, often less than 250μs. We show that
		  these “killer microsecond” service times can cause
		  state-of-the-art dynamic power management techniques to
		  break down, due to short idle period length and low power
		  state transition overheads. In this paper, we propose
		  μDPM, a dynamic power management scheme for the
		  microsecond era that coordinates request delaying, per-core
		  sleep states, and voltage frequency scaling. The idea is to
		  postpone the wake up of a CPU as long as possible and then
		  adjust the frequency so that the tail latency constraint of
		  requests are satisfied just-in-time. μDPM reduces
		  processor energy consumption by up to 32\% and consistently
		  outperforms state-of-the-art techniques by 2×.},
  booktitle	= {2019 {IEEE} {International} {Symposium} on {High}
		  {Performance} {Computer} {Architecture} ({HPCA})},
  author	= {Chou, Chih-Hsun and Bhuyan, Laxmi N. and Wong, Daniel},
  month		= feb,
  year		= {2019},
  note		= {ISSN: 2378-203X},
  keywords	= {Servers, Data centers, Energy consumption, Power demand,
		  Power system management, Market research, DVFS, Dynamic
		  power management, Linux, Sleep states},
  pages		= {120--132},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/3NBWSQES/8675218.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/LVMDBLL6/Chou et al. -
		  2019 - μDPM Dynamic Power Management for the
		  Microsecond.pdf:application/pdf}
}

@InProceedings{	  chrj-aaai21,
  title		= {Revisiting Consistent Hashing with Bounded Loads},
  author	= {Chen, John and Coleman, Benjamin and Shrivastava,
		  Anshumali},
  booktitle	= {Proceedings of the AAAI Conference on Artificial
		  Intelligence},
  volume	= {35},
  number	= {5},
  pages		= {3976--3983},
  year		= {2021}
}

@misc{nvml,
  title        = {NVIDIA Management Library},
  howpublished = {\url{https://developer.nvidia.com/nvidia-management-library-nvml}}
}

@Misc{		  cloudflare-workers,
  title		= {Cloudflare Workers},
  howpublished	= {\url{https://blog.cloudflare.com/introducing-cloudflare-workers/}}
}

@TechReport{	  clusterdata:reiss2011,
  author	= {Charles Reiss and John Wilkes and Joseph L. Hellerstein},
  title		= {{Google} cluster-usage traces: format + schema},
  institution	= {Google Inc.},
  year		= 2011,
  month		= nov,
  type		= {Technical Report},
  address	= {Mountain View, CA, USA},
  note		= {Revised 2014-11-17 for version 2.1. Posted at
		  \url{https://github.com/google/cluster-data}}
}

###TechReport{	  clusterdata:reiss2011,
  author	= {Charles Reiss and John Wilkes and Joseph L. Hellerstein},
  title		= {{Google} cluster-usage traces: format + schema},
  institution	= {Google Inc.},
  year		= 2011,
  month		= nov,
  type		= {Technical Report},
  address	= {Mountain View, CA, USA},
  note		= {Revised 2014-11-17 for version 2.1. Posted at
		  \url{https://github.com/google/cluster-data}}
}

@Article{	  colmant_next_2018,
  title		= {The next 700 {CPU} power models},
  volume	= {144},
  issn		= {01641212},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0164121218301377},
  doi		= {10.1016/j.jss.2018.07.001},
  abstract	= {Software power estimation of CPUs is a central concern for
		  energy eﬃciency and resource management in data centers.
		  Over the last few years, a dozen of ad hoc power models
		  have been proposed to cope with the wide diversity and the
		  growing complexity of modern CPU architectures. However,
		  most of these CPU power models rely on a thorough expertise
		  of the targeted architectures, thus leading to the design
		  of hardware-speciﬁc solutions that can hardly be ported
		  beyond the initial settings. In this article, we rather
		  propose a novel toolkit that uses a
		  conﬁgurable/interchangeable learning technique to
		  automatically learn the power model of a CPU, independently
		  of the features and the complexity it exhibits. In
		  particular, our learning approach automatically explores
		  the space of hardware performance counters made available
		  by a given CPU to isolate the ones that are best correlated
		  to the power consumption of the host, and then infers a
		  power model from the selected counters. Based on a
		  middleware toolkit devoted to the implementation of
		  software-deﬁned power meters, we implement the proposed
		  approach to generate CPU power models for a wide diversity
		  of CPU architectures (including Intel, ARM, and AMD
		  processors), and using a large variety of both CPU and
		  memoryintensive workloads. We show that the CPU power
		  models generated by our middleware toolkit estimate the
		  power consumption of the whole CPU or individual processes
		  with an accuracy of 98.5\% on average, thus competing with
		  the state-of-the-art power models.},
  language	= {en},
  urldate	= {2022-04-11},
  journal	= {Journal of Systems and Software},
  author	= {Colmant, Maxime and Rouvoy, Romain and Kurpicz, Mascha and
		  Sobe, Anita and Felber, Pascal and Seinturier, Lionel},
  month		= oct,
  year		= {2018},
  pages		= {382--396},
  file		= {Colmant et al. - 2018 - The next 700 CPU power
		  models.pdf:/home/prateeks/Zotero/storage/SJWFFQ4G/Colmant
		  et al. - 2018 - The next 700 CPU power
		  models.pdf:application/pdf}
}

@InProceedings{	  colmant_process-level_2015,
  address	= {Bordeaux France},
  title		= {Process-level power estimation in {VM}-based systems},
  isbn		= {978-1-4503-3238-5},
  url		= {https://dl.acm.org/doi/10.1145/2741948.2741971},
  doi		= {10.1145/2741948.2741971},
  abstract	= {Power estimation of software processes provides critical
		  indicators to drive scheduling or power capping heuristics.
		  State-of-the-art solutions can perform coarse-grained power
		  estimation in virtualized environments, typically treating
		  virtual machines (VMs) as a black box. Yet, VM-based
		  systems are nowadays commonly used to host multiple
		  applications for cost savings and better use of energy by
		  sharing common resources and assets.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the {Tenth} {European} {Conference} on
		  {Computer} {Systems}},
  publisher	= {ACM},
  author	= {Colmant, Maxime and Kurpicz, Mascha and Felber, Pascal and
		  Huertas, Loïc and Rouvoy, Romain and Sobe, Anita},
  month		= apr,
  year		= {2015},
  pages		= {1--14},
  file		= {Colmant et al. - 2015 - Process-level power estimation in
		  VM-based
		  systems.pdf:/home/prateeks/Zotero/storage/KBY8DRMZ/Colmant
		  et al. - 2015 - Process-level power estimation in VM-based
		  systems.pdf:application/pdf}
}

@Misc{		  containerd,
  title		= {{containerd: An industry-standard container runtime with
		  an emphasis on simplicity, robustness and portability}},
  howpublished	= {\url{https://containerd.io/}},
  year		= {2019}
}

###Misc{	  containerd,
  title		= {containerd},
  howpublished	= {\url{https://containerd.io/}}
}

###Misc{	  containerd,
  title		= {Containerd: An industry standard container runtime},
  howpublished	= {\url{https://containerd.io/}}
}

@InProceedings{	  copik_sebs_2021,
  address	= {Québec city Canada},
  title		= {{SeBS}: a serverless benchmark suite for
		  function-as-a-service computing},
  isbn		= {978-1-4503-8534-3},
  shorttitle	= {{SeBS}},
  url		= {https://dl.acm.org/doi/10.1145/3464298.3476133},
  doi		= {10.1145/3464298.3476133},
  abstract	= {Function-as-a-Service (FaaS) is one of the most promising
		  directions for the future of cloud services, and serverless
		  functions have immediately become a new middleware for
		  building scalable and cost-efficient applications. However,
		  the quickly moving technology hinders reproducibility, and
		  the lack of a standardized benchmarking suite leads to
		  using ad-hoc solutions and microbenchmarks in serverless
		  research, further complicating meta-analysis and comparison
		  of research solutions. To address this challenge, we
		  propose the Serverless Benchmark Suite: the benchmark for
		  FaaS computing that systematically covers a wide spectrum
		  of cloud resources and applications. Our benchmark consists
		  of the specification of representative workloads, the
		  accompanying implementation infrastructure, and the
		  evaluation methodology that facilitates reproducibility and
		  enables interpretability. We demonstrate that the abstract
		  model of a FaaS execution environment ensures the
		  applicability of our benchmark to multiple commercial
		  providers such as AWS, Azure, and Google Cloud. Our work
		  delivers a standardized, reliable, and evolving evaluation
		  methodology of performance, efficiency, scalability, and
		  reliability of middleware FaaS platforms.},
  language	= {en},
  urldate	= {2022-11-10},
  booktitle	= {Proceedings of the 22nd {International} {Middleware}
		  {Conference}},
  publisher	= {ACM},
  author	= {Copik, Marcin and Kwasniewski, Grzegorz and Besta, Maciej
		  and Podstawski, Michal and Hoefler, Torsten},
  month		= nov,
  year		= {2021},
  pages		= {64--78},
  file		= {Copik et al. - 2021 - SeBS a serverless benchmark suite
		  for
		  function-as.pdf:/home/prateeks/Zotero/storage/GBCVVVHL/Copik
		  et al. - 2021 - SeBS a serverless benchmark suite for
		  function-as.pdf:application/pdf}
}

@InProceedings{	  cortez2017resource,
  title		= {Resource central: Understanding and predicting workloads
		  for improved resource management in large cloud platforms},
  author	= {Cortez, Eli and Bonde, Anand and Muzio, Alexandre and
		  Russinovich, Mark and Fontoura, Marcus and Bianchini,
		  Ricardo},
  booktitle	= {Proceedings of the 26th Symposium on Operating Systems
		  Principles},
  pages		= {153--167},
  year		= {2017}
}

@InProceedings{	  counterstacks,
  title		= {Characterizing storage workloads with counter stacks},
  author	= {Wires, Jake and Ingram, Stephen and Drudi, Zachary and
		  Harvey, Nicholas JA and Warfield, Andrew},
  booktitle	= {11th {USENIX} Symposium on Operating Systems Design and
		  Implementation ({OSDI} 14)},
  pages		= {335--349},
  year		= 2014
}

@InProceedings{	  crankshaw2017clipper,
  title		= {Clipper: A low-latency online prediction serving system},
  author	= {Crankshaw, Daniel and Wang, Xin and Zhou, Guilio and
		  Franklin, Michael J and Gonzalez, Joseph E and Stoica,
		  Ion},
  booktitle	= {14th USENIX Symposium on Networked Systems Design and
		  Implementation (NSDI 17)},
  pages		= {613--627},
  year		= {2017}
}

@InProceedings{	  cremers_efficient_2022,
  address	= {Virtual Event},
  title		= {Efficient methods for approximating the shapley value for
		  asset sharing in energy communities},
  isbn		= {978-1-4503-9397-3},
  url		= {https://dl.acm.org/doi/10.1145/3538637.3538861},
  doi		= {10.1145/3538637.3538861},
  abstract	= {With the emergence of energy communities, where a number
		  of prosumers invest in shared renewable generation capacity
		  and battery storage, the issue of fair allocation of
		  benefits and costs has become increasingly important. The
		  Shapley value has attracted increasing interest for
		  redistribution in energy settings - however, computing it
		  exactly is intractable beyond a few dozen prosumers. In
		  this paper, we examine a number of methods for
		  approximating the Shapley value in realistic community
		  energy settings, and propose a new one. To compare the
		  performances of these methods, we also design a novel
		  method to compute the Shapley value exactly, for
		  communities of up to several hundred agents by clustering
		  consumers into a smaller number of demand profiles. We
		  compare the methods in a large-scale case study of a
		  community of up to 200 household consumers in the UK, and
		  show that our method can achieve very close redistribution
		  to the exact Shapley values but at a much lower (and
		  practically feasible) computation cost.},
  language	= {en},
  urldate	= {2022-09-11},
  booktitle	= {Proceedings of the {Thirteenth} {ACM} {International}
		  {Conference} on {Future} {Energy} {Systems}},
  publisher	= {ACM},
  author	= {Cremers, Sho and Robu, Valentin and Hofman, Daan and
		  Naber, Titus and Zheng, Kawin and Norbu, Sonam},
  month		= jun,
  year		= {2022},
  pages		= {320--324},
  file		= {Cremers et al. - 2022 - Efficient methods for
		  approximating the shapley
		  va.pdf:/home/prateeks/Zotero/storage/P8AXK3YK/Cremers et
		  al. - 2022 - Efficient methods for approximating the
		  shapley va.pdf:application/pdf}
}

@Misc{		  crun,
  title		= {{crun: A fast and low-memory footprint OCI Container
		  Runtime fully written in C.}},
  howpublished	= {\url{https://github.com/containers/crun}},
  year		= {2020}
}

###Article{	  crun,
  title		= {crun},
  howpublished	= {\url{https://github.com/containers/crun}}
}

@Article{	  dahlin2000interpreting,
  title		= {Interpreting stale load information},
  author	= {Dahlin, Michael},
  journal	= {IEEE Transactions on parallel and distributed systems},
  volume	= {11},
  number	= {10},
  pages		= {1033--1047},
  year		= {2000},
  publisher	= {IEEE}
}

@InProceedings{	  daw2021speedo,
  title		= {Speedo: Fast dispatch and orchestration of serverless
		  workflows},
  author	= {Daw, Nilanjan and Bellur, Umesh and Kulkarni,
		  Purushottam},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {585--599},
  year		= {2021}
}

@InProceedings{	  dean2017machine,
  title		= {Machine learning for systems and systems for machine
		  learning},
  author	= {Dean, Jeff},
  booktitle	= {Presentation at 2017 Conference on Neural Information
		  Processing Systems},
  year		= {2017}
}

@Article{	  decandia2007dynamo,
  title		= {Dynamo: Amazon's highly available key-value store},
  author	= {DeCandia, Giuseppe and Hastorun, Deniz and Jampani, Madan
		  and Kakulapati, Gunavardhan and Lakshman, Avinash and
		  Pilchin, Alex and Sivasubramanian, Swaminathan and
		  Vosshall, Peter and Vogels, Werner},
  journal	= {ACM SIGOPS operating systems review},
  volume	= {41},
  number	= {6},
  pages		= {205--220},
  year		= {2007},
  publisher	= {ACM New York, NY, USA}
}

@InProceedings{	  deflation-eurosys19,
  author	= {Sharma, Prateek and Ali-Eldin, Ahmed and Shenoy,
		  Prashant},
  title		= {Resource Deflation: A New Approach For Transient Resource
		  Reclamation},
  booktitle	= {Proceedings of the Fourteenth EuroSys Conference 2019},
  series	= {EuroSys '19},
  year		= {2019},
  isbn		= {978-1-4503-6281-8},
  location	= {Dresden, Germany},
  pages		= {33:1--33:17},
  articleno	= {33},
  numpages	= {17},
  url		= {http://doi.acm.org/10.1145/3302424.3303945},
  doi		= {10.1145/3302424.3303945},
  acmid		= {3303945},
  publisher	= {ACM},
  address	= {New York, NY, USA}
}

@InProceedings{	  desrochers_validation_2016,
  address	= {Alexandria VA USA},
  title		= {A {Validation} of {DRAM} {RAPL} {Power} {Measurements}},
  isbn		= {978-1-4503-4305-3},
  url		= {https://dl.acm.org/doi/10.1145/2989081.2989088},
  doi		= {10.1145/2989081.2989088},
  language	= {en},
  urldate	= {2022-04-21},
  booktitle	= {Proceedings of the {Second} {International} {Symposium} on
		  {Memory} {Systems}},
  publisher	= {ACM},
  author	= {Desrochers, Spencer and Paradis, Chad and Weaver, Vincent
		  M.},
  month		= oct,
  year		= {2016},
  pages		= {455--470},
  file		= {Desrochers et al. - 2016 - A Validation of DRAM RAPL Power
		  Measurements.pdf:/home/prateeks/Zotero/storage/GZWJY7EZ/Desrochers
		  et al. - 2016 - A Validation of DRAM RAPL Power
		  Measurements.pdf:application/pdf}
}

@Article{	  dichev_power_2022,
  title		= {Power {Log}’n’{Roll}: {Power}-{Efficient} {Localized}
		  {Rollback} for {MPI} {Applications} {Using} {Message}
		  {Logging} {Protocols}},
  volume	= {33},
  issn		= {1558-2183},
  shorttitle	= {Power {Log}’n’{Roll}},
  doi		= {10.1109/TPDS.2021.3107745},
  abstract	= {In fault tolerance for parallel and distributed systems,
		  message logging protocols have played a prominent role in
		  the last three decades. Such protocols enable local
		  rollback to provide recovery from fail-stop errors. Global
		  rollback techniques can be straightforward to implement but
		  at times lead to slower recovery than local rollback. Local
		  rollback is more complicated but can offer faster recovery
		  times. In this work, we study the power and energy
		  efficiency implications of global and local rollback. We
		  propose a power-efficient version of local rollback to
		  reduce power consumption for non-critical, blocked
		  processes, using Dynamic Voltage and Frequency Scaling
		  (DVFS) and clock modulation (CM). Our results for 3
		  different MPI codes on 2 parallel systems show that
		  power-efficient local rollback reduces CPU energy waste up
		  to 50\% during the recovery phase, compared to existing
		  global and local rollback techniques, without introducing
		  significant overheads. Furthermore, we show that savings
		  manifest for all blocked processes, which grow linearly
		  with the process count. We estimate that for settings with
		  high recovery overheads the total energy waste of parallel
		  codes is reduced with the proposed local rollback.},
  number	= {6},
  journal	= {IEEE Transactions on Parallel and Distributed Systems},
  author	= {Dichev, Kiril and De Sensi, Daniele and Nikolopoulos,
		  Dimitrios S. and Cameron, Kirk W. and Spence, Ivor},
  month		= jun,
  year		= {2022},
  note		= {Conference Name: IEEE Transactions on Parallel and
		  Distributed Systems},
  keywords	= {fail-stop errors, Fault tolerance, Fault tolerant systems,
		  local rollback, message logging, MPI, Payloads,
		  power/energy savings, Protocols, Resilience, Runtime,
		  Topology},
  pages		= {1276--1288},
  file		= {IEEE Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/7E8ERRB9/Dichev et al. -
		  2022 - Power Log’n’Roll Power-Efficient Localized
		  Rollba.pdf:application/pdf}
}

@Article{	  do_ptop_2009,
  title		= {{pTop}: {A} {Process}-level {Power} {Profiling} {Tool}},
  language	= {en},
  journal	= {HotPower},
  author	= {Do, Thanh and Rawshdeh, Suhib and Shi, Weisong},
  year		= {2009},
  pages		= {5},
  file		= {Do et al. - pTop A Process-level Power Proﬁling
		  Tool.pdf:/home/prateeks/Zotero/storage/C78B6NGK/Do et al. -
		  pTop A Process-level Power Proﬁling
		  Tool.pdf:application/pdf}
}

@Misc{		  docker-main,
  title		= {Docker},
  howpublished	= {\url{https://www.docker.com/}},
  year		= {2015},
  month		= {June}
}

###Misc{	  docker-main,
  title		= {Docker},
  howpublished	= {\url{https://www.docker.com/}},
  year		= {2015},
  month		= {June}
}

@InProceedings{	  dodge_measuring_2022,
  address	= {Seoul Republic of Korea},
  title		= {Measuring the {Carbon} {Intensity} of {AI} in {Cloud}
		  {Instances}},
  isbn		= {978-1-4503-9352-2},
  url		= {https://dl.acm.org/doi/10.1145/3531146.3533234},
  doi		= {10.1145/3531146.3533234},
  language	= {en},
  urldate	= {2022-09-30},
  booktitle	= {2022 {ACM} {Conference} on {Fairness}, {Accountability},
		  and {Transparency}},
  publisher	= {ACM},
  author	= {Dodge, Jesse and Prewitt, Taylor and Tachet des Combes,
		  Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma
		  and Luccioni, Alexandra Sasha and Smith, Noah A. and
		  DeCario, Nicole and Buchanan, Will},
  month		= jun,
  year		= {2022},
  pages		= {1877--1894},
  file		= {Full Text:/home/prateeks/Zotero/storage/FHVYNU4D/Dodge et
		  al. - 2022 - Measuring the Carbon Intensity of AI in Cloud
		  Inst.pdf:application/pdf}
}

@InProceedings{	  dong_rethink_2014,
  address	= {Maui Hawaii USA},
  title		= {Rethink energy accounting with cooperative game theory},
  isbn		= {978-1-4503-2783-1},
  url		= {https://dl.acm.org/doi/10.1145/2639108.2639128},
  doi		= {10.1145/2639108.2639128},
  abstract	= {Energy accounting determines how much a software principal
		  contributes to the total system energy consumption. It is
		  the foundation for evaluating software and for operating
		  system based energy management. While various energy
		  accounting policies have been tried, there is no known way
		  to evaluate them directly simply because it is hard to
		  track all hardware usage by software in a heterogeneous
		  multicore system like modern smartphones and tablets.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the 20th annual international conference on
		  {Mobile} computing and networking},
  publisher	= {ACM},
  author	= {Dong, Mian and Lan, Tian and Zhong, Lin},
  month		= sep,
  year		= {2014},
  pages		= {531--542},
  file		= {Dong et al. - 2014 - Rethink energy accounting with
		  cooperative game
		  th.pdf:/home/prateeks/Zotero/storage/SR8JMML5/Dong et al. -
		  2014 - Rethink energy accounting with cooperative game
		  th.pdf:application/pdf}
}

###InProceedings{ dong_rethink_2014,
  address	= {Maui Hawaii USA},
  title		= {Rethink energy accounting with cooperative game theory},
  isbn		= {978-1-4503-2783-1},
  url		= {https://dl.acm.org/doi/10.1145/2639108.2639128},
  doi		= {10.1145/2639108.2639128},
  abstract	= {Energy accounting determines how much a software principal
		  contributes to the total system energy consumption. It is
		  the foundation for evaluating software and for operating
		  system based energy management. While various energy
		  accounting policies have been tried, there is no known way
		  to evaluate them directly simply because it is hard to
		  track all hardware usage by software in a heterogeneous
		  multicore system like modern smartphones and tablets.},
  language	= {en},
  urldate	= {2022-07-08},
  booktitle	= {Proceedings of the 20th annual international conference on
		  {Mobile} computing and networking},
  publisher	= {ACM},
  author	= {Dong, Mian and Lan, Tian and Zhong, Lin},
  month		= sep,
  year		= {2014},
  pages		= {531--542},
  file		= {Dong et al. - 2014 - Rethink energy accounting with
		  cooperative game
		  th.pdf:/home/prateeks/Zotero/storage/NFEITVZA/Dong et al. -
		  2014 - Rethink energy accounting with cooperative game
		  th.pdf:application/pdf}
}

###InProceedings{ dong_rethink_2014,
  address	= {Maui Hawaii USA},
  title		= {Rethink energy accounting with cooperative game theory},
  isbn		= {978-1-4503-2783-1},
  url		= {https://dl.acm.org/doi/10.1145/2639108.2639128},
  doi		= {10.1145/2639108.2639128},
  abstract	= {Energy accounting determines how much a software principal
		  contributes to the total system energy consumption. It is
		  the foundation for evaluating software and for operating
		  system based energy management. While various energy
		  accounting policies have been tried, there is no known way
		  to evaluate them directly simply because it is hard to
		  track all hardware usage by software in a heterogeneous
		  multicore system like modern smartphones and tablets.},
  language	= {en},
  urldate	= {2022-09-30},
  booktitle	= {Proceedings of the 20th annual international conference on
		  {Mobile} computing and networking},
  publisher	= {ACM},
  author	= {Dong, Mian and Lan, Tian and Zhong, Lin},
  month		= sep,
  year		= {2014},
  pages		= {531--542},
  file		= {Dong et al. - 2014 - Rethink energy accounting with
		  cooperative game
		  th.pdf:/home/prateeks/Zotero/storage/4X9DB6K4/Dong et al. -
		  2014 - Rethink energy accounting with cooperative game
		  th.pdf:application/pdf}
}

@InProceedings{	  donkervliet2020towards,
  title		= {Towards supporting millions of users in modifiable virtual
		  environments by redesigning minecraft-like games as
		  serverless systems},
  author	= {Donkervliet, Jesse and Trivedi, Animesh and Iosup,
		  Alexandru},
  booktitle	= {12th USENIX Workshop on Hot Topics in Cloud Computing
		  (HotCloud 20)},
  year		= {2020}
}

@InProceedings{	  doppleganger-imc20,
  title		= {Using GANs for sharing networked time series data:
		  Challenges, initial promise, and open questions},
  author	= {Lin, Zinan and Jain, Alankar and Wang, Chen and Fanti,
		  Giulia and Sekar, Vyas},
  booktitle	= {Proceedings of the ACM Internet Measurement Conference},
  pages		= {464--483},
  year		= {2020}
}

@InProceedings{	  du2020catalyzer,
  title		= {Catalyzer: Sub-millisecond startup for serverless
		  computing with initialization-less booting},
  author	= {Du, Dong and Yu, Tianyi and Xia, Yubin and Zang, Binyu and
		  Yan, Guanglu and Qin, Chenggang and Wu, Qixuan and Chen,
		  Haibo},
  booktitle	= {Proceedings of the Twenty-Fifth International Conference
		  on Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {467--481},
  year		= {2020}
}

@inproceedings{du2022serverless,
  title={Serverless computing on heterogeneous computers},
  author={Du, Dong and Liu, Qingyuan and Jiang, Xueqiang and Xia, Yubin and Zang, Binyu and Chen, Haibo},
  booktitle={Proceedings of the 27th ACM international conference on architectural support for programming languages and operating systems},
  pages={797--813},
  year={2022}
}

@InProceedings{	  dukic2020photons,
  title		= {Photons: Lambdas on a diet},
  author	= {Dukic, Vojislav and Bruno, Rodrigo and Singla, Ankit and
		  Alonso, Gustavo},
  booktitle	= {Proceedings of the 11th ACM Symposium on Cloud Computing},
  pages		= {45--59},
  year		= {2020}
}

@InProceedings{	  eilam_towards_2021,
  address	= {Virtual Event Canada},
  title		= {Towards transparent and trustworthy cloud carbon
		  accounting},
  isbn		= {978-1-4503-9192-4},
  url		= {https://dl.acm.org/doi/10.1145/3501255.3501408},
  doi		= {10.1145/3501255.3501408},
  abstract	= {Climate Change is arguably the biggest challenge that
		  humanity faces today. Multiple trends such as the
		  exponential explosion of data transfer, the emergence and
		  popularity of power intensive workloads such as AI, and the
		  flattening of Moore’s law contribute to a rising concern
		  over the increasing carbon footprint cost of digital
		  computation. Any effective strategy to reduce the energy
		  consumption and associated carbon footprint of computations
		  must begin with an accurate and transparent quantification
		  method. However, while most businesses today run a
		  significant portion of their workloads on third party cloud
		  environments, transparent carbon quantification of tenant
		  workloads in cloud environments is lacking. This regretful
		  situation inhibits reliable reporting of Scope 3 Green
		  House Gas (GHG) by cloud users, meaningful comparison of
		  cloud carbon efficiencies, and measurable reduction
		  strategies. In this extended abstract we explain the unique
		  challenges that arise in multi-tenant cloud environments,
		  and propose and discuss an approach, consistent with the
		  GHG Protocol, for cloud carbon footprint quantification.
		  The quantification is a first step towards sustainable
		  cloud environments, that employ dynamic controllers to
		  quantify and reduce the carbon footprint at every layer of
		  the cloud stack.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the 22nd {International} {Middleware}
		  {Conference}: {Extended} {Abstracts}},
  publisher	= {ACM},
  author	= {Eilam, Tamar},
  month		= dec,
  year		= {2021},
  pages		= {1--5},
  file		= {Eilam - 2021 - Towards transparent and trustworthy cloud
		  carbon a.pdf:/home/prateeks/Zotero/storage/P4FEXT82/Eilam -
		  2021 - Towards transparent and trustworthy cloud carbon
		  a.pdf:application/pdf}
}

@Article{	  ein1985grosch,
  title		= {Grosch's law re-revisited: CPU power and the cost of
		  computation},
  author	= {Ein-Dor, Phillip},
  journal	= {Communications of the ACM},
  volume	= {28},
  number	= {2},
  pages		= {142--151},
  year		= {1985},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  einziger2017tinylfu,
  title		= {Tinylfu: A highly efficient cache admission policy},
  author	= {Einziger, Gil and Friedman, Roy and Manes, Ben},
  journal	= {ACM Transactions on Storage (ToS)},
  volume	= {13},
  number	= {4},
  pages		= {1--31},
  year		= {2017},
  publisher	= {ACM New York, NY, USA}
}

###Article{	  einziger2017tinylfu,
  title		= {Tinylfu: A highly efficient cache admission policy},
  author	= {Einziger, Gil and Friedman, Roy and Manes, Ben},
  journal	= {ACM Transactions on Storage (ToS)},
  volume	= {13},
  number	= {4},
  pages		= {1--31},
  year		= {2017},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  eismann2020serverless,
  title		= {Serverless applications: Why, when, and how?},
  author	= {Eismann, Simon and Scheuner, Joel and Van Eyk, Erwin and
		  Schwinger, Maximilian and Grohmann, Johannes and Herbst,
		  Nikolas and Abad, Cristina L and Iosup, Alexandru},
  journal	= {IEEE Software},
  volume	= {38},
  number	= {1},
  pages		= {32--39},
  year		= {2020},
  publisher	= {IEEE}
}

@InProceedings{	  eismann2021sizeless,
  title		= {Sizeless: Predicting the optimal size of serverless
		  functions},
  author	= {Eismann, Simon and Bui, Long and Grohmann, Johannes and
		  Abad, Cristina and Herbst, Nikolas and Kounev, Samuel},
  booktitle	= {Proceedings of the 22nd International Middleware
		  Conference},
  pages		= {248--259},
  year		= {2021}
}

@InProceedings{	  ensure-faas-acsos20,
  author	= {Suresh, Amoghavarsha and Somashekar, Gagan and
		  Varadarajan, Anandh and Kakarla, Veerendra Ramesh and
		  Upadhyay, Hima and Gandhi, Anshul},
  booktitle	= {2020 IEEE International Conference on Autonomic Computing
		  and Self-Organizing Systems (ACSOS)},
  title		= {ENSURE: Efficient Scheduling and Autonomous Resource
		  Management in Serverless Environments},
  year		= {2020},
  volume	= {},
  number	= {},
  pages		= {1-10},
  doi		= {10.1109/ACSOS49614.2020.00020}
}

@Article{	  ensure_acsos20,
  title		= {{ENSURE}: {Efficient} {Scheduling} and {Autonomous}
		  {Resource} {Management} in {Serverless} {Environments}},
  abstract	= {An imminent challenge in the serverless computing
		  landscape is the escalating cost of infrastructure needed
		  to handle the growing trafﬁc at scale. This work presents
		  ENSURE, a function-level scheduler and autonomous resource
		  manager designed to minimize provider resource costs while
		  meeting customer performance requirements. ENSURE works by
		  classifying incoming function requests at runtime and
		  carefully regulating the resource usage of colocated
		  functions on each invoker. Beyond a single invoker, ENSURE
		  elastically scales capacity, using concepts from operations
		  research, in response to varying workload trafﬁc to
		  prevent cold starts. Finally, ENSURE schedules requests by
		  concentrating load on an adequate number of invokers to
		  encourage reuse of active hosts (thus further avoiding cold
		  starts) and allow unneeded capacity to provably and
		  gracefully time out. We implement ENSURE on Apache
		  OpenWhisk and show that, across several serverless
		  applications and compared to existing baselines, ENSURE
		  signiﬁcantly improves resource efﬁciency, by as much as
		  52\%, while providing acceptable application latency.},
  language	= {en},
  author	= {Suresh, Amoghavarsha and Somashekar, Gagan and
		  Varadarajan, Anandh and Kakarla, Veerendra Ramesh and
		  Upadhyay, Hima and Gandhi, Anshu},
  pages		= {10},
  year		= {2020},
  booktitle	= {2020 IEEE International Conference on Autonomic Computing
		  and Self-Organizing Systems (ACSOS)},
  file		= {Suresh et al. - ENSURE Efficient Scheduling and Autonomous
		  Resour.pdf:/home/prateeks/Zotero/storage/WTNPXF79/Suresh et
		  al. - ENSURE Efficient Scheduling and Autonomous
		  Resour.pdf:application/pdf}
}

@InProceedings{	  eran2019nica,
  title		= {$\{$NICA$\}$: An infrastructure for inline acceleration of
		  network applications},
  author	= {Eran, Haggai and Zeno, Lior and Tork, Maroun and Malka,
		  Gabi and Silberstein, Mark},
  booktitle	= {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  pages		= {345--362},
  year		= {2019}
}

@Misc{		  esn-iu-news-22,
  title		= {{AI researchers to help Indiana manufacturers reduce
		  energy use}},
  author	= {IU News},
  howpublished	= {\url{https://news.iu.edu/live/news/27989-ai-researchers-to-help-indiana-manufacturers}},
  date		= {July 2022}
}

@Article{	  faas-survey-jan-2022,
  title		= {The Serverless Computing Survey: A Technical Primer for
		  Design Architecture},
  issn		= {1557-7341},
  url		= {http://dx.doi.org/10.1145/3508360},
  doi		= {10.1145/3508360},
  journal	= {ACM Computing Surveys},
  publisher	= {Association for Computing Machinery (ACM)},
  author	= {Li, Zijun and Guo, Linsong and Cheng, Jiagan and Chen,
		  Quan and He, BingSheng and Guo, Minyi},
  year		= {2022},
  month		= {Jan}
}

@InProceedings{	  faascache-asplos21,
  author	= {Fuerst, Alexander and Sharma, Prateek},
  title		= {FaasCache: Keeping Serverless Computing Alive with
		  Greedy-Dual Caching},
  year		= {2021},
  isbn		= {9781450383172},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3445814.3446757},
  doi		= {10.1145/3445814.3446757},
  abstract	= {Functions as a Service (also called serverless computing)
		  promises to revolutionize how applications use cloud
		  resources. However, functions suffer from cold-start
		  problems due to the overhead of initializing their code and
		  data dependencies before they can start executing. Keeping
		  functions alive and warm after they have finished execution
		  can alleviate the cold-start overhead. Keep-alive policies
		  must keep functions alive based on their resource and usage
		  characteristics, which is challenging due to the diversity
		  in FaaS workloads. Our insight is that keep-alive is
		  analogous to caching. Our caching-inspired Greedy-Dual
		  keep-alive policy can be effective in reducing the
		  cold-start overhead by more than 3\texttimes{} compared to
		  current approaches. Caching concepts such as reuse
		  distances and hit-ratio curves can also be used for
		  auto-scaled server resource provisioning, which can reduce
		  the resource requirement of FaaS providers by 30% for
		  real-world dynamic workloads. We implement caching-based
		  keep-alive and resource provisioning policies in our
		  FaasCache system, which is based on OpenWhisk. We hope that
		  our caching analogy opens the door to more principled and
		  optimized keep-alive and resource provisioning techniques
		  for future FaaS workloads and platforms.},
  booktitle	= {Proceedings of the 26th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {386--400},
  numpages	= {15},
  keywords	= {Functions as a Service, Caching, Serverless Computing,
		  Cloud Computing},
  location	= {Virtual, USA},
  series	= {ASPLOS 2021}
}

###InProceedings{ faascache-asplos21,
  author	= {Fuerst, Alexander and Sharma, Prateek},
  title		= {FaasCache: Keeping Serverless Computing Alive with
		  Greedy-Dual Caching},
  year		= {2021},
  isbn		= {9781450383172},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3445814.3446757},
  doi		= {10.1145/3445814.3446757},
  abstract	= {Functions as a Service (also called serverless computing)
		  promises to revolutionize how applications use cloud
		  resources. However, functions suffer from cold-start
		  problems due to the overhead of initializing their code and
		  data dependencies before they can start executing. Keeping
		  functions alive and warm after they have finished execution
		  can alleviate the cold-start overhead. Keep-alive policies
		  must keep functions alive based on their resource and usage
		  characteristics, which is challenging due to the diversity
		  in FaaS workloads. Our insight is that keep-alive is
		  analogous to caching. Our caching-inspired Greedy-Dual
		  keep-alive policy can be effective in reducing the
		  cold-start overhead by more than 3\texttimes{} compared to
		  current approaches. Caching concepts such as reuse
		  distances and hit-ratio curves can also be used for
		  auto-scaled server resource provisioning, which can reduce
		  the resource requirement of FaaS providers by 30% for
		  real-world dynamic workloads. We implement caching-based
		  keep-alive and resource provisioning policies in our
		  FaasCache system, which is based on OpenWhisk. We hope that
		  our caching analogy opens the door to more principled and
		  optimized keep-alive and resource provisioning techniques
		  for future FaaS workloads and platforms.},
  booktitle	= {Proceedings of the 26th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {386--400},
  numpages	= {15},
  keywords	= {Functions as a Service, Caching, Serverless Computing,
		  Cloud Computing},
  location	= {Virtual, USA},
  series	= {ASPLOS 2021}
}

@InProceedings{	  faaslb-hpdc22,
  author	= {Fuerst, Alexander and Sharma, Prateek},
  title		= {{Locality-aware Load-Balancing For Serverless Clusters}},
  year		= {2022},
  isbn		= {978-1-4503-9199-3/22/06},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  doi		= {10.1145/3502181.3531459},
  booktitle	= {Proceedings of the 31st International Symposium on
		  High-Performance Parallel and Distributed Computing},
  numpages	= {15},
  location	= {Minneapolis, USA},
  series	= {HPDC 2022}
}

@Article{	  fahad_accurate_2020,
  title		= {Accurate {Energy} {Modelling} of {Hybrid} {Parallel}
		  {Applications} on {Modern} {Heterogeneous} {Computing}
		  {Platforms} {Using} {System}-{Level} {Measurements}},
  volume	= {8},
  issn		= {2169-3536},
  doi		= {10.1109/ACCESS.2020.2994953},
  abstract	= {Modern high-performance computing platforms, cloud
		  computing systems, and data centers are highly
		  heterogeneous containing nodes where a multicore CPU is
		  tightly integrated with accelerators. An important
		  challenge for energy optimization of hybrid parallel
		  applications on such platforms is how to accurately
		  estimate the energy consumption of application components
		  running on different compute devices of the platform. In
		  this work, we propose a method for accurate estimation of
		  the application component-level energy consumption
		  employing system-level power measurements with power
		  meters. We experimentally validate the method on a cluster
		  of two hybrid heterogeneous computing nodes using three
		  parallel applications - matrix-matrix multiplication, 2D
		  fast Fourier transform and gene sequencing. The experiments
		  demonstrate a high estimation accuracy of the proposed
		  method, with the average estimation error ranging between
		  2\% and 5\%. The average error demonstrated by the
		  state-of-the-art estimation methods for the same
		  experimental setup ranges from 15\% to 75\%, while the
		  maximum reaches 178\%. We also show that the use of the
		  state-of-the-art estimation methods instead of the proposed
		  one in the energy optimization loop leads to significant
		  energy losses (up to 45\% in our case).},
  journal	= {IEEE Access},
  author	= {Fahad, Muhammad and Shahid, Arsalan and Manumachu, Ravi
		  Reddy and Lastovetsky, Alexey},
  year		= {2020},
  note		= {Conference Name: IEEE Access},
  keywords	= {Computational modeling, Optimization, Energy consumption,
		  Sensors, Energy measurement, Multicore processing, Energy
		  modelling, energy optimization, GPU, heterogeneous
		  platforms, HPC, Intel Xeon Phi, multicore CPU, on-chip
		  power sensors, parallel applications, power meters,
		  System-on-chip},
  pages		= {93793--93829},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/AW5RBNM7/9094309.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/A9PVU2W3/Fahad et al. -
		  2020 - Accurate Energy Modelling of Hybrid Parallel
		  Appli.pdf:application/pdf}
}

@InProceedings{	  fan2021deep,
  title		= {Deep reinforcement agent for scheduling in HPC},
  author	= {Fan, Yuping and Lan, Zhiling and Childers, Taylor and
		  Rich, Paul and Allcock, William and Papka, Michael E},
  booktitle	= {2021 IEEE International Parallel and Distributed
		  Processing Symposium (IPDPS)},
  pages		= {807--816},
  year		= {2021},
  organization	= {IEEE}
}

@InProceedings{	  fieni_selfwatts_2021,
  address	= {Melbourne, Australia},
  title		= {{SelfWatts}: {On}-the-fly {Selection} of {Performance}
		  {Events} to {Optimize} {Software}-defined {Power}
		  {Meters}},
  isbn		= {978-1-72819-586-5},
  shorttitle	= {{SelfWatts}},
  url		= {https://ieeexplore.ieee.org/document/9499557/},
  doi		= {10.1109/CCGrid51090.2021.00042},
  abstract	= {Fine-grained power monitoring of software-deﬁned
		  infrastructures is unavoidable to maximize the power usage
		  efﬁciency of data centers. However, the design of the
		  underlying power models that estimate the power consumption
		  of the monitored software components keeps being a long and
		  fragile process that remains tightly coupled to the host
		  machine and prevents a wider adoption by the industry
		  beyond the rich literature on this topic.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {2021 {IEEE}/{ACM} 21st {International} {Symposium} on
		  {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
  publisher	= {IEEE},
  author	= {Fieni, Guillaume and Rouvoy, Romain and Seiturier,
		  Lionel},
  month		= may,
  year		= {2021},
  pages		= {324--333},
  file		= {Fieni et al. - 2021 - SelfWatts On-the-fly Selection of
		  Performance
		  Eve.pdf:/home/prateeks/Zotero/storage/7XEUY5EQ/Fieni et al.
		  - 2021 - SelfWatts On-the-fly Selection of Performance
		  Eve.pdf:application/pdf}
}

@Article{	  fieni_smartwatts_2020,
  title		= {{SmartWatts}: {Self}-{Calibrating} {Software}-{Defined}
		  {Power} {Meter} for {Containers}},
  shorttitle	= {{SmartWatts}},
  url		= {http://arxiv.org/abs/2001.02505},
  abstract	= {Fine-grained power monitoring of software activities
		  becomes unavoidable to maximize the power usage efﬁciency
		  of data centers. In particular, achieving an optimal
		  scheduling of containers requires the deployment of
		  software-deﬁned power meters to go beyond the granularity
		  of hardware power monitoring sensors, such as Power
		  Distribution Units (PDU) or Intel’s Running Average Power
		  Limit (RAPL), to deliver power estimations of activities at
		  the granularity of software containers. However, the
		  deﬁnition of the underlying power models that estimate
		  the power consumption remains a long and fragile process
		  that is tightly coupled to the host machine.},
  language	= {en},
  urldate	= {2022-04-11},
  journal	= {arXiv:2001.02505 [cs]},
  author	= {Fieni, Guillaume and Rouvoy, Romain and Seinturier,
		  Lionel},
  month		= jan,
  year		= {2020},
  note		= {arXiv: 2001.02505},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Performance},
  annote	= {- Smartwatts. Practical control-loop PMC+model based
		  technique. Finds correlated power and PMCs. Smoothing.
		  Simple python code. - Repeated invoks can improve model. -
		  Try it out! },
  file		= {Fieni et al. - 2020 - SmartWatts Self-Calibrating
		  Software-Defined
		  Powe.pdf:/home/prateeks/Zotero/storage/QTAP5SJB/Fieni et
		  al. - 2020 - SmartWatts Self-Calibrating Software-Defined
		  Powe.pdf:application/pdf}
}

@InProceedings{	  firecracker-nsdi20,
  title		= {Firecracker: Lightweight Virtualization for Serverless
		  Applications},
  author	= {Agache, Alexandru and Brooker, Marc and Iordache,
		  Alexandra and Liguori, Anthony and Neugebauer, Rolf and
		  Piwonka, Phil and Popa, Diana-Maria},
  booktitle	= {17th {USENIX} Symposium on Networked Systems Design and
		  Implementation ({NSDI} 20)},
  pages		= {419--434},
  year		= {2020}
}

###InProceedings{ firecracker-nsdi20,
  title		= {Firecracker: Lightweight Virtualization for Serverless
		  Applications},
  author	= {Agache, Alexandru and Brooker, Marc and Iordache,
		  Alexandra and Liguori, Anthony and Neugebauer, Rolf and
		  Piwonka, Phil and Popa, Diana-Maria},
  booktitle	= {17th USENIX Symposium on Networked Systems Design and
		  Implementation (NSDI 20)},
  pages		= {419--434},
  year		= {2020}
}

@InProceedings{	  firestone2018azure,
  title		= {Azure accelerated networking: Smartnics in the public
		  cloud},
  author	= {Firestone, Daniel and Putnam, Andrew and Mundkur,
		  Sambhrama and Chiou, Derek and Dabagh, Alireza and
		  Andrewartha, Mike and Angepat, Hari and Bhanu, Vivek and
		  Caulfield, Adrian and Chung, Eric and others},
  booktitle	= {15th USENIX Symposium on Networked Systems Design and
		  Implementation (NSDI 18)},
  pages		= {51--66},
  year		= {2018}
}

@Article{	  flinn_energy-aware_1999,
  title		= {Energy-aware adaptation for mobile applications},
  abstract	= {In this paper, we demonstrate that a collaborative
		  relationship between the operating system and applications
		  can be used to meet user-specified goals for battery
		  duration. We first show how applications can dynamically
		  modify their behavior to conserve energy. We then show how
		  the Linux operating system can guide such adaptation to
		  yield a batterylife of desired duration. By monitoring
		  energy supply and demand, it is able to select the correct
		  tradeoff between energy conservation and application
		  quality. Our evaluation shows that this approach can meet
		  goals that extend battery life by as much as 30\%.},
  language	= {en},
  journal	= {ACM SIGOPS Operating Systems Review},
  author	= {Flinn, Jason and Satyanarayanan, M},
  year		= {1999},
  pages		= {16},
  file		= {Flinn and Satyanarayanan - Energy-aware adaptation for
		  mobile
		  applications.pdf:/home/prateeks/Zotero/storage/8XMD6YT4/Flinn
		  and Satyanarayanan - Energy-aware adaptation for mobile
		  applications.pdf:application/pdf}
}

@Article{	  flinn_managing_2004,
  title		= {Managing battery lifetime with energy-aware adaptation},
  volume	= {22},
  issn		= {0734-2071, 1557-7333},
  url		= {https://dl.acm.org/doi/10.1145/986533.986534},
  doi		= {10.1145/986533.986534},
  abstract	= {We demonstrate that a collaborative relationship between
		  the operating system and applications can be used to meet
		  user-specified goals for battery duration. We first
		  describe a novel profiling-based approach for accurately
		  measuring application and system energy consumption. We
		  then show how applications can dynamically modify their
		  behavior to conserve energy. We extend the Linux operating
		  system to yield battery lifetimes of user-specified
		  duration. By monitoring energy supply and demand and by
		  maintaining a history of application energy use, the
		  approach can dynamically balance energy conservation and
		  application quality. Our evaluation shows that this
		  approach can meet goals that extend battery life by as much
		  as 30\%.},
  language	= {en},
  number	= {2},
  urldate	= {2022-07-08},
  journal	= {ACM Transactions on Computer Systems},
  author	= {Flinn, Jason and Satyanarayanan, M.},
  month		= may,
  year		= {2004},
  pages		= {137--179},
  file		= {Flinn and Satyanarayanan - 2004 - Managing battery
		  lifetime with energy-aware
		  adapta.pdf:/home/prateeks/Zotero/storage/4F2U2ZWA/Flinn and
		  Satyanarayanan - 2004 - Managing battery lifetime with
		  energy-aware adapta.pdf:application/pdf}
}

@InProceedings{	  flinn_powerscope_1999,
  title		= {{PowerScope}: a tool for profiling the energy usage of
		  mobile applications},
  shorttitle	= {{PowerScope}},
  doi		= {10.1109/MCSA.1999.749272},
  abstract	= {We describe the design and implementation of PowerScope, a
		  tool for profiling energy usage by applications. PowerScope
		  maps energy consumption to program structure, in much the
		  same way that CPU profilers map processor cycles to
		  specific processes and procedures. Our approach combines
		  hardware instrumentation to measure current level with
		  kernel software support to perform statistical sampling of
		  system activity. Postprocessing software maps the sample
		  data to program structure and produces a profile of energy
		  usage by process and procedure. Using PowerScope, we have
		  been able to reduce the energy consumption of an adaptive
		  video playing application by 46\%.},
  booktitle	= {Proceedings {WMCSA}'99. {Second} {IEEE} {Workshop} on
		  {Mobile} {Computing} {Systems} and {Applications}},
  author	= {Flinn, J. and Satyanarayanan, M.},
  month		= feb,
  year		= {1999},
  keywords	= {Application software, Current measurement, Energy
		  consumption, Hardware, Instruments, Kernel, Performance
		  evaluation, Sampling methods, Software measurement,
		  Software performance},
  pages		= {2--10},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/YIE7KIKD/749272.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/HBELNJQB/Flinn and
		  Satyanarayanan - 1999 - PowerScope a tool for profiling the
		  energy usage .pdf:application/pdf}
}

###InProceedings{ flinn_powerscope_1999,
  title		= {{PowerScope}: a tool for profiling the energy usage of
		  mobile applications},
  shorttitle	= {{PowerScope}},
  doi		= {10.1109/MCSA.1999.749272},
  abstract	= {We describe the design and implementation of PowerScope, a
		  tool for profiling energy usage by applications. PowerScope
		  maps energy consumption to program structure, in much the
		  same way that CPU profilers map processor cycles to
		  specific processes and procedures. Our approach combines
		  hardware instrumentation to measure current level with
		  kernel software support to perform statistical sampling of
		  system activity. Postprocessing software maps the sample
		  data to program structure and produces a profile of energy
		  usage by process and procedure. Using PowerScope, we have
		  been able to reduce the energy consumption of an adaptive
		  video playing application by 46\%.},
  booktitle	= {Proceedings {WMCSA}'99. {Second} {IEEE} {Workshop} on
		  {Mobile} {Computing} {Systems} and {Applications}},
  author	= {Flinn, J. and Satyanarayanan, M.},
  month		= feb,
  year		= {1999},
  keywords	= {Hardware, Performance evaluation, Current measurement,
		  Energy consumption, Instruments, Kernel, Application
		  software, Sampling methods, Software measurement, Software
		  performance},
  pages		= {2--10},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/YIE7KIKD/749272.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/HBELNJQB/Flinn and
		  Satyanarayanan - 1999 - PowerScope a tool for profiling the
		  energy usage .pdf:application/pdf}
}

@Article{	  fonseca_quanto_2008,
  title		= {Quanto: {Tracking} {Energy} in {Networked} {Embedded}
		  {Systems}},
  abstract	= {We present Quanto, a network-wide time and energy
		  proﬁler for embedded network devices. By combining
		  well-deﬁned interfaces for hardware power states, fast
		  high-resolution energy metering, and causal tracking of
		  programmer-deﬁned activities, Quanto can map how energy
		  and time are spent on nodes and across a network.
		  Implementing Quanto on the TinyOS operating system required
		  modifying under 350 lines of code and adding 1275 new
		  lines. We show that being able to take ﬁnegrained energy
		  consumption measurements as fast as reading a counter
		  allows developers to precisely quantify the effects of
		  low-level system implementation decisions, such as using
		  DMA versus direct bus operations, or the effect of external
		  interference on the power draw of a low duty-cycle radio.
		  Finally, Quanto is lightweight enough that it has a minimal
		  effect on system behavior: each sample takes 100 CPU cycles
		  and 12 bytes of RAM.},
  language	= {en},
  journal	= {OSDI},
  author	= {Fonseca, Rodrigo and Dutta, Prabal and Levis, Philip and
		  Stoica, Ion},
  year		= {2008},
  pages		= {16},
  file		= {Fonseca et al. - Quanto Tracking Energy in Networked
		  Embedded
		  Syst.pdf:/home/prateeks/Zotero/storage/ZQWLLHT9/Fonseca et
		  al. - Quanto Tracking Energy in Networked Embedded
		  Syst.pdf:application/pdf}
}

@InProceedings{	  fouladi2017encoding,
  title		= {Encoding, fast and slow: Low-latency video processing
		  using thousands of tiny threads},
  author	= {Fouladi, Sadjad and Wahby, Riad S and Shacklett, Brennan
		  and Balasubramaniam, Karthikeyan Vasuki and Zeng, William
		  and Bhalerao, Rahul and Sivaraman, Anirudh and Porter,
		  George and Winstein, Keith},
  booktitle	= {14th USENIX Symposium on Networked Systems Design and
		  Implementation (NSDI 17)},
  pages		= {363--376},
  year		= {2017}
}

@Article{	  fouladi_laptop_2019,
  title		= {From {Laptop} to {Lambda}: {Outsourcing} {Everyday} {Jobs}
		  to {Thousands} of {Transient} {Functional} {Containers}},
  abstract	= {We present gg, a framework and a set of command-line tools
		  that helps people execute everyday applications—e.g.,
		  software compilation, unit tests, video encoding, or object
		  recognition—using thousands of parallel threads on a
		  cloudfunctions service to achieve near-interactive
		  completion times. In the future, instead of running these
		  tasks on a laptop, or keeping a warm cluster running in the
		  cloud, users might push a button that spawns 10,000
		  parallel cloud functions to execute a large job in a few
		  seconds from start. gg is designed to make this practical
		  and easy.},
  language	= {en},
  journal	= {USENIX Annual Technical Conference},
  author	= {Fouladi, Sadjad and Romero, Francisco and Iter, Dan and
		  Li, Qian and Chatterjee, Shuvo},
  year		= {2019},
  pages		= {15},
  file		= {Fouladi et al. - From Laptop to Lambda Outsourcing
		  Everyday Jobs
		  t.pdf:/home/prateeks/Zotero/storage/F5ZQT529/Fouladi et al.
		  - From Laptop to Lambda Outsourcing Everyday Jobs
		  t.pdf:application/pdf}
}

###Article{	  fouladi_laptop_2019,
  title		= {From {Laptop} to {Lambda}: {Outsourcing} {Everyday} {Jobs}
		  to {Thousands} of {Transient} {Functional} {Containers}},
  abstract	= {We present gg, a framework and a set of command-line tools
		  that helps people execute everyday applications—e.g.,
		  software compilation, unit tests, video encoding, or object
		  recognition—using thousands of parallel threads on a
		  cloudfunctions service to achieve near-interactive
		  completion times. In the future, instead of running these
		  tasks on a laptop, or keeping a warm cluster running in the
		  cloud, users might push a button that spawns 10,000
		  parallel cloud functions to execute a large job in a few
		  seconds from start. gg is designed to make this practical
		  and easy.},
  language	= {en},
  journal	= {USENIX ATC},
  author	= {Fouladi, Sadjad and Romero, Francisco and Iter, Dan and
		  Li, Qian and Chatterjee, Shuvo},
  year		= {2019},
  pages		= {15},
  file		= {Fouladi et al. - From Laptop to Lambda Outsourcing
		  Everyday Jobs
		  t.pdf:/home/prateeks/Zotero/storage/F5ZQT529/Fouladi et al.
		  - From Laptop to Lambda Outsourcing Everyday Jobs
		  t.pdf:application/pdf}
}

@InProceedings{	  fox2019learning,
  title		= {Learning everywhere: Pervasive machine learning for
		  effective high-performance computation},
  author	= {Fox, Geoffrey and Glazier, James A and Kadupitiya, JCS and
		  Jadhao, Vikram and Kim, Minje and Qiu, Judy and Sluka,
		  James P and Somogyi, Endre and Marathe, Madhav and Adiga,
		  Abhijin and others},
  booktitle	= {2019 IEEE International Parallel and Distributed
		  Processing Symposium Workshops (IPDPSW)},
  pages		= {422--429},
  year		= {2019},
  organization	= {IEEE}
}

@InProceedings{	  fu2022sfs,
  title		= {SFS: Smart OS Scheduling for Serverless Functions},
  author	= {Fu, Yuqi and Liu, Li and Wang, Haoliang and Cheng, Yue and
		  Chen, Songqing},
  booktitle	= {2022 SC22: International Conference for High Performance
		  Computing, Networking, Storage and Analysis (SC)},
  pages		= {584--599},
  year		= {2022},
  organization	= {IEEE Computer Society}
}

@Misc{		  functionbench,
  title		= {{FunctionBench}},
  howpublished	= {\url{https://github.com/ddps-lab/serverless-faas-workbench}},
  year		= {2019}
}

@InProceedings{	  funcx_hpdc_20,
  author	= {Chard, Ryan and Babuji, Yadu and Li, Zhuozhao and
		  Skluzacek, Tyler and Woodard, Anna and Blaiszik, Ben and
		  Foster, Ian and Chard, Kyle},
  title		= {FuncX: A Federated Function Serving Fabric for Science},
  year		= {2020},
  isbn		= {9781450370523},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3369583.3392683},
  doi		= {10.1145/3369583.3392683},
  booktitle	= {Proceedings of the 29th International Symposium on
		  High-Performance Parallel and Distributed Computing},
  pages		= {65-76},
  numpages	= {12},
  keywords	= {federated function serving, function as a service, funcX},
  location	= {Stockholm, Sweden},
  series	= {HPDC 20}
}

###InProceedings{ funcx_hpdc_20,
  author	= {Chard, Ryan and Babuji, Yadu and Li, Zhuozhao and
		  Skluzacek, Tyler and Woodard, Anna and Blaiszik, Ben and
		  Foster, Ian and Chard, Kyle},
  title		= {FuncX: A Federated Function Serving Fabric for Science},
  year		= {2020},
  isbn		= {9781450370523},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3369583.3392683},
  doi		= {10.1145/3369583.3392683},
  booktitle	= {Proceedings of the 29th International Symposium on
		  High-Performance Parallel and Distributed Computing},
  pages		= {65-76},
  numpages	= {12},
  keywords	= {federated function serving, function as a service, funcX},
  location	= {Stockholm, Sweden}
}

@InProceedings{	  gadepalli_challenges_2019,
  title		= {Challenges and {Opportunities} for {Efficient}
		  {Serverless} {Computing} at the {Edge}},
  doi		= {10.1109/SRDS47363.2019.00036},
  abstract	= {Serverless computing frameworks allow users to execute a
		  small application (dedicated to a specific task) without
		  handling operational issues such as server provisioning,
		  resource management, and resource scaling for the increased
		  load. Serverless computing originally emerged as a Cloud
		  computing framework, but might be a perfect match for IoT
		  data processing at the Edge. However, the existing
		  serverless solutions, based on VMs and containers, are too
		  heavy-weight (large memory footprint and high function
		  invocation time) for operating efficiency and elastic
		  scaling at the Edge. Moreover, many novel IoT applications
		  require low-latency data processing and near real-time
		  responses, which makes the current cloud-based serverless
		  solutions unsuitable. Recently, WebAssembly (Wasm) has been
		  proposed as an alternative method for running serverless
		  applications at near-native speeds, while having a small
		  memory footprint and optimized invocation time. In this
		  paper, we discuss some existing serverless solutions, their
		  design details, and unresolved performance challenges for
		  an efficient serverless management at the Edge. We outline
		  our serverless framework, called aWsm, based on the
		  WebAssembly approach, and discuss the opportunities enabled
		  by the aWsm design, including function profiling and
		  SLO-driven performance management of users' functions.
		  Finally, we present an initial assessment of aWsm
		  performance featuring average startup time (12μs to 30μs)
		  and an economical memory footprint (ranging from 10s to
		  100s of kB) for a subset of MiBench microbenchmarks used as
		  functions.},
  booktitle	= {2019 38th {Symposium} on {Reliable} {Distributed}
		  {Systems} ({SRDS})},
  author	= {Gadepalli, Phani Kishore and Peach, Gregor and Cherkasova,
		  Ludmila and Aitken, Rob and Parmer, Gabriel},
  month		= oct,
  year		= {2019},
  note		= {ISSN: 2575-8462},
  keywords	= {Cloud computing, Hardware, Virtual machine monitors,
		  Runtime, Containers, Internet of Things, Libraries, Cloud
		  computing, Serverless, FaaS, IoT, Edge computing,
		  WebAssembly, Wasm, performance management, SLOs.},
  pages		= {261--2615},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/QZDGYDT3/9049531.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/7C9HCK4W/Gadepalli et al.
		  - 2019 - Challenges and Opportunities for Efficient
		  Serverl.pdf:application/pdf}
}

@Article{	  gandhi-autoscale-12,
  author	= {Gandhi, Anshul and Harchol-Balter, Mor and Raghunathan,
		  Ram and Kozuch, Michael A.},
  title		= {AutoScale: Dynamic, Robust Capacity Management for
		  Multi-Tier Data Centers},
  year		= {2012},
  issue_date	= {November 2012},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {30},
  number	= {4},
  issn		= {0734-2071},
  url		= {https://doi.org/10.1145/2382553.2382556},
  doi		= {10.1145/2382553.2382556},
  abstract	= {Energy costs for data centers continue to rise, already
		  exceeding $15 billion yearly. Sadly much of this power is
		  wasted. Servers are only busy 10--30% of the time on
		  average, but they are often left on, while idle, utilizing
		  60% or more of peak power when in the idle state.We
		  introduce a dynamic capacity management policy, AutoScale,
		  that greatly reduces the number of servers needed in data
		  centers driven by unpredictable, time-varying load, while
		  meeting response time SLAs. AutoScale scales the data
		  center capacity, adding or removing servers as needed.
		  AutoScale has two key features: (i) it autonomically
		  maintains just the right amount of spare capacity to handle
		  bursts in the request rate; and (ii) it is robust not just
		  to changes in the request rate of real-world traces, but
		  also request size and server efficiency.We evaluate our
		  dynamic capacity management approach via implementation on
		  a 38-server multi-tier data center, serving a web site of
		  the type seen in Facebook or Amazon, with a key-value store
		  workload. We demonstrate that AutoScale vastly improves
		  upon existing dynamic capacity management policies with
		  respect to meeting SLAs and robustness.},
  journal	= {ACM Trans. Comput. Syst.},
  month		= {nov},
  articleno	= {14},
  numpages	= {26},
  keywords	= {power management, Data centers, resource provisioning}
}

@Article{	  gandhi2012autoscale,
  title		= {Autoscale: Dynamic, robust capacity management for
		  multi-tier data centers},
  author	= {Gandhi, Anshul and Harchol-Balter, Mor and Raghunathan,
		  Ram and Kozuch, Michael A},
  journal	= {ACM Transactions on Computer Systems (TOCS)},
  volume	= {30},
  number	= {4},
  pages		= {1--26},
  year		= {2012},
  publisher	= {ACM New York, NY, USA}
}

@InProceedings{	  gandhi2015halo,
  title		= {HALO: heterogeneity-aware load balancing},
  author	= {Gandhi, Anshul and Zhang, Xi and Mittal, Naman},
  booktitle	= {2015 IEEE 23rd International Symposium on Modeling,
		  Analysis, and Simulation of Computer and Telecommunication
		  Systems},
  pages		= {242--251},
  year		= {2015},
  organization	= {IEEE}
}

###InProceedings{ gandhi2015halo,
  title		= {HALO: heterogeneity-aware load balancing},
  author	= {Gandhi, Anshul and Zhang, Xi and Mittal, Naman},
  booktitle	= {2015 IEEE 23rd International Symposium on Modeling,
		  Analysis, and Simulation of Computer and Telecommunication
		  Systems},
  pages		= {242--251},
  year		= {2015},
  organization	= {IEEE}
}

@Article{	  garcia-lopez_servermix_2019,
  title		= {{ServerMix}: {Tradeoffs} and {Challenges} of {Serverless}
		  {Data} {Analytics}},
  shorttitle	= {{ServerMix}},
  url		= {http://arxiv.org/abs/1907.11465},
  abstract	= {Serverless computing has become very popular today since
		  it largely simpliﬁes cloud programming. Developers do not
		  need to longer worry about provisioning or operating
		  servers, and they pay only for the compute resources used
		  when their code is run. This new cloud paradigm suits well
		  for many applications, and researchers have already begun
		  investigating the feasibility of serverless computing for
		  data analytics. Unfortunately, today’s serverless
		  computing presents important limitations that make it
		  really difﬁcult to support all sorts of analytics
		  workloads. This paper ﬁrst starts by analyzing three
		  fundamental trade-offs of today’s serverless computing
		  model and their relationship with data analytics. It
		  studies how by relaxing disaggregation, isolation, and
		  simple scheduling, it is possible to increase the overall
		  computing performance, but at the expense of essential
		  aspects of the model such as elasticity, security, or
		  sub-second activations, respectively. The consequence of
		  these trade-offs is that analytics applications may well
		  end up embracing hybrid systems composed of serverless and
		  serverful components, which we call ServerMix in this
		  paper. We will review the existing related work to show
		  that most applications can be actually categorized as
		  ServerMix. Finally, this paper will introduce the major
		  challenges of the CloudButton research project to manage
		  these trade-offs.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1907.11465 [cs]},
  author	= {García-López, Pedro and Sánchez-Artigas, Marc and
		  Shillaker, Simon and Pietzuch, Peter and Breitgand, David
		  and Vernik, Gil and Sutra, Pierre and Tarrant, Tristan and
		  Ferrer, Ana Juan},
  month		= jul,
  year		= {2019},
  note		= {arXiv: 1907.11465},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  annote	= {Comment: 15 pages, 1 figure, 1 table},
  file		= {García-López et al. - 2019 - ServerMix Tradeoffs and
		  Challenges of Serverless
		  .pdf:/home/prateeks/Zotero/storage/CR66N8UR/García-López
		  et al. - 2019 - ServerMix Tradeoffs and Challenges of
		  Serverless .pdf:application/pdf}
}

###Article{	  garcia-lopez_servermix_2019,
  title		= {{ServerMix}: {Tradeoffs} and {Challenges} of {Serverless}
		  {Data} {Analytics}},
  shorttitle	= {{ServerMix}},
  url		= {http://arxiv.org/abs/1907.11465},
  abstract	= {Serverless computing has become very popular today since
		  it largely simpliﬁes cloud programming. Developers do not
		  need to longer worry about provisioning or operating
		  servers, and they pay only for the compute resources used
		  when their code is run. This new cloud paradigm suits well
		  for many applications, and researchers have already begun
		  investigating the feasibility of serverless computing for
		  data analytics. Unfortunately, today’s serverless
		  computing presents important limitations that make it
		  really difﬁcult to support all sorts of analytics
		  workloads. This paper ﬁrst starts by analyzing three
		  fundamental trade-offs of today’s serverless computing
		  model and their relationship with data analytics. It
		  studies how by relaxing disaggregation, isolation, and
		  simple scheduling, it is possible to increase the overall
		  computing performance, but at the expense of essential
		  aspects of the model such as elasticity, security, or
		  sub-second activations, respectively. The consequence of
		  these trade-offs is that analytics applications may well
		  end up embracing hybrid systems composed of serverless and
		  serverful components, which we call ServerMix in this
		  paper. We will review the existing related work to show
		  that most applications can be actually categorized as
		  ServerMix. Finally, this paper will introduce the major
		  challenges of the CloudButton research project to manage
		  these trade-offs.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1907.11465 [cs]},
  author	= {García-López, Pedro and Sánchez-Artigas, Marc and
		  Shillaker, Simon and Pietzuch, Peter and Breitgand, David
		  and Vernik, Gil and Sutra, Pierre and Tarrant, Tristan and
		  Ferrer, Ana Juan},
  month		= jul,
  year		= {2019},
  note		= {arXiv: 1907.11465},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  annote	= {Comment: 15 pages, 1 figure, 1 table},
  file		= {García-López et al. - 2019 - ServerMix Tradeoffs and
		  Challenges of Serverless
		  .pdf:/home/prateeks/Zotero/storage/CR66N8UR/García-López
		  et al. - 2019 - ServerMix Tradeoffs and Challenges of
		  Serverless .pdf:application/pdf}
}

###Article{	  garcia-lopez_servermix_2019,
  title		= {{ServerMix}: {Tradeoffs} and {Challenges} of {Serverless}
		  {Data} {Analytics}},
  shorttitle	= {{ServerMix}},
  url		= {http://arxiv.org/abs/1907.11465},
  abstract	= {Serverless computing has become very popular today since
		  it largely simpliﬁes cloud programming. Developers do not
		  need to longer worry about provisioning or operating
		  servers, and they pay only for the compute resources used
		  when their code is run. This new cloud paradigm suits well
		  for many applications, and researchers have already begun
		  investigating the feasibility of serverless computing for
		  data analytics. Unfortunately, today’s serverless
		  computing presents important limitations that make it
		  really difﬁcult to support all sorts of analytics
		  workloads. This paper ﬁrst starts by analyzing three
		  fundamental trade-offs of today’s serverless computing
		  model and their relationship with data analytics. It
		  studies how by relaxing disaggregation, isolation, and
		  simple scheduling, it is possible to increase the overall
		  computing performance, but at the expense of essential
		  aspects of the model such as elasticity, security, or
		  sub-second activations, respectively. The consequence of
		  these trade-offs is that analytics applications may well
		  end up embracing hybrid systems composed of serverless and
		  serverful components, which we call ServerMix in this
		  paper. We will review the existing related work to show
		  that most applications can be actually categorized as
		  ServerMix. Finally, this paper will introduce the major
		  challenges of the CloudButton research project to manage
		  these trade-offs.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1907.11465 [cs]},
  author	= {García-López, Pedro and Sánchez-Artigas, Marc and
		  Shillaker, Simon and Pietzuch, Peter and Breitgand, David
		  and Vernik, Gil and Sutra, Pierre and Tarrant, Tristan and
		  Ferrer, Ana Juan},
  month		= jul,
  year		= {2019},
  note		= {arXiv: 1907.11465},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  annote	= {Comment: 15 pages, 1 figure, 1 table},
  file		= {García-López et al. - 2019 - ServerMix Tradeoffs and
		  Challenges of Serverless
		  .pdf:/home/prateeks/Zotero/storage/CR66N8UR/García-López
		  et al. - 2019 - ServerMix Tradeoffs and Challenges of
		  Serverless .pdf:application/pdf}
}

@Misc{		  gcp_carbon_22,
  title		= {Google Cloud Carbon {Footprint}},
  howpublished	= {\url{https://cloud.google.com/carbon-footprint}},
  abstract	= {Google Cloud Carbon Footprint provides the carbon
		  emissions of your cloud usage. Measure, report and disclose
		  carbon emissions for ESG reporting.},
  language	= {en},
  date		= {2022-10-12},
  file		= {Snapshot:/home/prateeks/Zotero/storage/BBMQTZN9/carbon-footprint.html:text/html}
}

@InCollection{	  gdfs_2001,
  address	= {Berlin, Heidelberg},
  title		= {Role of {Aging}, {Frequency}, and {Size} in {Web} {Cache}
		  {Replacement} {Policies}},
  volume	= {2110},
  isbn		= {978-3-540-42293-8 978-3-540-48228-4},
  url		= {http://link.springer.com/10.1007/3-540-48228-8_12},
  abstract	= {Document caching on is used to improve Web performance. An
		  eﬃcient caching policy keeps popular documents in the
		  cache and replaces rarely used ones. The latest web cache
		  replacement policies incorporate the document size,
		  frequency, and age in the decision process. The
		  recently-proposed and very popular Greedy-Dual-Size (GDS)
		  policy is based on document size and has an elegant aging
		  mechanism. Similarly, the Greedy-Dual-Frequency (GDF)
		  policy takes into account ﬁle frequency and exploits the
		  aging mechanism to deal with cache pollution. The
		  eﬃciency of a cache replacement policy can be evaluated
		  along two popular metrics: ﬁle hit ratio and byte hit
		  ratio. Using four diﬀerent web server logs, we show that
		  GDS-like replacement policies emphasizing size yield the
		  best ﬁle hit ratio but typically show poor byte hit
		  ratio, while GDF-like replacement policies emphasizing
		  frequency have better byte hit ratio but result in worse
		  ﬁle hit ratio. In this paper, we propose a generalization
		  of Greedy-Dual-Frequency-Size policy which allows to
		  balance the emphasis on size vs. frequency. We perform a
		  sensitivity study to derive the impact of size and
		  frequency on ﬁle and byte hit ratio, identifying
		  parameters that aim at optimizing both metrics.},
  language	= {en},
  urldate	= {2020-08-17},
  booktitle	= {High-{Performance} {Computing} and {Networking}},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Cherkasova, Ludmila and Ciardo, Gianfranco},
  editor	= {Goos, G. and Hartmanis, J. and van Leeuwen, J. and
		  Hertzberger, Bob and Hoekstra, Alfons and Williams, Roy},
  year		= {2001},
  doi		= {10.1007/3-540-48228-8_12},
  note		= {Series Title: Lecture Notes in Computer Science},
  pages		= {114--123},
  file		= {Cherkasova and Ciardo - 2001 - Role of Aging, Frequency,
		  and Size in Web Cache
		  Re.pdf:/home/prateeks/Zotero/storage/Y4ZD5TA5/Cherkasova
		  and Ciardo - 2001 - Role of Aging, Frequency, and Size in
		  Web Cache Re.pdf:application/pdf}
}

@TechReport{	  gdsf,
  title		= {{Improving WWW Proxies Performance with
		  Greedy-Dual-Size-Frequency Caching Policy}},
  author	= {Cherkasova, Ludmila},
  institution	= {{HP Labs Technical Report 98-69 (R.1)}},
  year		= 1998
}

###InProceedings{ gdsf,
  title		= {{Improving WWW Proxies Performance with
		  Greedy-Dual-Size-Frequency Caching Policy}},
  author	= {Cherkasova, Ludmila},
  booktitle	= {{HP Labs Technical Report 98-69 (R.1)}},
  year		= {1998}
}

@Article{	  ge_powerpack_2010,
  title		= {{PowerPack}: {Energy} {Profiling} and {Analysis} of
		  {High}-{Performance} {Systems} and {Applications}},
  volume	= {21},
  issn		= {1558-2183},
  shorttitle	= {{PowerPack}},
  doi		= {10.1109/TPDS.2009.76},
  abstract	= {Energy efficiency is a major concern in modern
		  high-performance computing system design. In the past few
		  years, there has been mounting evidence that power usage
		  limits system scale and computing density, and thus,
		  ultimately system performance. However, despite the impact
		  of power and energy on the computer systems community, few
		  studies provide insight to where and how power is consumed
		  on high-performance systems and applications. In previous
		  work, we designed a framework called PowerPack that was the
		  first tool to isolate the power consumption of devices
		  including disks, memory, NICs, and processors in a
		  high-performance cluster and correlate these measurements
		  to application functions. In this work, we extend our
		  framework to support systems with multicore,
		  multiprocessor-based nodes, and then provide in-depth
		  analyses of the energy consumption of parallel applications
		  on clusters of these systems. These analyses include the
		  impacts of chip multiprocessing on power and energy
		  efficiency, and its interaction with application
		  executions. In addition, we use PowerPack to study the
		  power dynamics and energy efficiencies of dynamic voltage
		  and frequency scaling (DVFS) techniques on clusters. Our
		  experiments reveal conclusively how intelligent DVFS
		  scheduling can enhance system energy efficiency while
		  maintaining performance.},
  number	= {5},
  journal	= {IEEE Transactions on Parallel and Distributed Systems},
  author	= {Ge, Rong and Feng, Xizhou and Song, Shuaiwen and Chang,
		  Hung-Ching and Li, Dong and Cameron, Kirk W.},
  month		= may,
  year		= {2010},
  note		= {Conference Name: IEEE Transactions on Parallel and
		  Distributed Systems},
  keywords	= {Application software, CMP-based cluster, Distributed
		  system, dynamic voltage and frequency scaling., Dynamic
		  voltage scaling, Energy consumption, energy efficiency,
		  Energy efficiency, Frequency, Multicore processing, power
		  management, power measurement, Power measurement,
		  Semiconductor device measurement, System analysis and
		  design, System performance, system tools},
  pages		= {658--671},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/KCAHVGM6/4906989.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/59BEJALJ/Ge et al. - 2010
		  - PowerPack Energy Profiling and Analysis of
		  High-P.pdf:application/pdf}
}

###Article{	  ge_powerpack_2010,
  title		= {{PowerPack}: {Energy} {Profiling} and {Analysis} of
		  {High}-{Performance} {Systems} and {Applications}},
  volume	= {21},
  issn		= {1558-2183},
  shorttitle	= {{PowerPack}},
  doi		= {10.1109/TPDS.2009.76},
  abstract	= {Energy efficiency is a major concern in modern
		  high-performance computing system design. In the past few
		  years, there has been mounting evidence that power usage
		  limits system scale and computing density, and thus,
		  ultimately system performance. However, despite the impact
		  of power and energy on the computer systems community, few
		  studies provide insight to where and how power is consumed
		  on high-performance systems and applications. In previous
		  work, we designed a framework called PowerPack that was the
		  first tool to isolate the power consumption of devices
		  including disks, memory, NICs, and processors in a
		  high-performance cluster and correlate these measurements
		  to application functions. In this work, we extend our
		  framework to support systems with multicore,
		  multiprocessor-based nodes, and then provide in-depth
		  analyses of the energy consumption of parallel applications
		  on clusters of these systems. These analyses include the
		  impacts of chip multiprocessing on power and energy
		  efficiency, and its interaction with application
		  executions. In addition, we use PowerPack to study the
		  power dynamics and energy efficiencies of dynamic voltage
		  and frequency scaling (DVFS) techniques on clusters. Our
		  experiments reveal conclusively how intelligent DVFS
		  scheduling can enhance system energy efficiency while
		  maintaining performance.},
  number	= {5},
  journal	= {IEEE Transactions on Parallel and Distributed Systems},
  author	= {Ge, Rong and Feng, Xizhou and Song, Shuaiwen and Chang,
		  Hung-Ching and Li, Dong and Cameron, Kirk W.},
  month		= may,
  year		= {2010},
  note		= {Conference Name: IEEE Transactions on Parallel and
		  Distributed Systems},
  keywords	= {Application software, CMP-based cluster, Distributed
		  system, dynamic voltage and frequency scaling., Dynamic
		  voltage scaling, Energy consumption, energy efficiency,
		  Energy efficiency, Frequency, Multicore processing, power
		  management, power measurement, Power measurement,
		  Semiconductor device measurement, System analysis and
		  design, System performance, system tools},
  pages		= {658--671},
  file		= {IEEE Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/4US43CJH/Ge et al. - 2010
		  - PowerPack Energy Profiling and Analysis of
		  High-P.pdf:application/pdf}
}

@InProceedings{	  gerhorst_energybudgets_2020,
  title		= {{EnergyBudgets}: {Integrating} {Physical} {Energy}
		  {Measurement} {Devices} into {Systems} {Software}},
  shorttitle	= {{EnergyBudgets}},
  doi		= {10.1109/SBESC51047.2020.9277849},
  abstract	= {Excessive energy consumption is a critical problem for
		  mobile computing systems due to their limited battery
		  capacity. Software developers aim to improve energy
		  efficiency by monitoring and profiling the energy
		  consumption of their systems in order to discover and
		  resolve energy hot-spots. However, energy measurement is
		  often tedious since it involves a hardware setup as well as
		  software integration. To support accurate but also
		  convenient energy measurements, we propose the inclusion of
		  external energy measurement devices into existing
		  performance profiling subsystems. This approach allows the
		  energy-consumption analysis of applications that run on the
		  system under test (SUT) using the same tools as used for
		  other performance metrics. To enable low-overhead
		  self-monitoring, we propose a modular analysis approach,
		  EnergyBudgets, which bridges external energy measurement
		  hardware to the Linux perf subsystem. The evaluation of our
		  implementation shows that energy budgets accurately measure
		  the energy consumed by different workloads and allow for an
		  overhead-reduction on the SUT by 20\% to 51\% in comparison
		  to regular timers, while still guaranteeing the same level
		  of precision.},
  booktitle	= {2020 {X} {Brazilian} {Symposium} on {Computing} {Systems}
		  {Engineering} ({SBESC})},
  author	= {Gerhorst, Luis and Reif, Stefan and Herzog, Benedict and
		  Hönig, Timo},
  month		= nov,
  year		= {2020},
  note		= {ISSN: 2324-7894},
  keywords	= {Monitoring, Hardware, Program processors, performance
		  evaluation, resource management, Current measurement,
		  Energy consumption, Power measurement, design
		  methodologies, energy aware systems, Energy measurement,
		  tools},
  pages		= {1--8},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/JXR2QVHX/9277849.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/3YJHHGJW/Gerhorst et al.
		  - 2020 - EnergyBudgets Integrating Physical Energy
		  Measure.pdf:application/pdf}
}

@InProceedings{	  ghanei_os-based_2016,
  address	= {San Francisco Airport CA USA},
  title		= {{OS}-based {Resource} {Accounting} for {Asynchronous}
		  {Resource} {Use} in {Mobile} {Systems}},
  isbn		= {978-1-4503-4185-1},
  url		= {https://dl.acm.org/doi/10.1145/2934583.2934639},
  doi		= {10.1145/2934583.2934639},
  abstract	= {One essential functionality of a modern operating system
		  is to accurately account for the resource usage of the
		  underlying hardware. This is especially important for
		  computing systems that operate on battery power, since
		  energy management requires accurately attributing resource
		  uses to processes. However, components such as sensors,
		  actuators and specialized network interfaces are often used
		  in an asynchronous fashion, and makes it diﬃcult to
		  conduct accurate resource accounting. For example, a
		  process that makes a request to a sensor may not be running
		  on the processor for the full duration of the resource
		  usage; and current mechanisms of resource accounting fail
		  to provide accurate accounting for such asynchronous uses.
		  This paper proposes a new mechanism to accurately account
		  for the asynchronous usage of resources in mobile systems.
		  Our insight is that by accurately relating the user
		  requests with kernel requests to device and corresponding
		  device responses, we can accurately attribute resource use
		  to the requesting process. Our prototype implemented in
		  Linux demonstrates that we can account for the usage of
		  asynchronous resources such as GPS and WiFi accurately.},
  language	= {en},
  urldate	= {2022-07-08},
  booktitle	= {Proceedings of the 2016 {International} {Symposium} on
		  {Low} {Power} {Electronics} and {Design}},
  publisher	= {ACM},
  author	= {Ghanei, Farshad and Tipnis, Pranav and Marcus, Kyle and
		  Dantu, Karthik and Ko, Steve and Ziarek, Lukasz},
  month		= aug,
  year		= {2016},
  pages		= {296--301},
  file		= {Ghanei et al. - 2016 - OS-based Resource Accounting for
		  Asynchronous
		  Reso.pdf:/home/prateeks/Zotero/storage/KQR4FJYH/Ghanei et
		  al. - 2016 - OS-based Resource Accounting for Asynchronous
		  Reso.pdf:application/pdf}
}

@Article{	  ghanei_os-based_2019,
  title		= {{OS}-{Based} {Energy} {Accounting} for {Asynchronous}
		  {Resources} in {IoT} {Devices}},
  volume	= {6},
  issn		= {2327-4662},
  doi		= {10.1109/JIOT.2019.2907668},
  abstract	= {Rapid advancements in computing, communication, sensing,
		  and actuation have seen the growth of Internet of Things
		  (IoT) devices in our daily life. One of the fundamental
		  constraints of a typical IoT device is energy as IoT
		  devices rely on a battery. Therefore, it is crucial for
		  their operating system (OS) to be able to accurately
		  account for system-wide energy usage. Specifically, the OS
		  should be able to attribute such accounted energy to the
		  running applications accurately. Traditional OSs have
		  limited capability when it comes to tracking components
		  such as sensors, actuators and network interfaces, as they
		  are often used in an asynchronous fashion. This would make
		  it difficult to conduct energy accounting accurately. This
		  paper proposes a new mechanism to accurately account for
		  the asynchronous energy usage of resources in mobile
		  systems and IoT devices. Our insight is that by accurately
		  relating the application requests with kernel requests to
		  device and corresponding device responses, we can
		  accurately attribute time of use to the requesting process.
		  However, resources such as WiFi reception violate this
		  assumption. In such cases, we can measure usage by the
		  number of bytes in each individual transaction. Using such
		  a hybrid approach, we can account for energy usage with
		  94\% accuracy and perform much better than using each of
		  these models individually.},
  number	= {3},
  journal	= {IEEE Internet of Things Journal},
  author	= {Ghanei, Farshad and Tipnis, Pranav and Marcus, Kyle and
		  Dantu, Karthik and Ko, Steven Y. and Ziarek, Lukasz},
  month		= jun,
  year		= {2019},
  note		= {Conference Name: IEEE Internet of Things Journal},
  keywords	= {Hardware, Mobile handsets, Internet of Things, Internet of
		  Things (IoT), resource management, Kernel, Power demand,
		  Sensors, Batteries, Embedded systems, operating systems
		  (OSs), power management},
  pages		= {5841--5852},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/LSXN6F2S/8674793.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/UIHZTBT2/Ghanei et al. -
		  2019 - OS-Based Energy Accounting for Asynchronous
		  Resour.pdf:application/pdf}
}

@Article{	  ghosh_caching_2019,
  title		= {Caching {Techniques} to {Improve} {Latency} in
		  {Serverless} {Architectures}},
  url		= {http://arxiv.org/abs/1911.07351},
  abstract	= {Serverless computing has gained a signiﬁcant traction in
		  recent times because of its simplicity of development,
		  deployment and ﬁne-grained billing. However, while
		  implementing complex services comprising databases, ﬁle
		  stores, or more than one serverless function, the
		  performance in terms of latency of serving requests often
		  degrades severely. In this work, we analyze different
		  serverless architectures with AWS Lambda services and
		  compare their performance in terms of latency with a
		  traditional virtual machine (VM) based approach. We observe
		  that database access latency in serverless architecture is
		  almost 14 times than that in VM based setup. Further, we
		  introduce some caching strategies which can improve the
		  response time signiﬁcantly, and compare their
		  performance.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1911.07351 [cs]},
  author	= {Ghosh, Bishakh Chandra and Addya, Sourav Kanti and Somy,
		  Nishant Baranwal and Nath, Shubha Brata and Chakraborty,
		  Sandip and Ghosh, Soumya K.},
  month		= nov,
  year		= {2019},
  note		= {arXiv: 1911.07351},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Networking and Internet
		  Architecture},
  file		= {Ghosh et al. - 2019 - Caching Techniques to Improve
		  Latency in
		  Serverles.pdf:/home/prateeks/Zotero/storage/GTL43XNY/Ghosh
		  et al. - 2019 - Caching Techniques to Improve Latency in
		  Serverles.pdf:application/pdf}
}

###Article{	  ghosh_caching_2019,
  title		= {Caching {Techniques} to {Improve} {Latency} in
		  {Serverless} {Architectures}},
  url		= {http://arxiv.org/abs/1911.07351},
  abstract	= {Serverless computing has gained a signiﬁcant traction in
		  recent times because of its simplicity of development,
		  deployment and ﬁne-grained billing. However, while
		  implementing complex services comprising databases, ﬁle
		  stores, or more than one serverless function, the
		  performance in terms of latency of serving requests often
		  degrades severely. In this work, we analyze different
		  serverless architectures with AWS Lambda services and
		  compare their performance in terms of latency with a
		  traditional virtual machine (VM) based approach. We observe
		  that database access latency in serverless architecture is
		  almost 14 times than that in VM based setup. Further, we
		  introduce some caching strategies which can improve the
		  response time signiﬁcantly, and compare their
		  performance.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1911.07351 [cs]},
  author	= {Ghosh, Bishakh Chandra and Addya, Sourav Kanti and Somy,
		  Nishant Baranwal and Nath, Shubha Brata and Chakraborty,
		  Sandip and Ghosh, Soumya K.},
  month		= nov,
  year		= {2019},
  note		= {arXiv: 1911.07351},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Networking and Internet
		  Architecture},
  file		= {Ghosh et al. - 2019 - Caching Techniques to Improve
		  Latency in
		  Serverles.pdf:/home/prateeks/Zotero/storage/GTL43XNY/Ghosh
		  et al. - 2019 - Caching Techniques to Improve Latency in
		  Serverles.pdf:application/pdf}
}

@InProceedings{	  gill2008multi,
  title		= {On Multi-level Exclusive Caching: Offline Optimality and
		  Why promotions are better than demotions.},
  author	= {Gill, Binny S},
  booktitle	= {FAST},
  volume	= {8},
  pages		= {1--17},
  year		= {2008}
}

@Article{	  gilly2011up,
  title		= {An up-to-date survey in web load balancing},
  author	= {Gilly, Katja and Juiz, Carlos and Puigjaner, Ramon},
  journal	= {World Wide Web},
  volume	= {14},
  number	= {2},
  pages		= {105--131},
  year		= {2011},
  publisher	= {Springer}
}

@InProceedings{	  glikson_runbox_2019,
  address	= {Haifa, Israel},
  title		= {Runbox: serverless interactive computing platform},
  isbn		= {978-1-4503-6749-3},
  shorttitle	= {Runbox},
  url		= {http://dl.acm.org/citation.cfm?doid=3319647.3325852},
  doi		= {10.1145/3319647.3325852},
  abstract	= {Serverless computing revolutionizes cloud software by
		  eliminating the need to manage the underlying
		  infrastructure, while providing efficient scaling,
		  performance and security isolation as well as usage
		  metering.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 12th {ACM} {International} {Conference}
		  on {Systems} and {Storage} - {SYSTOR} '19},
  publisher	= {ACM Press},
  author	= {Glikson, Alex and Nie, Shichao and Breitgand, David},
  year		= {2019},
  pages		= {191--191},
  file		= {Glikson et al. - 2019 - Runbox serverless interactive
		  computing
		  platform.pdf:/home/prateeks/Zotero/storage/858ZRWXW/Glikson
		  et al. - 2019 - Runbox serverless interactive computing
		  platform.pdf:application/pdf}
}

###InProceedings{ glikson_runbox_2019,
  address	= {Haifa, Israel},
  title		= {Runbox: serverless interactive computing platform},
  isbn		= {978-1-4503-6749-3},
  shorttitle	= {Runbox},
  url		= {http://dl.acm.org/citation.cfm?doid=3319647.3325852},
  doi		= {10.1145/3319647.3325852},
  abstract	= {Serverless computing revolutionizes cloud software by
		  eliminating the need to manage the underlying
		  infrastructure, while providing efficient scaling,
		  performance and security isolation as well as usage
		  metering.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 12th {ACM} {International} {Conference}
		  on {Systems} and {Storage} - {SYSTOR} '19},
  publisher	= {ACM Press},
  author	= {Glikson, Alex and Nie, Shichao and Breitgand, David},
  year		= {2019},
  pages		= {191--191},
  file		= {Glikson et al. - 2019 - Runbox serverless interactive
		  computing
		  platform.pdf:/home/prateeks/Zotero/storage/858ZRWXW/Glikson
		  et al. - 2019 - Runbox serverless interactive computing
		  platform.pdf:application/pdf}
}

@Misc{		  google-functions,
  title		= {{Google Cloud Functions}},
  year		= {2020 },
  howpublished	= {\url{https://cloud.google.com/functions }}
}

###Misc{	  google-functions,
  title		= {{Google Cloud Functions}},
  year		= {2020 },
  howpublished	= {\url{https://cloud.google.com/functions }}
}

@Misc{		  graalvm,
  doi		= {10.48550/ARXIV.2212.10131},
  url		= {https://arxiv.org/abs/2212.10131},
  author	= {Bruno, Rodrigo and Ivanenko, Serhii and Wang, Sutao and
		  Stevanovic, Jovan and Jovanovic, Vojin},
  keywords	= {Distributed, Parallel, and Cluster Computing (cs.DC),
		  Programming Languages (cs.PL), FOS: Computer and
		  information sciences, FOS: Computer and information
		  sciences},
  title		= {Graalvisor: Virtualized Polyglot Runtime for Serverless
		  Applications},
  publisher	= {arXiv},
  year		= {2022},
  copyright	= {Creative Commons Attribution Non Commercial Share Alike
		  4.0 International}
}

@InProceedings{	  guliani_per-application_2019,
  address	= {Dresden Germany},
  title		= {Per-{Application} {Power} {Delivery}},
  isbn		= {978-1-4503-6281-8},
  url		= {https://dl.acm.org/doi/10.1145/3302424.3303981},
  doi		= {10.1145/3302424.3303981},
  abstract	= {Datacenter servers are often under-provisioned for peak
		  power consumption due to the substantial cost of providing
		  power. When there is insufficient power for the workload,
		  servers can lower voltage and frequency levels to reduce
		  consumption, but at the cost of performance. Current
		  processors provide power limiting mechanisms, but they
		  generally apply uniformly to all CPUs on a chip. For
		  servers running heterogeneous jobs, though, it is necessary
		  to differentiate the power provided to different jobs. This
		  prevents interference when a job may be throttled by
		  another job hitting a power limit. While some recent CPUs
		  support per-CPU power management, there are no clear
		  policies on how to distribute power between applications.
		  Current hardware power limiters, such as Intel’s RAPL
		  throttle the fastest core first, which harms high-priority
		  applications.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the {Fourteenth} {EuroSys} {Conference}
		  2019},
  publisher	= {ACM},
  author	= {Guliani, Akhil and Swift, Michael M.},
  month		= mar,
  year		= {2019},
  pages		= {1--16},
  file		= {Guliani and Swift - 2019 - Per-Application Power
		  Delivery.pdf:/home/prateeks/Zotero/storage/JV3M83X7/Guliani
		  and Swift - 2019 - Per-Application Power
		  Delivery.pdf:application/pdf}
}

@InProceedings{	  gunasekaran2020fifer,
  title		= {Fifer: Tackling resource underutilization in the
		  serverless era},
  author	= {Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and
		  Nachiappan, Nachiappan C and Kandemir, Mahmut Taylan and
		  Das, Chita R},
  booktitle	= {Proceedings of the 21st International Middleware
		  Conference},
  pages		= {280--295},
  year		= {2020}
}

@InProceedings{	  gunasekaran_spock_2019,
  title		= {Spock: {Exploiting} {Serverless} {Functions} for {SLO} and
		  {Cost} {Aware} {Resource} {Procurement} in {Public}
		  {Cloud}},
  shorttitle	= {Spock},
  doi		= {10.1109/CLOUD.2019.00043},
  abstract	= {We are witnessing the emergence of elastic web services
		  which are hosted in public cloud infrastructures. For
		  reasons of cost-effectiveness, it is crucial for the
		  elasticity of these web services to match the
		  dynamically-evolving user demand. Traditional approaches
		  employ clusters of virtual machines (VMs) to dynamically
		  scale resources based on application demand. However, they
		  still face challenges such as higher cost due to
		  over-provisioning or incur service level objective (SLO)
		  violations due to under-provisioning. Motivated by this
		  observation, we propose Spock, a new scalable and elastic
		  control system that exploits both VMs and serverless
		  functions to reduce cost and ensure SLO for elastic web
		  services. We show that under two different scaling
		  policies, Spock reduces SLO violations of queries by up to
		  74\% when compared to VM-based resource procurement
		  schemes. Further, Spock yields significant cost savings, by
		  up to 33\% compared to traditional approaches which use
		  only VMs.},
  booktitle	= {2019 {IEEE} 12th {International} {Conference} on {Cloud}
		  {Computing} ({CLOUD})},
  author	= {Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and
		  Kandemir, Mahmut Taylan and Urgaonkar, Bhuvan and Kesidis,
		  George and Das, Chita},
  month		= jul,
  year		= {2019},
  note		= {ISSN: 2159-6182},
  keywords	= {application demand, autoscaling, cloud computing, cost
		  aware resource procurement, cost savings, cost-aware,
		  cost-effectiveness, dynamically scale resources,
		  dynamically-evolving user demand, elastic control system,
		  elastic Web services, elasticity, FaaS, lambda, public
		  cloud infrastructures, public domain software, resource
		  allocation, resource procurement schemes, scalable control
		  system, serverless, serverless functions, service level
		  objective violations, SLO, Spock, virtual machines, Web
		  services},
  pages		= {199--208},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/GUTUWQ6M/8814535.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/GIJDWZLU/Gunasekaran et
		  al. - 2019 - Spock Exploiting Serverless Functions for SLO
		  and.pdf:application/pdf}
}

###InProceedings{ gunasekaran_spock_2019,
  title		= {Spock: {Exploiting} {Serverless} {Functions} for {SLO} and
		  {Cost} {Aware} {Resource} {Procurement} in {Public}
		  {Cloud}},
  shorttitle	= {Spock},
  doi		= {10.1109/CLOUD.2019.00043},
  abstract	= {We are witnessing the emergence of elastic web services
		  which are hosted in public cloud infrastructures. For
		  reasons of cost-effectiveness, it is crucial for the
		  elasticity of these web services to match the
		  dynamically-evolving user demand. Traditional approaches
		  employ clusters of virtual machines (VMs) to dynamically
		  scale resources based on application demand. However, they
		  still face challenges such as higher cost due to
		  over-provisioning or incur service level objective (SLO)
		  violations due to under-provisioning. Motivated by this
		  observation, we propose Spock, a new scalable and elastic
		  control system that exploits both VMs and serverless
		  functions to reduce cost and ensure SLO for elastic web
		  services. We show that under two different scaling
		  policies, Spock reduces SLO violations of queries by up to
		  74\% when compared to VM-based resource procurement
		  schemes. Further, Spock yields significant cost savings, by
		  up to 33\% compared to traditional approaches which use
		  only VMs.},
  booktitle	= {2019 {IEEE} 12th {International} {Conference} on {Cloud}
		  {Computing} ({CLOUD})},
  author	= {Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and
		  Kandemir, Mahmut Taylan and Urgaonkar, Bhuvan and Kesidis,
		  George and Das, Chita},
  month		= jul,
  year		= {2019},
  note		= {ISSN: 2159-6182},
  keywords	= {application demand, autoscaling, cloud computing, cost
		  aware resource procurement, cost savings, cost-aware,
		  cost-effectiveness, dynamically scale resources,
		  dynamically-evolving user demand, elastic control system,
		  elastic Web services, elasticity, FaaS, lambda, public
		  cloud infrastructures, public domain software, resource
		  allocation, resource procurement schemes, scalable control
		  system, serverless, serverless functions, service level
		  objective violations, SLO, Spock, virtual machines, Web
		  services},
  pages		= {199--208},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/GUTUWQ6M/8814535.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/GIJDWZLU/Gunasekaran et
		  al. - 2019 - Spock Exploiting Serverless Functions for SLO
		  and.pdf:application/pdf}
}

@Misc{		  guo_decomposing_2022,
  title		= {Decomposing and {Executing} {Serverless} {Applications} as
		  {Resource} {Graphs}},
  url		= {http://arxiv.org/abs/2206.13444},
  abstract	= {Today’s serverless computing has several key limitations
		  including per-function resource limits, ﬁxed
		  CPU-to-memory ratio, and constant resource allocation
		  throughout a function execution and across different
		  invocations of it. The root cause of these limitations is
		  the “function-centric” model: a function is a
		  ﬁxed-size box that is allocated, executed, and terminated
		  as an inseparable unit. This unit is pre-deﬁned by the
		  cloud provider and cannot properly capture user needs.},
  language	= {en},
  urldate	= {2023-01-09},
  publisher	= {arXiv},
  author	= {Guo, Zhiyuan and Blanco, Zachary and Shahrad, Mohammad and
		  Wei, Zerui and Dong, Bili and Li, Jinmou and Pota, Ishaan
		  and Xu, Harry and Zhang, Yiying},
  month		= dec,
  year		= {2022},
  note		= {arXiv:2206.13444 [cs]},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  file		= {Guo et al. - 2022 - Decomposing and Executing Serverless
		  Applications
		  .pdf:/home/prateeks/Zotero/storage/X656P88J/Guo et al. -
		  2022 - Decomposing and Executing Serverless Applications
		  .pdf:application/pdf}
}

###Misc{	  guo_decomposing_2022,
  title		= {Decomposing and {Executing} {Serverless} {Applications} as
		  {Resource} {Graphs}},
  url		= {http://arxiv.org/abs/2206.13444},
  abstract	= {Today’s serverless computing has several key limitations
		  including per-function resource limits, ﬁxed
		  CPU-to-memory ratio, and constant resource allocation
		  throughout a function execution and across different
		  invocations of it. The root cause of these limitations is
		  the “function-centric” model: a function is a
		  ﬁxed-size box that is allocated, executed, and terminated
		  as an inseparable unit. This unit is pre-deﬁned by the
		  cloud provider and cannot properly capture user needs.},
  language	= {en},
  urldate	= {2023-01-09},
  publisher	= {arXiv},
  author	= {Guo, Zhiyuan and Blanco, Zachary and Shahrad, Mohammad and
		  Wei, Zerui and Dong, Bili and Li, Jinmou and Pota, Ishaan
		  and Xu, Harry and Zhang, Yiying},
  month		= dec,
  year		= {2022},
  note		= {arXiv:2206.13444 [cs]},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  file		= {Guo et al. - 2022 - Decomposing and Executing Serverless
		  Applications
		  .pdf:/home/prateeks/Zotero/storage/X656P88J/Guo et al. -
		  2022 - Decomposing and Executing Serverless Applications
		  .pdf:application/pdf}
}

@InProceedings{	  guo_power_2018,
  address	= {Porto Portugal},
  title		= {Power sandbox: power awareness redefined},
  isbn		= {978-1-4503-5584-1},
  shorttitle	= {Power sandbox},
  url		= {https://dl.acm.org/doi/10.1145/3190508.3190533},
  doi		= {10.1145/3190508.3190533},
  abstract	= {Many apps benefit from knowing their power consumption and
		  adapting their behaviors on the fly. To offer apps power
		  knowledge at run time, an OS often meters system power and
		  divides it among apps. Since the impacts of concurrent apps
		  on system power are entangled, this approach not only makes
		  it difficult to reason about power but also results in
		  power side channels, a serious vulnerability. To this end,
		  we introduce a new OS principal called power sandbox, which
		  enables one app to observe the fine-grained power
		  consumption of itself running in its vertical slice of the
		  hardware/software stack. The observed power is insulated
		  from the impacts of other apps. Our contribution is a set
		  of lightweight kernel extensions that simultaneously i)
		  enforce the power sandbox boundaries and ii) confine
		  entailed performance loss to the sandboxed apps. Our
		  experiences on two embedded platforms show that power
		  sandboxes simplify reasoning about power, maintain fairness
		  among apps, and minimize power side channels, thus
		  facilitating construction of power-aware apps.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the {Thirteenth} {EuroSys} {Conference}},
  publisher	= {ACM},
  author	= {Guo, Liwei and Xu, Tiantu and Xu, Mengwei and Liu, Xuanzhe
		  and Lin, Felix Xiaozhu},
  month		= apr,
  year		= {2018},
  pages		= {1--15},
  annote	= {- Power sandbox. Security angle/leakage overplayed.
		  Temporary highoverhead container. Useful for sampled fn
		  invoks? - Resource balloons. Spatial ballon: no colocation
		  by running dummy-OS tasks. Temporal balloon: no
		  simultaenous inflight I/O, accel reqs. Dummy kernel tasks
		  instead of restricting multiplexing at various kernel
		  subsystems. - Strange evaluation. Effectiveness not
		  obvious. No comparisons/baselines. },
  file		= {Guo et al. - 2018 - Power sandbox power awareness
		  redefined.pdf:/home/prateeks/Zotero/storage/3W8QGH6R/Guo et
		  al. - 2018 - Power sandbox power awareness
		  redefined.pdf:application/pdf}
}

###InProceedings{ guo_power_2018,
  address	= {Porto Portugal},
  title		= {Power sandbox: power awareness redefined},
  isbn		= {978-1-4503-5584-1},
  shorttitle	= {Power sandbox},
  url		= {https://dl.acm.org/doi/10.1145/3190508.3190533},
  doi		= {10.1145/3190508.3190533},
  abstract	= {Many apps benefit from knowing their power consumption and
		  adapting their behaviors on the fly. To offer apps power
		  knowledge at run time, an OS often meters system power and
		  divides it among apps. Since the impacts of concurrent apps
		  on system power are entangled, this approach not only makes
		  it difficult to reason about power but also results in
		  power side channels, a serious vulnerability. To this end,
		  we introduce a new OS principal called power sandbox, which
		  enables one app to observe the fine-grained power
		  consumption of itself running in its vertical slice of the
		  hardware/software stack. The observed power is insulated
		  from the impacts of other apps. Our contribution is a set
		  of lightweight kernel extensions that simultaneously i)
		  enforce the power sandbox boundaries and ii) confine
		  entailed performance loss to the sandboxed apps. Our
		  experiences on two embedded platforms show that power
		  sandboxes simplify reasoning about power, maintain fairness
		  among apps, and minimize power side channels, thus
		  facilitating construction of power-aware apps.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the {Thirteenth} {EuroSys} {Conference}},
  publisher	= {ACM},
  author	= {Guo, Liwei and Xu, Tiantu and Xu, Mengwei and Liu, Xuanzhe
		  and Lin, Felix Xiaozhu},
  month		= apr,
  year		= {2018},
  pages		= {1--15},
  annote	= {- Power sandbox. Security angle/leakage overplayed.
		  Temporary highoverhead container. Useful for sampled fn
		  invoks? - Resource balloons. Spatial ballon: no colocation
		  by running dummy-OS tasks. Temporal balloon: no
		  simultaenous inflight I/O, accel reqs. Dummy kernel tasks
		  instead of restricting multiplexing at various kernel
		  subsystems. - Strange evaluation. Effectiveness not
		  obvious. No comparisons/baselines. },
  file		= {Guo et al. - 2018 - Power sandbox power awareness
		  redefined.pdf:/home/prateeks/Zotero/storage/3W8QGH6R/Guo et
		  al. - 2018 - Power sandbox power awareness
		  redefined.pdf:application/pdf}
}

@Article{	  gupta2007analysis,
  title		= {Analysis of join-the-shortest-queue routing for web server
		  farms},
  author	= {Gupta, Varun and Balter, Mor Harchol and Sigman, Karl and
		  Whitt, Ward},
  journal	= {Performance Evaluation},
  volume	= {64},
  number	= {9-12},
  pages		= {1062--1081},
  year		= {2007},
  publisher	= {Elsevier}
}

@InProceedings{	  hackenberg_energy_2015,
  title		= {An {Energy} {Efficiency} {Feature} {Survey} of the {Intel}
		  {Haswell} {Processor}},
  doi		= {10.1109/IPDPSW.2015.70},
  abstract	= {The recently introduced Intel Xeon E5-1600 v3 and E5-2600
		  v3 series processors – codenamed Haswell-EP – implement
		  major changes compared to their predecessors. Among these
		  changes are integrated voltage regulators that enable
		  individual voltages and frequencies for every core. In this
		  paper we analyze a number of consequences of this
		  development that are of utmost importance for energy
		  efficiency optimization strategies such as dynamic voltage
		  and frequency scaling (DVFS) and dynamic concurrency
		  throttling (DCT). This includes the enhanced RAPL
		  implementation and its improved accuracy as it moves from
		  modeling to actual measurement. Another fundamental change
		  is that every clock speed above AVX frequency – including
		  nominal frequency – is opportunistic and unreliable,
		  which vastly decreases performance predictability with
		  potential effects on scalability. Moreover, we characterize
		  significantly changed p-state transition behavior, and
		  determine crucial memory performance data.},
  booktitle	= {2015 {IEEE} {International} {Parallel} and {Distributed}
		  {Processing} {Symposium} {Workshop}},
  author	= {Hackenberg, Daniel and Schöne, Robert and Ilsche, Thomas
		  and Molka, Daniel and Schuchart, Joseph and Geyer, Robin},
  month		= may,
  year		= {2015},
  keywords	= {Benchmark testing, Random access memory, Power demand,
		  Energy measurement, Voltage control, Regulators, Frequency
		  measurement},
  pages		= {896--904},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/MMXT2VP3/7284406.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/EMEIZSUV/Hackenberg et
		  al. - 2015 - An Energy Efficiency Feature Survey of the
		  Intel H.pdf:application/pdf}
}

@Article{	  hahnel_measuring_2012,
  title		= {Measuring energy consumption for short code paths using
		  {RAPL}},
  volume	= {40},
  issn		= {0163-5999},
  url		= {https://dl.acm.org/doi/10.1145/2425248.2425252},
  doi		= {10.1145/2425248.2425252},
  abstract	= {Measuring the energy consumption of software components is
		  a major building block for generating models that allow for
		  energy-aware scheduling, accounting and budgeting. Current
		  measurement techniques focus on coarse-grained measurements
		  of application or system events. However, ﬁne grain
		  adjustments in particular in the operating-system kernel
		  and in application-level servers require power proﬁles at
		  the level of a single software function. Until recently,
		  this appeared to be impossible due to the lacking ﬁne
		  grain resolution and high costs of measurement equipment.},
  language	= {en},
  number	= {3},
  urldate	= {2022-04-21},
  journal	= {ACM SIGMETRICS Performance Evaluation Review},
  author	= {Hähnel, Marcus and Döbel, Björn and Völp, Marcus and
		  Härtig, Hermann},
  month		= dec,
  year		= {2012},
  pages		= {13--17},
  file		= {Hähnel et al. - 2012 - Measuring energy consumption for
		  short code paths
		  .pdf:/home/prateeks/Zotero/storage/A98EPQY4/Hähnel et al.
		  - 2012 - Measuring energy consumption for short code paths
		  .pdf:application/pdf}
}

@InProceedings{	  hall_execution_2019,
  address	= {Montreal, Quebec, Canada},
  title		= {An execution model for serverless functions at the edge},
  isbn		= {978-1-4503-6283-2},
  url		= {http://dl.acm.org/citation.cfm?doid=3302505.3310084},
  doi		= {10.1145/3302505.3310084},
  abstract	= {Serverless computing platforms allow developers to host
		  singlepurpose applications that automatically scale with
		  demand. In contrast to traditional long-running
		  applications on dedicated, virtualized, or container-based
		  platforms, serverless applications are intended to be
		  instantiated when called, execute a single function, and
		  shut down when finished. State-of-the-art serverless
		  platforms achieve these goals by creating a new container
		  instance to host a function when it is called and
		  destroying the container when it completes. This design
		  allows for cost and resource savings when hosting simple
		  applications, such as those supporting IoT devices at the
		  edge of the network. However, the use of containers
		  introduces some overhead which may be unsuitable for
		  applications requiring low-latency response or hardware
		  platforms with limited resources, such as those served by
		  edge computing environments. In this paper, we present a
		  nomenclature for characterizing serverless function access
		  patterns which allows us to derive the basic requirements
		  of a serverless computing runtime. We then propose the use
		  of WebAssembly as an alternative method for running
		  serverless applications while meeting these requirements.
		  Finally, we demonstrate how a WebAssembly-based serverless
		  platform provides many of the same isolation and
		  performance guarantees of container-based platforms while
		  reducing average application start times and the resources
		  needed to host them.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {International} {Conference} on
		  {Internet} of {Things} {Design} and {Implementation} -
		  {IoTDI} '19},
  publisher	= {ACM Press},
  author	= {Hall, Adam and Ramachandran, Umakishore},
  year		= {2019},
  pages		= {225--236},
  file		= {Hall and Ramachandran - 2019 - An execution model for
		  serverless functions at
		  the.pdf:/home/prateeks/Zotero/storage/Y62LI2RX/Hall and
		  Ramachandran - 2019 - An execution model for serverless
		  functions at the.pdf:application/pdf}
}

###InProceedings{ hall_execution_2019,
  address	= {Montreal, Quebec, Canada},
  title		= {An execution model for serverless functions at the edge},
  isbn		= {978-1-4503-6283-2},
  url		= {http://dl.acm.org/citation.cfm?doid=3302505.3310084},
  doi		= {10.1145/3302505.3310084},
  abstract	= {Serverless computing platforms allow developers to host
		  singlepurpose applications that automatically scale with
		  demand. In contrast to traditional long-running
		  applications on dedicated, virtualized, or container-based
		  platforms, serverless applications are intended to be
		  instantiated when called, execute a single function, and
		  shut down when finished. State-of-the-art serverless
		  platforms achieve these goals by creating a new container
		  instance to host a function when it is called and
		  destroying the container when it completes. This design
		  allows for cost and resource savings when hosting simple
		  applications, such as those supporting IoT devices at the
		  edge of the network. However, the use of containers
		  introduces some overhead which may be unsuitable for
		  applications requiring low-latency response or hardware
		  platforms with limited resources, such as those served by
		  edge computing environments. In this paper, we present a
		  nomenclature for characterizing serverless function access
		  patterns which allows us to derive the basic requirements
		  of a serverless computing runtime. We then propose the use
		  of WebAssembly as an alternative method for running
		  serverless applications while meeting these requirements.
		  Finally, we demonstrate how a WebAssembly-based serverless
		  platform provides many of the same isolation and
		  performance guarantees of container-based platforms while
		  reducing average application start times and the resources
		  needed to host them.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {International} {Conference} on
		  {Internet} of {Things} {Design} and {Implementation} -
		  {IoTDI} '19},
  publisher	= {ACM Press},
  author	= {Hall, Adam and Ramachandran, Umakishore},
  year		= {2019},
  pages		= {225--236},
  file		= {Hall and Ramachandran - 2019 - An execution model for
		  serverless functions at
		  the.pdf:/home/prateeks/Zotero/storage/Y62LI2RX/Hall and
		  Ramachandran - 2019 - An execution model for serverless
		  functions at the.pdf:application/pdf}
}

@Article{	  harizopoulos_energy_2009,
  title		= {Energy {Efficiency}: {The} {New} {Holy} {Grail} of {Data}
		  {Management} {Systems} {Research}},
  abstract	= {Energy costs are quickly rising in large-scale data
		  centers and are soon projected to overtake the cost of
		  hardware. As a result, data center operators have recently
		  started turning into using more energy-friendly hardware.
		  Despite the growing body of research in power management
		  techniques, there has been little work to date on energy
		  efficiency from a data management software perspective.},
  language	= {en},
  journal	= {CIDR},
  author	= {Harizopoulos, Stavros and Shah, Mehul A and Meza, Justin
		  and Ranganathan, Parthasarathy},
  year		= {2009},
  pages		= {8},
  file		= {Harizopoulos et al. - 2009 - Energy Efficiency The New
		  Holy Grail of Data
		  Mana.pdf:/home/prateeks/Zotero/storage/7LBCSI7F/Harizopoulos
		  et al. - 2009 - Energy Efficiency The New Holy Grail of
		  Data Mana.pdf:application/pdf}
}

@InProceedings{	  harvest-osdi20,
  title		= {Providing slos for resource-harvesting vms in cloud
		  platforms},
  author	= {Ambati, Pradeep and Goiri, {\'I}{\~n}igo and Frujeri,
		  Felipe and Gun, Alper and Wang, Ke and Dolan, Brian and
		  Corell, Brian and Pasupuleti, Sekhar and Moscibroda, Thomas
		  and Elnikety, Sameh and others},
  booktitle	= {14th USENIX Symposium on Operating Systems Design and
		  Implementation ($\{$OSDI$\}$ 20)},
  pages		= {735--751},
  year		= {2020}
}

@Article{	  hassan2021survey,
  title		= {Survey on serverless computing},
  author	= {Hassan, Hassan B and Barakat, Saman A and Sarhan, Qusay
		  I},
  journal	= {Journal of Cloud Computing},
  volume	= {10},
  number	= {1},
  pages		= {1--29},
  year		= {2021},
  publisher	= {SpringerOpen}
}

@Article{	  hellerstein_serverless_2019,
  title		= {Serverless {Computing}: {One} {Step} {Forward}, {Two}
		  {Steps} {Back}},
  abstract	= {Serverless computing offers the potential to program the
		  cloud in an autoscaling, pay-as-you go manner. In this
		  paper we address critical gaps in first-generation
		  serverless computing, which place its autoscaling potential
		  at odds with dominant trends in modern computing: notably
		  data-centric and distributed computing, but also open
		  source and custom hardware. Put together, these gaps make
		  current serverless offerings a bad fit for cloud innovation
		  and particularly bad for data systems innovation. In
		  addition to pinpointing some of the main shortfalls of
		  current serverless architectures, we raise a set of
		  challenges we believe must be met to unlock the radical
		  potential that the cloud—with its exabytes of storage and
		  millions of cores—should offer to innovative
		  developers.},
  language	= {en},
  author	= {Hellerstein, Joseph M and Faleiro, Jose and Gonzalez,
		  Joseph E and Schleier-Smith, Johann and Sreekanti, Vikram
		  and Tumanov, Alexey and Wu, Chenggang},
  year		= {2019},
  journal	= {CIDR 2019, 9th Biennial Conference on Innovative
		  DataSystems Research, Asilomar, CA, USA, January 13-16,
		  2019},
  pages		= {9},
  file		= {Hellerstein et al. - 2019 - Serverless Computing One Step
		  Forward, Two Steps
		  .pdf:/home/prateeks/Zotero/storage/FGFM5GNE/Hellerstein et
		  al. - 2019 - Serverless Computing One Step Forward, Two
		  Steps .pdf:application/pdf}
}

@InProceedings{	  hendrickson2016serverless,
  title		= {Serverless computation with openlambda},
  author	= {Hendrickson, Scott and Sturdevant, Stephen and Harter,
		  Tyler and Venkataramani, Venkateshwaran and Arpaci-Dusseau,
		  Andrea C and Arpaci-Dusseau, Remzi H},
  booktitle	= {8th {USENIX} Workshop on Hot Topics in Cloud Computing
		  (HotCloud 16)},
  year		= {2016}
}

###InProceedings{ hendrickson2016serverless,
  title		= {Serverless computation with openlambda},
  author	= {Hendrickson, Scott and Sturdevant, Stephen and Harter,
		  Tyler and Venkataramani, Venkateshwaran and Arpaci-Dusseau,
		  Andrea C and Arpaci-Dusseau, Remzi H},
  booktitle	= {8th USENIX Workshop on Hot Topics in Cloud Computing
		  (HotCloud 16)},
  year		= {2016}
}

###InProceedings{ hendrickson2016serverless,
  title		= {Serverless computation with openlambda},
  author	= {Hendrickson, Scott and Sturdevant, Stephen and Harter,
		  Tyler and Venkataramani, Venkateshwaran and Arpaci-Dusseau,
		  Andrea C and Arpaci-Dusseau, Remzi H},
  booktitle	= {8th $\{$USENIX$\}$ workshop on hot topics in cloud
		  computing (HotCloud 16)},
  year		= {2016}
}

@Article{	  hoffmann_dynamic_2011,
  title		= {Dynamic knobs for responsive power-aware computing},
  abstract	= {We present PowerDial, a system for dynamically adapting
		  application behavior to execute successfully in the face of
		  load and power ﬂuctuations. PowerDial transforms static
		  conﬁguration parameters into dynamic knobs that the
		  PowerDial control system can manipulate to dynamically
		  trade oﬀ the accuracy of the computation in return for
		  reductions in the computational resources that the
		  application requires to produce its results. These
		  reductions translate directly into performance improvements
		  and power savings.},
  language	= {en},
  journal	= {ASPLOS},
  author	= {Hoffmann, Henry and Sidiroglou, Stelios and Carbin,
		  Michael and Misailovic, Sasa and Agarwal, Anant and Rinard,
		  Martin},
  year		= {2011},
  pages		= {14},
  file		= {Hoffmann et al. - Dynamic knobs for responsive power-aware
		  computing.pdf:/home/prateeks/Zotero/storage/LK4W9IDW/Hoffmann
		  et al. - Dynamic knobs for responsive power-aware
		  computing.pdf:application/pdf}
}

@InProceedings{	  hoffmann_jouleguard_2015,
  address	= {Monterey California},
  title		= {{JouleGuard}: energy guarantees for approximate
		  applications},
  isbn		= {978-1-4503-3834-9},
  shorttitle	= {{JouleGuard}},
  url		= {https://dl.acm.org/doi/10.1145/2815400.2815403},
  doi		= {10.1145/2815400.2815403},
  abstract	= {Energy consumption limits battery life in mobile devices
		  and increases costs for servers and data centers.
		  Approximate computing addresses energy concerns by allowing
		  applications to trade accuracy for decreased energy
		  consumption. Approximation frameworks can guarantee
		  accuracy or performance and generally reduce energy usage;
		  however, they provide no energy guarantees. Such guarantees
		  would be beneﬁcial for users who have a ﬁxed energy
		  budget and want to maximize application accuracy within
		  that budget. We address this need by presenting JouleGuard:
		  a runtime control system that coordinates approximate
		  applications with system resource usage to provide control
		  theoretic formal guarantees of energy consumption, while
		  maximizing accuracy. We implement JouleGuard and test it on
		  three different platforms (a mobile, tablet, and server)
		  with eight different approximate applications created from
		  two different frameworks. We ﬁnd that JouleGuard respects
		  energy budgets, provides near optimal accuracy, adapts to
		  phases in application workload, and provides better
		  outcomes than application approximation or system resource
		  adaptation alone. JouleGuard is general with respect to the
		  applications and systems it controls, making it a suitable
		  runtime for a number of approximate computing frameworks.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the 25th {Symposium} on {Operating}
		  {Systems} {Principles}},
  publisher	= {ACM},
  author	= {Hoffmann, Henry},
  month		= oct,
  year		= {2015},
  pages		= {198--214},
  file		= {Hoffmann - 2015 - JouleGuard energy guarantees for
		  approximate
		  appl.pdf:/home/prateeks/Zotero/storage/LRCLTNXU/Hoffmann -
		  2015 - JouleGuard energy guarantees for approximate
		  appl.pdf:application/pdf}
}

@Article{	  hotc-blind,
  title		= {Blinded},
  author	= {Anon.},
  year		= {2022}
}

@Article{	  hotcarbon22-faas,
  title		= {Challenges and Opportunities in Sustainable Serverless
		  Computing},
  author	= {Sharma, Prateek},
  language	= {en},
  journal	= {HotCarbon 2022: 1st Workshop on Sustainable Computer
		  Systems Design and Implementation}
}

@InProceedings{	  hu2016kinetic,
  title		= {Kinetic modeling of data eviction in cache},
  author	= {Hu, Xiameng and Wang, Xiaolin and Zhou, Lan and Luo,
		  Yingwei and Ding, Chen and Wang, Zhenlin},
  booktitle	= {2016 USENIX Annual Technical Conference (USENIX ATC 16)},
  pages		= {351--364},
  year		= {2016}
}

@InProceedings{	  hunhoff2020proactive,
  title		= {Proactive Serverless Function Resource Management},
  author	= {Hunhoff, Erika and Irshad, Shazal and Thurimella, Vijay
		  and Tariq, Ali and Rozner, Eric},
  booktitle	= {Proceedings of the 2020 Sixth International Workshop on
		  Serverless Computing},
  pages		= {61--66},
  year		= {2020}
}

###InProceedings{ hunhoff2020proactive,
  title		= {Proactive Serverless Function Resource Management},
  author	= {Hunhoff, Erika and Irshad, Shazal and Thurimella, Vijay
		  and Tariq, Ali and Rozner, Eric},
  booktitle	= {Proceedings of the 2020 Sixth International Workshop on
		  Serverless Computing},
  pages		= {61--66},
  year		= {2020}
}

@Article{	  hyytia2012size,
  title		= {Size-and state-aware dispatching problem with
		  queue-specific job sizes},
  author	= {Hyyti{\"a}, Esa and Penttinen, Aleksi and Aalto, Samuli},
  journal	= {European Journal of Operational Research},
  volume	= {217},
  number	= {2},
  pages		= {357--370},
  year		= {2012},
  publisher	= {Elsevier}
}

@InCollection{	  ipipe_smartnic_19,
  title		= {Offloading distributed applications onto smartNICs using
		  iPipe},
  author	= {Liu, Ming and Cui, Tianyi and Schuh, Henry and
		  Krishnamurthy, Arvind and Peter, Simon and Gupta, Karan},
  booktitle	= {Proceedings of the ACM Special Interest Group on Data
		  Communication},
  pages		= {318--333},
  year		= {2019}
}

@InProceedings{	  irwin2011towards,
  title		= {Towards continuous policy-driven demand response in data
		  centers},
  author	= {Irwin, David and Sharma, Navin and Shenoy, Prashant},
  booktitle	= {Proceedings of the 2nd ACM SIGCOMM workshop on Green
		  networking},
  pages		= {19--24},
  year		= {2011}
}

@InProceedings{	  islam2016new,
  title		= {A New Perspective on Energy Accounting in
		  $\{$Multi-Tenant$\}$ Data Centers},
  author	= {Islam, Mohammad A and Ren, Shaolei},
  booktitle	= {USENIX Workshop on Cool Topics on Sustainable Data Centers
		  (CoolDC 16)},
  year		= {2016}
}

@InProceedings{	  jia2021nightcore,
  title		= {Nightcore: efficient and scalable serverless computing for
		  latency-sensitive, interactive microservices},
  author	= {Jia, Zhipeng and Witchel, Emmett},
  booktitle	= {Proceedings of the 26th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {152--166},
  year		= {2021}
}

@Article{	  jiang2018convergence,
  title		= {On the convergence of the ttl approximation for an lru
		  cache under independent stationary request processes},
  author	= {Jiang, Bo and Nain, Philippe and Towsley, Don},
  journal	= {ACM Transactions on Modeling and Performance Evaluation of
		  Computing Systems (TOMPECS)},
  volume	= {3},
  number	= {4},
  pages		= {1--31},
  year		= {2018},
  publisher	= {ACM New York, NY, USA}
}

@InProceedings{	  jiang_non-it_2018,
  address	= {Vienna},
  title		= {Non-{IT} {Energy} {Accounting} in {Virtualized}
		  {Datacenter}},
  isbn		= {978-1-5386-6871-9},
  url		= {https://ieeexplore.ieee.org/document/8416301/},
  doi		= {10.1109/ICDCS.2018.00038},
  abstract	= {Energy accounting plays a crucial role in datacenter
		  energy management, wherein the energy consumption of nonIT
		  units (e.g., UPS and cooling system) makes up a
		  signiﬁcant portion. However, it is challenging to fairly
		  account for non-IT energy on an individual VM basis,
		  because the non-IT units are shared by multiple VMs in a
		  virtualized datacenter and only the system-level non-IT
		  energy consumption can be measured. Existing policies,
		  e.g., equally or proportionally allocating nonIT energy to
		  VMs based on their IT energy, are not fair, in the sense
		  that they can not satisfy a set of desired axiomatic
		  principles of fair allocation. In this paper, we propose
		  LEAPS, a Lightweight Energy Accounting Policy based on a
		  provably fair methodology called Shapley value. We evaluate
		  it using real-world datacenter trace and demonstrate that,
		  compared to original Shapley value approach that has an
		  exponential complexity, LEAPS yields almost the same energy
		  accounting result within a maximum relative error less than
		  6.97\%, while having a negligible computation time.},
  language	= {en},
  urldate	= {2022-09-11},
  booktitle	= {2018 {IEEE} 38th {International} {Conference} on
		  {Distributed} {Computing} {Systems} ({ICDCS})},
  publisher	= {IEEE},
  author	= {Jiang, Weixiang and Ren, Shaolei and Liu, Fangming and
		  Jin, Hai},
  month		= jul,
  year		= {2018},
  pages		= {300--310},
  file		= {Jiang et al. - 2018 - Non-IT Energy Accounting in
		  Virtualized
		  Datacenter.pdf:/home/prateeks/Zotero/storage/T5JKQA9B/Jiang
		  et al. - 2018 - Non-IT Energy Accounting in Virtualized
		  Datacenter.pdf:application/pdf}
}

@InProceedings{	  jiang_virtual_2017,
  address	= {Atlanta, GA, USA},
  title		= {Virtual {Machine} {Power} {Accounting} with {Shapley}
		  {Value}},
  isbn		= {978-1-5386-1792-2},
  url		= {http://ieeexplore.ieee.org/document/7980105/},
  doi		= {10.1109/ICDCS.2017.235},
  abstract	= {The ever-increasing power consumption of datacenters has
		  eaten up a large portion of their proﬁt. One possible
		  solution is to charge datacenter users for their actual
		  power usage. However, it poses a great technical challenge
		  as the power of VMs co-existing in a physical machine
		  cannot be measured directly. It is thus critical to develop
		  a fair method to disaggregate the power of a physical
		  machine to individual VMs. We tackle the above challenge by
		  modeling the power disaggregation problem as a cooperative
		  game and propose non-deterministic Shapley value to
		  discover the fair power share of VMs (in the sense of
		  satisfying four desired axiomatic principles), while
		  compensating the negative impact of VM power variation. We
		  demonstrate that the results from existing power
		  model-based solution can deviate from the “ground
		  truth” by 25.22\% ⇠ 46.15\%. And compared with the
		  exact Shapley value, our non-deterministic Shapley value
		  can achieve less than 5\% error for 90\% of the time.},
  language	= {en},
  urldate	= {2022-09-11},
  booktitle	= {2017 {IEEE} 37th {International} {Conference} on
		  {Distributed} {Computing} {Systems} ({ICDCS})},
  publisher	= {IEEE},
  author	= {Jiang, Weixiang and Liu, Fangming and Tang, Guoming and
		  Wu, Kui and Jin, Hai},
  month		= jun,
  year		= {2017},
  pages		= {1683--1693},
  file		= {Jiang et al. - 2017 - Virtual Machine Power Accounting
		  with Shapley
		  Valu.pdf:/home/prateeks/Zotero/storage/B72J6W78/Jiang et
		  al. - 2017 - Virtual Machine Power Accounting with Shapley
		  Valu.pdf:application/pdf}
}

@InProceedings{	  jin2000popularity,
  title		= {Popularity-aware greedy dual-size web proxy caching
		  algorithms},
  author	= {Jin, Shudong and Bestavros, Azer},
  booktitle	= {Proceedings 20th IEEE International Conference on
		  Distributed Computing Systems},
  pages		= {254--261},
  year		= {2000},
  organization	= {IEEE}
}

@InProceedings{	  john_sweep_2019,
  address	= {Auckland, New Zealand},
  title		= {{SWEEP}: {Accelerating} {Scientific} {Research} {Through}
		  {Scalable} {Serverless} {Workflows}},
  isbn		= {978-1-4503-7044-8},
  shorttitle	= {{SWEEP}},
  url		= {http://dl.acm.org/citation.cfm?doid=3368235.3368839},
  doi		= {10.1145/3368235.3368839},
  abstract	= {Scientific and commercial applications are increasingly
		  being executed in the cloud, but the difficulties
		  associated with cluster management render on-demand
		  resources inaccessible or inefficient to many users.
		  Recently, the serverless execution model, in which the
		  provisioning of resources is abstracted from the user, has
		  gained prominence as an alternative to traditional
		  cyberinfrastructure solutions. With its inherent
		  elasticity, the serverless paradigm constitutes a promising
		  computational model for scientific workflows, allowing
		  domain specialists to develop and deploy workflows that are
		  subject to varying workloads and intermittent usage without
		  the overhead of infrastructure maintenance. We present the
		  Serverless Workflow Enablement and Execution Platform
		  (SWEEP), a cloud-agnostic workflow management system with a
		  purely serverless execution model that allows users to
		  define, run and monitor generic cloud-native workflows. We
		  demonstrate the use of SWEEP on workflows from two
		  disparate scientific domains and present an evaluation of
		  performance and scaling.},
  language	= {en},
  urldate	= {2020-07-29},
  booktitle	= {Proceedings of the 12th {IEEE}/{ACM} {International}
		  {Conference} on {Utility} and {Cloud} {Computing}
		  {Companion} - {UCC} '19 {Companion}},
  publisher	= {ACM Press},
  author	= {John, Aji and Ausmees, Kristiina and Muenzen, Kathleen and
		  Kuhn, Catherine and Tan, Amanda},
  year		= {2019},
  pages		= {43--50},
  file		= {John et al. - 2019 - SWEEP Accelerating Scientific
		  Research Through
		  Sc.pdf:/home/prateeks/Zotero/storage/5MWV8GFP/John et al. -
		  2019 - SWEEP Accelerating Scientific Research Through
		  Sc.pdf:application/pdf}
}

@InProceedings{	  jonas2017occupy,
  title		= {Occupy the cloud: Distributed computing for the 99\%},
  author	= {Jonas, Eric and Pu, Qifan and Venkataraman, Shivaram and
		  Stoica, Ion and Recht, Benjamin},
  booktitle	= {Proceedings of the 2017 Symposium on Cloud Computing},
  pages		= {445--451},
  year		= {2017},
  organization	= {ACM}
}

###InProceedings{ jonas2017occupy,
  title		= {Occupy the cloud: Distributed computing for the 99\%},
  author	= {Jonas, Eric and Pu, Qifan and Venkataraman, Shivaram and
		  Stoica, Ion and Recht, Benjamin},
  booktitle	= {Proceedings of the 2017 Symposium on Cloud Computing},
  pages		= {445--451},
  year		= {2017},
  organization	= {ACM}
}

@Article{	  jonas_cloud_2019,
  title		= {Cloud {Programming} {Simplified}: {A} {Berkeley} {View} on
		  {Serverless} {Computing}},
  shorttitle	= {Cloud {Programming} {Simplified}},
  url		= {http://arxiv.org/abs/1902.03383},
  abstract	= {Serverless cloud computing handles virtually all the
		  system administration operations needed to make it easier
		  for programmers to use the cloud. It provides an interface
		  that greatly simpliﬁes cloud programming, and represents
		  an evolution that parallels the transition from assembly
		  language to high-level programming languages. This paper
		  gives a quick history of cloud computing, including an
		  accounting of the predictions of the 2009 Berkeley View of
		  Cloud Computing paper, explains the motivation for
		  serverless computing, describes applications that stretch
		  the current limits of serverless, and then lists obstacles
		  and research opportunities required for serverless
		  computing to fulﬁll its full potential. Just as the 2009
		  paper identiﬁed challenges for the cloud and predicted
		  they would be addressed and that cloud use would
		  accelerate, we predict these issues are solvable and that
		  serverless computing will grow to dominate the future of
		  cloud computing.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1902.03383 [cs]},
  author	= {Jonas, Eric and Schleier-Smith, Johann and Sreekanti,
		  Vikram and Tsai, Chia-Che and Khandelwal, Anurag and Pu,
		  Qifan and Shankar, Vaishaal and Carreira, Joao and Krauth,
		  Karl and Yadwadkar, Neeraja and Gonzalez, Joseph E. and
		  Popa, Raluca Ada and Stoica, Ion and Patterson, David A.},
  month		= feb,
  year		= {2019},
  note		= {arXiv: 1902.03383},
  keywords	= {Computer Science - Operating Systems},
  file		= {Jonas et al. - 2019 - Cloud Programming Simplified A
		  Berkeley View on
		  S.pdf:/home/prateeks/Zotero/storage/33K5MS3Z/Jonas et al. -
		  2019 - Cloud Programming Simplified A Berkeley View on
		  S.pdf:application/pdf}
}

###Article{	  jonas_cloud_2019,
  title		= {Cloud {Programming} {Simplified}: {A} {Berkeley} {View} on
		  {Serverless} {Computing}},
  shorttitle	= {Cloud {Programming} {Simplified}},
  url		= {http://arxiv.org/abs/1902.03383},
  abstract	= {Serverless cloud computing handles virtually all the
		  system administration operations needed to make it easier
		  for programmers to use the cloud. It provides an interface
		  that greatly simpliﬁes cloud programming, and represents
		  an evolution that parallels the transition from assembly
		  language to high-level programming languages. This paper
		  gives a quick history of cloud computing, including an
		  accounting of the predictions of the 2009 Berkeley View of
		  Cloud Computing paper, explains the motivation for
		  serverless computing, describes applications that stretch
		  the current limits of serverless, and then lists obstacles
		  and research opportunities required for serverless
		  computing to fulﬁll its full potential. Just as the 2009
		  paper identiﬁed challenges for the cloud and predicted
		  they would be addressed and that cloud use would
		  accelerate, we predict these issues are solvable and that
		  serverless computing will grow to dominate the future of
		  cloud computing.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1902.03383 [cs]},
  author	= {Jonas, Eric and Schleier-Smith, Johann and Sreekanti,
		  Vikram and Tsai, Chia-Che and Khandelwal, Anurag and Pu,
		  Qifan and Shankar, Vaishaal and Carreira, Joao and Krauth,
		  Karl and Yadwadkar, Neeraja and Gonzalez, Joseph E. and
		  Popa, Raluca Ada and Stoica, Ion and Patterson, David A.},
  month		= feb,
  year		= {2019},
  note		= {arXiv: 1902.03383},
  keywords	= {Computer Science - Operating Systems},
  file		= {Jonas et al. - 2019 - Cloud Programming Simplified A
		  Berkeley View on
		  S.pdf:/home/prateeks/Zotero/storage/33K5MS3Z/Jonas et al. -
		  2019 - Cloud Programming Simplified A Berkeley View on
		  S.pdf:application/pdf}
}

@Article{	  joyner_ripple_2020,
  title		= {Ripple: {A} {Practical} {Declarative} {Programming}
		  {Framework} for {Serverless} {Compute}},
  shorttitle	= {Ripple},
  url		= {http://arxiv.org/abs/2001.00222},
  abstract	= {Serverless computing has emerged as a promising
		  alternative to infrastructure- (IaaS) and
		  platform-as-a-service (PaaS) cloud platforms for
		  applications with ample parallelism and intermittent
		  activity. Serverless promises greater resource elasticity,
		  signiﬁcant cost savings, and simpliﬁed application
		  deployment. All major cloud providers, including Amazon,
		  Google, and Microsoft, have introduced serverless to their
		  public cloud offerings. For serverless to reach its
		  potential, there is a pressing need for programming
		  frameworks that abstract the deployment complexity away
		  from the user. This includes simplifying the process of
		  writing applications for serverless environments,
		  automating task and data partitioning, and handling
		  scheduling and fault tolerance. We present Ripple, a
		  programming framework designed to speciﬁcally take
		  applications written for single-machine execution and allow
		  them to take advantage of the task parallelism of
		  serverless. Ripple exposes a simple interface that users
		  can leverage to express the high-level dataﬂow of a wide
		  spectrum of applications, including machine learning (ML)
		  analytics, genomics, and proteomics. Ripple also automates
		  resource provisioning, meeting user-deﬁned QoS targets,
		  and handles fault tolerance by eagerly detecting straggler
		  tasks. We port Ripple over AWS Lambda and show that, across
		  a set of diverse applications, it provides an expressive
		  and generalizable programming framework that simpliﬁes
		  running data-parallel applications on serverless, and can
		  improve performance by up to 80x compared to IaaS/PaaS
		  clouds for similar costs.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:2001.00222 [cs]},
  author	= {Joyner, Shannon and MacCoss, Michael and Delimitrou,
		  Christina and Weatherspoon, Hakim},
  month		= jan,
  year		= {2020},
  note		= {arXiv: 2001.00222},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  file		= {Joyner et al. - 2020 - Ripple A Practical Declarative
		  Programming
		  Framew.pdf:/home/prateeks/Zotero/storage/4MI8G7BU/Joyner et
		  al. - 2020 - Ripple A Practical Declarative Programming
		  Framew.pdf:application/pdf}
}

###Article{	  joyner_ripple_2020,
  title		= {Ripple: {A} {Practical} {Declarative} {Programming}
		  {Framework} for {Serverless} {Compute}},
  shorttitle	= {Ripple},
  url		= {http://arxiv.org/abs/2001.00222},
  abstract	= {Serverless computing has emerged as a promising
		  alternative to infrastructure- (IaaS) and
		  platform-as-a-service (PaaS) cloud platforms for
		  applications with ample parallelism and intermittent
		  activity. Serverless promises greater resource elasticity,
		  signiﬁcant cost savings, and simpliﬁed application
		  deployment. All major cloud providers, including Amazon,
		  Google, and Microsoft, have introduced serverless to their
		  public cloud offerings. For serverless to reach its
		  potential, there is a pressing need for programming
		  frameworks that abstract the deployment complexity away
		  from the user. This includes simplifying the process of
		  writing applications for serverless environments,
		  automating task and data partitioning, and handling
		  scheduling and fault tolerance. We present Ripple, a
		  programming framework designed to speciﬁcally take
		  applications written for single-machine execution and allow
		  them to take advantage of the task parallelism of
		  serverless. Ripple exposes a simple interface that users
		  can leverage to express the high-level dataﬂow of a wide
		  spectrum of applications, including machine learning (ML)
		  analytics, genomics, and proteomics. Ripple also automates
		  resource provisioning, meeting user-deﬁned QoS targets,
		  and handles fault tolerance by eagerly detecting straggler
		  tasks. We port Ripple over AWS Lambda and show that, across
		  a set of diverse applications, it provides an expressive
		  and generalizable programming framework that simpliﬁes
		  running data-parallel applications on serverless, and can
		  improve performance by up to 80x compared to IaaS/PaaS
		  clouds for similar costs.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:2001.00222 [cs]},
  author	= {Joyner, Shannon and MacCoss, Michael and Delimitrou,
		  Christina and Weatherspoon, Hakim},
  month		= jan,
  year		= {2020},
  note		= {arXiv: 2001.00222},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  file		= {Joyner et al. - 2020 - Ripple A Practical Declarative
		  Programming
		  Framew.pdf:/home/prateeks/Zotero/storage/4MI8G7BU/Joyner et
		  al. - 2020 - Ripple A Practical Declarative Programming
		  Framew.pdf:application/pdf}
}

@InProceedings{	  kaffes2019centralized,
  title		= {Centralized core-granular scheduling for serverless
		  functions},
  author	= {Kaffes, Kostis and Yadwadkar, Neeraja J and Kozyrakis,
		  Christos},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {158--164},
  year		= {2019}
}

@InProceedings{	  kaffes2019shinjuku,
  title		= {Shinjuku: Preemptive scheduling for $\mu$ second-scale
		  tail latency},
  author	= {Kaffes, Kostis and Chong, Timothy and Humphries, Jack
		  Tigar and Belay, Adam and Mazi{\`e}res, David and
		  Kozyrakis, Christos},
  booktitle	= {16th USENIX Symposium on Networked Systems Design and
		  Implementation (NSDI 19)},
  pages		= {345--360},
  year		= {2019}
}
@inproceedings{alzayat2023groundhog,
  title={Groundhog: Efficient request isolation in FaaS},
  author={Alzayat, Mohamed and Mace, Jonathan and Druschel, Peter and Garg, Deepak},
  booktitle={Proceedings of the Eighteenth European Conference on Computer Systems},
  pages={398--415},
  year={2023}
}

@InProceedings{	  kaffes_centralized_2019,
  address	= {Santa Cruz CA USA},
  title		= {Centralized {Core}-granular {Scheduling} for {Serverless}
		  {Functions}},
  isbn		= {978-1-4503-6973-2},
  url		= {https://dl.acm.org/doi/10.1145/3357223.3362709},
  doi		= {10.1145/3357223.3362709},
  abstract	= {In recent years, many applications have started using
		  serverless computing platforms primarily due to the ease of
		  deployment and cost efficiency they offer. However, the
		  existing scheduling mechanisms of serverless platforms fall
		  short in catering to the unique characteristics of such
		  applications: burstiness, short and variable execution
		  times, statelessness and use of a single core.
		  Specifically, the existing mechanisms fall short in meeting
		  the requirements generated due to the combined effect of
		  these characteristics: scheduling at a scale of millions of
		  function invocations per second while achieving predictable
		  performance. In this paper, we argue for a cluster-level
		  centralized and coregranular scheduler for serverless
		  functions. By maintaining a global view of the cluster
		  resources, the centralized approach eliminates queue
		  imbalances while the core granularity reduces interference;
		  together these properties enable reduced performance
		  variability. We expect such a scheduler to increase the
		  adoption of serverless computing platforms by various
		  latency and throughput sensitive applications.},
  language	= {en},
  urldate	= {2023-01-09},
  booktitle	= {Proceedings of the {ACM} {Symposium} on {Cloud}
		  {Computing}},
  publisher	= {ACM},
  author	= {Kaffes, Kostis and Yadwadkar, Neeraja J. and Kozyrakis,
		  Christos},
  month		= nov,
  year		= {2019},
  pages		= {158--164},
  file		= {Kaffes et al. - 2019 - Centralized Core-granular
		  Scheduling for
		  Serverles.pdf:/home/prateeks/Zotero/storage/XZUKL2VP/Kaffes
		  et al. - 2019 - Centralized Core-granular Scheduling for
		  Serverles.pdf:application/pdf}
}

@InProceedings{	  kaffes_hermod_2022,
  address	= {San Francisco California},
  title		= {Hermod: principled and practical scheduling for serverless
		  functions},
  isbn		= {978-1-4503-9414-7},
  shorttitle	= {Hermod},
  url		= {https://dl.acm.org/doi/10.1145/3542929.3563468},
  doi		= {10.1145/3542929.3563468},
  abstract	= {Serverless computing has seen rapid growth due to the
		  easeof-use and cost-e�ciency it provides. However,
		  function scheduling, a critical component of serverless
		  systems, has been overlooked. In this paper, we take a
		  �rst-principles approach toward designing a scheduler
		  that caters to the unique characteristics of serverless
		  functions as seen in realworld deployments. We �rst
		  create a taxonomy of scheduling policies along three
		  dimensions. Next, we use simulation to explore the
		  scheduling policy space and show that frequently used
		  features such as late binding and random load balancing are
		  sub-optimal for common execution time distributions and
		  load ranges. We use these insights to design Hermod, a
		  scheduler for serverless functions with two key
		  characteristics. First, to avoid head-of-line blocking due
		  to high function execution time variability, Hermod uses a
		  combination of early binding and processor sharing for
		  scheduling at individual worker machines. Second, Hermod is
		  cost, load, and locality-aware. It improves consolidation
		  at low load, it employs least-loaded balancing at high load
		  to retain high performance, and it reduces the number of
		  cold starts compared to pure load-based policies. We
		  implement Hermod for Apache OpenWhisk and demonstrate that,
		  for the case of the function patterns observed in
		  real-world traces, it achieves up to 85\% lower function
		  slowdown and 60\% higher throughput compared to existing
		  production and state-of-the-art research schedulers.},
  language	= {en},
  urldate	= {2023-01-09},
  booktitle	= {Proceedings of the 13th {Symposium} on {Cloud}
		  {Computing}},
  publisher	= {ACM},
  author	= {Kaffes, Kostis and Yadwadkar, Neeraja J. and Kozyrakis,
		  Christos},
  month		= nov,
  year		= {2022},
  pages		= {289--305},
  file		= {Kaffes et al. - 2022 - Hermod principled and practical
		  scheduling for
		  se.pdf:/home/prateeks/Zotero/storage/NF8SFVSP/Kaffes et al.
		  - 2022 - Hermod principled and practical scheduling for
		  se.pdf:application/pdf}
}

@Misc{		  kafka,
  title		= {{Apache Kafka: Open Source Distributed Event Streaming
		  Platform}},
  howpublished	= {\url{https://kafka.apache.org/}},
  year		= {2020}
}

###Misc{	  kafka,
  title		= {Apache Kafka},
  howpublished	= {\url{https://kafka.apache.org/}}
}

@InProceedings{	  kannan_grandslam_2019,
  address	= {Dresden, Germany},
  title		= {{GrandSLAm}: {Guaranteeing} {SLAs} for {Jobs} in
		  {Microservices} {Execution} {Frameworks}},
  isbn		= {978-1-4503-6281-8},
  shorttitle	= {{GrandSLAm}},
  url		= {http://dl.acm.org/citation.cfm?doid=3302424.3303958},
  doi		= {10.1145/3302424.3303958},
  abstract	= {The microservice architecture has dramatically reduced
		  user effort in adopting and maintaining servers by
		  providing a catalog of functions as services that can be
		  used as building blocks to construct applications. This has
		  enabled datacenter operators to look at managing datacenter
		  hosting microservices quite differently from traditional
		  infrastructures. Such a paradigm shift calls for a need to
		  rethink resource management strategies employed in such
		  execution environments. We observe that the visibility
		  enabled by a microservices execution framework can be
		  exploited to achieve high throughput and resource
		  utilization while still meeting Service Level Agreements,
		  especially in multi-tenant execution scenarios. In this
		  study, we present GrandSLAm, a microservice execution
		  framework that improves utilization of datacenters hosting
		  microservices. GrandSLAm estimates time of completion of
		  requests propagating through individual microservice stages
		  within an application. It then leverages this estimate to
		  drive a runtime system that dynamically batches and
		  reorders requests at each microservice in a manner where
		  individual jobs meet their respective target latency while
		  achieving high throughput. GrandSLAm significantly
		  increases throughput by up to 3× compared to the our
		  baseline, without violating SLAs for a wide range of
		  real-world AI and ML applications.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {Fourteenth} {EuroSys} {Conference}
		  2019 {CD}-{ROM} on {ZZZ} - {EuroSys} '19},
  publisher	= {ACM Press},
  author	= {Kannan, Ram Srivatsa and Subramanian, Lavanya and Raju,
		  Ashwin and Ahn, Jeongseob and Mars, Jason and Tang,
		  Lingjia},
  year		= {2019},
  pages		= {1--16},
  file		= {Kannan et al. - 2019 - GrandSLAm Guaranteeing SLAs for
		  Jobs in
		  Microserv.pdf:/home/prateeks/Zotero/storage/F2IZXGBL/Kannan
		  et al. - 2019 - GrandSLAm Guaranteeing SLAs for Jobs in
		  Microserv.pdf:application/pdf}
}

###InProceedings{ kannan_grandslam_2019,
  address	= {Dresden, Germany},
  title		= {{GrandSLAm}: {Guaranteeing} {SLAs} for {Jobs} in
		  {Microservices} {Execution} {Frameworks}},
  isbn		= {978-1-4503-6281-8},
  shorttitle	= {{GrandSLAm}},
  url		= {http://dl.acm.org/citation.cfm?doid=3302424.3303958},
  doi		= {10.1145/3302424.3303958},
  abstract	= {The microservice architecture has dramatically reduced
		  user effort in adopting and maintaining servers by
		  providing a catalog of functions as services that can be
		  used as building blocks to construct applications. This has
		  enabled datacenter operators to look at managing datacenter
		  hosting microservices quite differently from traditional
		  infrastructures. Such a paradigm shift calls for a need to
		  rethink resource management strategies employed in such
		  execution environments. We observe that the visibility
		  enabled by a microservices execution framework can be
		  exploited to achieve high throughput and resource
		  utilization while still meeting Service Level Agreements,
		  especially in multi-tenant execution scenarios. In this
		  study, we present GrandSLAm, a microservice execution
		  framework that improves utilization of datacenters hosting
		  microservices. GrandSLAm estimates time of completion of
		  requests propagating through individual microservice stages
		  within an application. It then leverages this estimate to
		  drive a runtime system that dynamically batches and
		  reorders requests at each microservice in a manner where
		  individual jobs meet their respective target latency while
		  achieving high throughput. GrandSLAm significantly
		  increases throughput by up to 3× compared to the our
		  baseline, without violating SLAs for a wide range of
		  real-world AI and ML applications.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {Fourteenth} {EuroSys} {Conference}
		  2019 {CD}-{ROM} on {ZZZ} - {EuroSys} '19},
  publisher	= {ACM Press},
  author	= {Kannan, Ram Srivatsa and Subramanian, Lavanya and Raju,
		  Ashwin and Ahn, Jeongseob and Mars, Jason and Tang,
		  Lingjia},
  year		= {2019},
  pages		= {1--16},
  file		= {Kannan et al. - 2019 - GrandSLAm Guaranteeing SLAs for
		  Jobs in
		  Microserv.pdf:/home/prateeks/Zotero/storage/F2IZXGBL/Kannan
		  et al. - 2019 - GrandSLAm Guaranteeing SLAs for Jobs in
		  Microserv.pdf:application/pdf}
}

@Article{	  kansal_fine-grained_2008,
  title		= {Fine-grained energy profiling for power-aware application
		  design},
  volume	= {36},
  issn		= {0163-5999},
  url		= {https://doi.org/10.1145/1453175.1453180},
  doi		= {10.1145/1453175.1453180},
  abstract	= {Significant opportunities for power optimization exist at
		  application design stage and are not yet fully exploited by
		  system and application designers. We describe the
		  challenges developers face in optimizing software for
		  energy efficiency by exploiting application-level
		  knowledge. To address these challenges, we propose the
		  development of automated tools that profile the energy
		  usage of various resource components used by an application
		  and guide the design choices accordingly. We use a
		  preliminary version of a tool we have developed to
		  demonstrate how automated energy profiling helps a
		  developer choose between alternative designs in the
		  energy-performance trade-off space.},
  number	= {2},
  urldate	= {2022-10-11},
  journal	= {ACM SIGMETRICS Performance Evaluation Review},
  author	= {Kansal, Aman and Zhao, Feng},
  month		= aug,
  year		= {2008},
  pages		= {26--31},
  file		= {Full Text
		  PDF:/home/prateeks/Zotero/storage/XKLQRZ6Y/Kansal and Zhao
		  - 2008 - Fine-grained energy profiling for power-aware
		  appl.pdf:application/pdf}
}

@InProceedings{	  karger1997consistent,
  title		= {Consistent hashing and random trees: Distributed caching
		  protocols for relieving hot spots on the world wide web},
  author	= {Karger, David and Lehman, Eric and Leighton, Tom and
		  Panigrahy, Rina and Levine, Matthew and Lewin, Daniel},
  booktitle	= {Proceedings of the twenty-ninth annual ACM symposium on
		  Theory of computing},
  pages		= {654--663},
  year		= {1997}
}

@Article{	  karger1999web,
  title		= {Web caching with consistent hashing},
  author	= {Karger, David and Sherman, Alex and Berkheimer, Andy and
		  Bogstad, Bill and Dhanidina, Rizwan and Iwamoto, Ken and
		  Kim, Brian and Matkins, Luke and Yerushalmi, Yoav},
  journal	= {Computer Networks},
  volume	= {31},
  number	= {11-16},
  pages		= {1203--1213},
  year		= {1999},
  publisher	= {Elsevier}
}

@InProceedings{	  karhula_checkpointing_2019,
  address	= {Dresden, Germany},
  title		= {Checkpointing and {Migration} of {IoT} {Edge}
		  {Functions}},
  isbn		= {978-1-4503-6275-7},
  url		= {http://dl.acm.org/citation.cfm?doid=3301418.3313947},
  doi		= {10.1145/3301418.3313947},
  abstract	= {The serverless and functions as a service (FaaS) paradigms
		  are currently trending among cloud providers and are now
		  increasingly being applied to the network edge, and to the
		  Internet of Things (IoT) devices. The benefits include
		  reduced latency for communication, less network traffic and
		  increased privacy for data processing. However, there are
		  challenges as IoT devices have limited resources for
		  running multiple simultaneous containerized functions, and
		  also FaaS does not typically support long-running
		  functions. Our implementation utilizes Docker and CRIU for
		  checkpointing and suspending long-running blocking
		  functions. The results show that checkpointing is slightly
		  slower than regular Docker pause, but it saves memory and
		  allows for more long-running functions to be run on an IoT
		  device. Furthermore, the resulting checkpoint files are
		  small, hence they are suitable for live migration and
		  backing up stateful functions, therefore improving
		  availability and reliability of the system.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 2nd {International} {Workshop} on
		  {Edge} {Systems}, {Analytics} and {Networking} - {EdgeSys}
		  '19},
  publisher	= {ACM Press},
  author	= {Karhula, Pekka and Janak, Jan and Schulzrinne, Henning},
  year		= {2019},
  pages		= {60--65},
  file		= {Karhula et al. - 2019 - Checkpointing and Migration of IoT
		  Edge
		  Functions.pdf:/home/prateeks/Zotero/storage/XVMPRXRL/Karhula
		  et al. - 2019 - Checkpointing and Migration of IoT Edge
		  Functions.pdf:application/pdf}
}

###InProceedings{ karhula_checkpointing_2019,
  address	= {Dresden, Germany},
  title		= {Checkpointing and {Migration} of {IoT} {Edge}
		  {Functions}},
  isbn		= {978-1-4503-6275-7},
  url		= {http://dl.acm.org/citation.cfm?doid=3301418.3313947},
  doi		= {10.1145/3301418.3313947},
  abstract	= {The serverless and functions as a service (FaaS) paradigms
		  are currently trending among cloud providers and are now
		  increasingly being applied to the network edge, and to the
		  Internet of Things (IoT) devices. The benefits include
		  reduced latency for communication, less network traffic and
		  increased privacy for data processing. However, there are
		  challenges as IoT devices have limited resources for
		  running multiple simultaneous containerized functions, and
		  also FaaS does not typically support long-running
		  functions. Our implementation utilizes Docker and CRIU for
		  checkpointing and suspending long-running blocking
		  functions. The results show that checkpointing is slightly
		  slower than regular Docker pause, but it saves memory and
		  allows for more long-running functions to be run on an IoT
		  device. Furthermore, the resulting checkpoint files are
		  small, hence they are suitable for live migration and
		  backing up stateful functions, therefore improving
		  availability and reliability of the system.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 2nd {International} {Workshop} on
		  {Edge} {Systems}, {Analytics} and {Networking} - {EdgeSys}
		  '19},
  publisher	= {ACM Press},
  author	= {Karhula, Pekka and Janak, Jan and Schulzrinne, Henning},
  year		= {2019},
  pages		= {60--65},
  file		= {Karhula et al. - 2019 - Checkpointing and Migration of IoT
		  Edge
		  Functions.pdf:/home/prateeks/Zotero/storage/XVMPRXRL/Karhula
		  et al. - 2019 - Checkpointing and Migration of IoT Edge
		  Functions.pdf:application/pdf}
}

@InProceedings{	  kelly_neural_2015,
  address	= {Seoul South Korea},
  title		= {Neural {NILM}: {Deep} {Neural} {Networks} {Applied} to
		  {Energy} {Disaggregation}},
  isbn		= {978-1-4503-3981-0},
  shorttitle	= {Neural {NILM}},
  url		= {https://dl.acm.org/doi/10.1145/2821650.2821672},
  doi		= {10.1145/2821650.2821672},
  abstract	= {Energy disaggregation estimates appliance-by-appliance
		  electricity consumption from a single meter that measures
		  the whole home’s electricity demand. Recently, deep
		  neural networks have driven remarkable improvements in
		  classiﬁcation performance in neighbouring machine
		  learning ﬁelds such as image classiﬁcation and
		  automatic speech recognition. In this paper, we adapt three
		  deep neural network architectures to energy disaggregation:
		  1) a form of recurrent neural network called ‘long
		  short-term memory’ (LSTM); 2) denoising autoencoders; and
		  3) a network which regresses the start time, end time and
		  average power demand of each appliance activation. We use
		  seven metrics to test the performance of these algorithms
		  on real aggregate power data from ﬁve appliances. Tests
		  are performed against a house not seen during training and
		  against houses seen during training. We ﬁnd that all
		  three neural nets achieve better F1 scores (averaged over
		  all ﬁve appliances) than either combinatorial
		  optimisation or factorial hidden Markov models and that our
		  neural net algorithms generalise well to an unseen house.},
  language	= {en},
  urldate	= {2022-09-30},
  booktitle	= {Proceedings of the 2nd {ACM} {International} {Conference}
		  on {Embedded} {Systems} for {Energy}-{Efficient} {Built}
		  {Environments}},
  publisher	= {ACM},
  author	= {Kelly, Jack and Knottenbelt, William},
  month		= nov,
  year		= {2015},
  pages		= {55--64},
  file		= {Kelly and Knottenbelt - 2015 - Neural NILM Deep Neural
		  Networks Applied to
		  Energ.pdf:/home/prateeks/Zotero/storage/KDUQ4SQU/Kelly and
		  Knottenbelt - 2015 - Neural NILM Deep Neural Networks
		  Applied to Energ.pdf:application/pdf}
}

@Article{	  khan_rapl_2018,
  title		= {{RAPL} in {Action}: {Experiences} in {Using} {RAPL} for
		  {Power} {Measurements}},
  volume	= {3},
  issn		= {2376-3639, 2376-3647},
  shorttitle	= {{RAPL} in {Action}},
  url		= {https://dl.acm.org/doi/10.1145/3177754},
  doi		= {10.1145/3177754},
  abstract	= {To improve energy efficiency and comply with the power
		  budgets, it is important to be able to measure the power
		  consumption of cloud computing servers. Intel’s Running
		  Average Power Limit (RAPL) interface is a powerful tool for
		  this purpose. RAPL provides power limiting features and
		  accurate energy readings for CPUs and DRAM, which are
		  easily accessible through different interfaces on large
		  distributed computing systems. Since its introduction, RAPL
		  has been used extensively in power measurement and
		  modeling. However, the advantages and disadvantages of RAPL
		  have not been well investigated yet. To fill this gap, we
		  conduct a series of experiments to disclose the underlying
		  strengths and weaknesses of the RAPL interface by using
		  both customized microbenchmarks and three well-known
		  application level benchmarks: Stream , Stress-ng , and
		  ParFullCMS . Moreover, to make the analysis as realistic as
		  possible, we leverage two production-level power
		  measurement datasets from the Taito , a supercomputing
		  cluster of the Finnish Center of Scientific Computing and
		  also replicate our experiments on Amazon EC2. Our results
		  illustrate different aspects of RAPL and document the
		  findings through comprehensive analysis. Our observations
		  reveal that RAPL readings are highly correlated with plug
		  power, promisingly accurate enough, and have negligible
		  performance overhead. Experimental results suggest RAPL can
		  be a very useful tool to measure and monitor the energy
		  consumption of servers without deploying any complex power
		  meters. We also show that there are still some open issues,
		  such as driver support, non-atomicity of register updates,
		  and unpredictable timings that might weaken the usability
		  of RAPL in certain scenarios. For such scenarios, we
		  pinpoint solutions and workarounds.},
  language	= {en},
  number	= {2},
  urldate	= {2022-07-15},
  journal	= {ACM Transactions on Modeling and Performance Evaluation of
		  Computing Systems},
  author	= {Khan, Kashif Nizam and Hirki, Mikael and Niemi, Tapio and
		  Nurminen, Jukka K. and Ou, Zhonghong},
  month		= jun,
  year		= {2018},
  pages		= {1--26},
  file		= {Khan et al. - 2018 - RAPL in Action Experiences in Using
		  RAPL for
		  Powe.pdf:/home/prateeks/Zotero/storage/AAT77N75/Khan et al.
		  - 2018 - RAPL in Action Experiences in Using RAPL for
		  Powe.pdf:application/pdf}
}

@InProceedings{	  khandelwal2020taureau,
  title		= {Le Taureau: Deconstructing the serverless landscape \& A
		  look forward},
  author	= {Khandelwal, Anurag and Kejariwal, Arun and Ramasamy,
		  Karthikeyan},
  booktitle	= {Proceedings of the 2020 ACM SIGMOD International
		  Conference on Management of Data},
  pages		= {2641--2650},
  year		= {2020}
}

@InProceedings{	  kidambi_dynamic_1999,
  address	= {Boston, MA, USA},
  title		= {Dynamic token bucket ({DTB}): a fair bandwidth allocation
		  algorithm for high-speed networks},
  isbn		= {978-0-7803-5794-5},
  shorttitle	= {Dynamic token bucket ({DTB})},
  url		= {http://ieeexplore.ieee.org/document/805491/},
  doi		= {10.1109/ICCCN.1999.805491},
  abstract	= {Fair allocation of available bandwidth to competing ﬂows
		  is a simple form of quality of service (QoS) that can be
		  provided to customers in packet-switched networks. A number
		  of packet-scheduling and buffer-management techniques have
		  been proposed in the literature to achieve this goal
		  efﬁciently. However, the complexity of the existing
		  algorithms prevents a high-speed implementation with the
		  current state of router technology. We propose a
		  computationally simpler mechanism based on token-bucket
		  policing to achieve almost equal bandwidth allocation for a
		  set of competing ﬂows. The proposed method adjusts the
		  token-bucket threshold dynamically and measures the
		  instantaneous arrival rate of ﬂows. It uses this
		  information to decide whether or not to admit a packet
		  arriving at the network edge. With minor modiﬁcations,
		  our framework can be used in the Internet and Frame Relay
		  based virtual private networks (VPNs). We present a
		  detailed simulation study that evaluates the performance of
		  our algorithm. The simulation results indicate that DTB is
		  fair, efﬁcient, and robust.},
  language	= {en},
  urldate	= {2022-09-21},
  booktitle	= {Proceedings {Eight} {International} {Conference} on
		  {Computer} {Communications} and {Networks} ({Cat}.
		  {No}.{99EX370})},
  publisher	= {IEEE},
  author	= {Kidambi, J. and Ghosal, D. and Mukherjee, B.},
  year		= {1999},
  pages		= {24--29},
  file		= {Kidambi et al. - 1999 - Dynamic token bucket (DTB) a fair
		  bandwidth
		  alloc.pdf:/home/prateeks/Zotero/storage/HN92SYXR/Kidambi et
		  al. - 1999 - Dynamic token bucket (DTB) a fair bandwidth
		  alloc.pdf:application/pdf}
}

@InProceedings{	  kim2019functionbench,
  title		= {Functionbench: A suite of workloads for serverless cloud
		  function service},
  author	= {Kim, Jeongchul and Lee, Kyungyong},
  booktitle	= {2019 IEEE 12th International Conference on Cloud Computing
		  (CLOUD)},
  pages		= {502--504},
  year		= {2019},
  organization	= {IEEE}
}

@InProceedings{	  kim_functionbench_2019,
  title		= {{FunctionBench}: {A} {Suite} of {Workloads} for
		  {Serverless} {Cloud} {Function} {Service}},
  shorttitle	= {{FunctionBench}},
  doi		= {10.1109/CLOUD.2019.00091},
  abstract	= {Serverless computing is attracting considerable attention
		  recently, but many published papers use micro-benchmarks
		  for evaluation that might result in impracticality. To
		  address this, we present FunctionBench, a suite of
		  practical function workloads for public services. It
		  contains realistic data-oriented applications that utilize
		  various resources during execution. The source codes
		  customized for various cloud service providers are publicly
		  available. We are positive that it suggests opportunities
		  for new function applications with lessen experiment setup
		  overheads.},
  booktitle	= {2019 {IEEE} 12th {International} {Conference} on {Cloud}
		  {Computing} ({CLOUD})},
  author	= {Kim, Jeongchul and Lee, Kyungyong},
  month		= jul,
  year		= {2019},
  note		= {ISSN: 2159-6182},
  keywords	= {cloud, cloud computing, FaaS, serverless, benchmark, cloud
		  service providers, data-oriented applications, function
		  applications, function workloads, FunctionBench,
		  microbenchmarks, public services, serverless cloud function
		  service, serverless computing, source codes, workload},
  pages		= {502--504},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/436476Y3/8814583.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/6RIZVX2H/Kim and Lee -
		  2019 - FunctionBench A Suite of Workloads for
		  Serverless.pdf:application/pdf}
}

@InProceedings{	  kim_scheduling_2021,
  title		= {Scheduling {Containers} {Rather} {Than} {Functions} for
		  {Function}-as-a-{Service}},
  doi		= {10.1109/CCGrid51090.2021.00056},
  abstract	= {Function-as-a-Service (FaaS) is a compelling technology
		  that allows users to run functions in an event-driven way
		  without concerns about server management. Container-based
		  virtualization enables functions to run in a lightweight
		  and isolated run-time environment, but frequent function
		  executions accompanied with container initialization (cold
		  starts) make the platform busy and unresponsive. For
		  performance sake, warm starts, which is to execute
		  functions on already initialized containers, are
		  encouraged, and thus FaaS platforms make efforts to
		  schedule functions to warm containers.From our experience
		  operating an on-premise FaaS platform, we found that the
		  existing scheduler showed poor performance and unstable
		  behavior against multi-tenant and highly concurrent
		  workloads. This paper proposes a novel FaaS scheduling
		  algorithm, named FPCSch, that schedules
		  Function-Pulling-Containers instead of scheduling functions
		  to containers. As FPCSch lets containers continuously pull
		  functions of the same type, cold starts decrease
		  dramatically. Our evaluations show that Apache OpenWhisk
		  equipped with FPCSch has many desirable features for FaaS
		  platforms; (1) quite stable throughput against the
		  multi-tenant workloads mixed by the increasing numbers of
		  function types, (2) growing throughput for increasing
		  concurrency, (3) uniformly load-balancing
		  resource-intensive workloads, and (4) nearly proportional
		  performance for scale-out.},
  booktitle	= {2021 {IEEE}/{ACM} 21st {International} {Symposium} on
		  {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
  author	= {Kim, Dong Kyoung and Roh, Hyun-Gul},
  month		= may,
  year		= {2021},
  keywords	= {Apache OpenWhisk, Cloud, Cloud computing, Cold Starts,
		  Concurrent computing, Container, Containers, FAA, FaaS,
		  FaaS platform, Function as a Service, Load balancing,
		  OpenWhisk, Scalability, Scale out, Schedules, Scheduling,
		  Scheduling algorithms, Servereless, Throughput,
		  Virtualization},
  pages		= {465--474},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/HFC5W97W/stamp.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/Y6TPI2R7/Kim and Roh -
		  2021 - Scheduling Containers Rather Than Functions for
		  Fu.pdf:application/pdf}
}

@InProceedings{	  kluyver2016jupyter,
  title		= {Jupyter Notebooks-a publishing format for reproducible
		  computational workflows.},
  author	= {Kluyver, Thomas and Ragan-Kelley, Benjamin and P{\'e}rez,
		  Fernando and Granger, Brian E and Bussonnier, Matthias and
		  Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica B
		  and Grout, Jason and Corlay, Sylvain and others},
  booktitle	= {ELPUB},
  pages		= {87--90},
  year		= {2016}
}

@Misc{		  knative,
  title		= {{Knative is an Open-Source Enterprise-level solution to
		  build Serverless and Event Driven Applications}},
  howpublished	= {\url{https://knative.dev/docs/}},
  year		= {2023}
}

@InProceedings{	  kochura2019batch,
  title		= {Batch size influence on performance of graphic and tensor
		  processing units during training and inference phases},
  author	= {Kochura, Yuriy and Gordienko, Yuri and Taran, Vlad and
		  Gordienko, Nikita and Rokovyi, Alexandr and Alienin, Oleg
		  and Stirenko, Sergii},
  booktitle	= {International Conference on Computer Science, Engineering
		  and Education Applications},
  pages		= {658--668},
  year		= {2019},
  organization	= {Springer}
}

@Article{	  kolter_energy_2010,
  title		= {Energy {Disaggregation} via {Discriminative} {Sparse}
		  {Coding}},
  abstract	= {Energy disaggregation is the task of taking a whole-home
		  energy signal and separating it into its component
		  appliances. Studies have shown that having devicelevel
		  energy information can cause users to conserve signiﬁcant
		  amounts of energy, but current electricity meters only
		  report whole-home data. Thus, developing algorithmic
		  methods for disaggregation presents a key technical
		  challenge in the effort to maximize energy conservation. In
		  this paper, we examine a large scale energy disaggregation
		  task, and apply a novel extension of sparse coding to this
		  problem. In particular, we develop a method, based upon
		  structured prediction, for discriminatively training sparse
		  coding algorithms speciﬁcally to maximize disaggregation
		  performance. We show that this signiﬁcantly improves the
		  performance of sparse coding algorithms on the energy task
		  and illustrate how these disaggregation results can provide
		  useful information about energy usage.},
  language	= {en},
  journal	= {Neurips},
  author	= {Kolter, J Z and Batra, Siddharth and Ng, Andrew Y},
  year		= {2010},
  pages		= {9},
  file		= {Kolter et al. - Energy Disaggregation via Discriminative
		  Sparse Co.pdf:/home/prateeks/Zotero/storage/FUJ4887F/Kolter
		  et al. - Energy Disaggregation via Discriminative Sparse
		  Co.pdf:application/pdf}
}

@InProceedings{	  kotni2021faastlane,
  title		= {Faastlane: Accelerating Function-as-a-Service Workflows},
  author	= {Kotni, Swaroop and Nayak, Ajay and Ganapathy, Vinod and
		  Basu, Arkaprava},
  booktitle	= {2021 USENIX Annual Technical Conference (USENIXATC 21)},
  pages		= {805--820},
  year		= {2021}
}

@InProceedings{	  krol_computation_2018,
  address	= {Boston, Massachusetts},
  title		= {Computation offloading with {ICN}},
  isbn		= {978-1-4503-5959-7},
  url		= {http://dl.acm.org/citation.cfm?doid=3267955.3269009},
  doi		= {10.1145/3267955.3269009},
  abstract	= {This demo shows an implementation of a computation-centric
		  architecture over NDN. The system is able to perform
		  in-network load balancing of incoming computation requests,
		  reliably authenticate consumers and allow them to submit
		  large payloads without routable prefixes. The system is
		  able to migrate requested function in a form of unikernels
		  where they are needed, follows ICN pull-based model and
		  introduces only minimal changes to the NDN stack.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 5th {ACM} {Conference} on
		  {Information}-{Centric} {Networking} - {ICN} '18},
  publisher	= {ACM Press},
  author	= {Król, Michał and Nicolaescu, Adrian-Cristian and Reñé,
		  Sergi and Ascigil, Onur and Psaras, Ioannis and Oran, David
		  and Kutscher, Dirk},
  year		= {2018},
  pages		= {220--221},
  file		= {Król et al. - 2018 - Computation offloading with
		  ICN.pdf:/home/prateeks/Zotero/storage/V4CMVLSR/Król et al.
		  - 2018 - Computation offloading with
		  ICN.pdf:application/pdf}
}

###InProceedings{ krol_computation_2018,
  address	= {Boston, Massachusetts},
  title		= {Computation offloading with {ICN}},
  isbn		= {978-1-4503-5959-7},
  url		= {http://dl.acm.org/citation.cfm?doid=3267955.3269009},
  doi		= {10.1145/3267955.3269009},
  abstract	= {This demo shows an implementation of a computation-centric
		  architecture over NDN. The system is able to perform
		  in-network load balancing of incoming computation requests,
		  reliably authenticate consumers and allow them to submit
		  large payloads without routable prefixes. The system is
		  able to migrate requested function in a form of unikernels
		  where they are needed, follows ICN pull-based model and
		  introduces only minimal changes to the NDN stack.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 5th {ACM} {Conference} on
		  {Information}-{Centric} {Networking} - {ICN} '18},
  publisher	= {ACM Press},
  author	= {Król, Michał and Nicolaescu, Adrian-Cristian and Reñé,
		  Sergi and Ascigil, Onur and Psaras, Ioannis and Oran, David
		  and Kutscher, Dirk},
  year		= {2018},
  pages		= {220--221},
  file		= {Król et al. - 2018 - Computation offloading with
		  ICN.pdf:/home/prateeks/Zotero/storage/V4CMVLSR/Król et al.
		  - 2018 - Computation offloading with
		  ICN.pdf:application/pdf}
}

@InProceedings{	  krol_nfaas_2017,
  address	= {Berlin, Germany},
  title		= {{NFaaS}: named function as a service},
  isbn		= {978-1-4503-5122-5},
  shorttitle	= {{NFaaS}},
  url		= {http://dl.acm.org/citation.cfm?doid=3125719.3125727},
  doi		= {10.1145/3125719.3125727},
  abstract	= {In the past, the Information-centric networking (ICN)
		  community has focused on issues mainly pertaining to
		  traditional content delivery (e.g., routing and forwarding
		  scalability, congestion control and in-network caching).
		  However, to keep up with future Internet architectural
		  trends the wider area of future Internet paradigms, there
		  is a pressing need to support edge/fog computing
		  environments, where cloud functionality is available more
		  proximate to where the data is generated and needs
		  processing. With this goal in mind, we propose Named
		  Function as a Service (NFaaS), a framework that extends the
		  Named Data Networking architecture to support in-network
		  function execution. In contrast to existing works,
		  NFaaSbuilds on very lightweight VMs and allows for dynamic
		  execution of custom code. Functions can be downloaded and
		  run by any node in the network. Functions can move between
		  nodes according to user demand, making resolution of moving
		  functions a first-class challenge. NFaaSincludes a Kernel
		  Store component, which is responsible not only for storing
		  functions, but also for making decisions on which functions
		  to run locally. NFaaSincludes a routing protocol and a
		  number of forwarding strategies to deploy and dynamically
		  migrate functions within the network. We validate our
		  design through extensive simulations, which show that
		  delay-sensitive functions are deployed closer to the edge,
		  while less delay-sensitive ones closer to the core.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 4th {ACM} {Conference} on
		  {Information}-{Centric} {Networking} - {ICN} '17},
  publisher	= {ACM Press},
  author	= {Król, Michał and Psaras, Ioannis},
  year		= {2017},
  pages		= {134--144},
  file		= {Król and Psaras - 2017 - NFaaS named function as a
		  service.pdf:/home/prateeks/Zotero/storage/WUEYLHHF/Król
		  and Psaras - 2017 - NFaaS named function as a
		  service.pdf:application/pdf}
}

###InProceedings{ krol_nfaas_2017,
  address	= {Berlin, Germany},
  title		= {{NFaaS}: named function as a service},
  isbn		= {978-1-4503-5122-5},
  shorttitle	= {{NFaaS}},
  url		= {http://dl.acm.org/citation.cfm?doid=3125719.3125727},
  doi		= {10.1145/3125719.3125727},
  abstract	= {In the past, the Information-centric networking (ICN)
		  community has focused on issues mainly pertaining to
		  traditional content delivery (e.g., routing and forwarding
		  scalability, congestion control and in-network caching).
		  However, to keep up with future Internet architectural
		  trends the wider area of future Internet paradigms, there
		  is a pressing need to support edge/fog computing
		  environments, where cloud functionality is available more
		  proximate to where the data is generated and needs
		  processing. With this goal in mind, we propose Named
		  Function as a Service (NFaaS), a framework that extends the
		  Named Data Networking architecture to support in-network
		  function execution. In contrast to existing works,
		  NFaaSbuilds on very lightweight VMs and allows for dynamic
		  execution of custom code. Functions can be downloaded and
		  run by any node in the network. Functions can move between
		  nodes according to user demand, making resolution of moving
		  functions a first-class challenge. NFaaSincludes a Kernel
		  Store component, which is responsible not only for storing
		  functions, but also for making decisions on which functions
		  to run locally. NFaaSincludes a routing protocol and a
		  number of forwarding strategies to deploy and dynamically
		  migrate functions within the network. We validate our
		  design through extensive simulations, which show that
		  delay-sensitive functions are deployed closer to the edge,
		  while less delay-sensitive ones closer to the core.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 4th {ACM} {Conference} on
		  {Information}-{Centric} {Networking} - {ICN} '17},
  publisher	= {ACM Press},
  author	= {Król, Michał and Psaras, Ioannis},
  year		= {2017},
  pages		= {134--144},
  file		= {Król and Psaras - 2017 - NFaaS named function as a
		  service.pdf:/home/prateeks/Zotero/storage/WUEYLHHF/Król
		  and Psaras - 2017 - NFaaS named function as a
		  service.pdf:application/pdf}
}

###InProceedings{ krol_nfaas_2017,
  address	= {Berlin, Germany},
  title		= {{NFaaS}: named function as a service},
  isbn		= {978-1-4503-5122-5},
  shorttitle	= {{NFaaS}},
  url		= {http://dl.acm.org/citation.cfm?doid=3125719.3125727},
  doi		= {10.1145/3125719.3125727},
  abstract	= {In the past, the Information-centric networking (ICN)
		  community has focused on issues mainly pertaining to
		  traditional content delivery (e.g., routing and forwarding
		  scalability, congestion control and in-network caching).
		  However, to keep up with future Internet architectural
		  trends the wider area of future Internet paradigms, there
		  is a pressing need to support edge/fog computing
		  environments, where cloud functionality is available more
		  proximate to where the data is generated and needs
		  processing. With this goal in mind, we propose Named
		  Function as a Service (NFaaS), a framework that extends the
		  Named Data Networking architecture to support in-network
		  function execution. In contrast to existing works,
		  NFaaSbuilds on very lightweight VMs and allows for dynamic
		  execution of custom code. Functions can be downloaded and
		  run by any node in the network. Functions can move between
		  nodes according to user demand, making resolution of moving
		  functions a first-class challenge. NFaaSincludes a Kernel
		  Store component, which is responsible not only for storing
		  functions, but also for making decisions on which functions
		  to run locally. NFaaSincludes a routing protocol and a
		  number of forwarding strategies to deploy and dynamically
		  migrate functions within the network. We validate our
		  design through extensive simulations, which show that
		  delay-sensitive functions are deployed closer to the edge,
		  while less delay-sensitive ones closer to the core.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 4th {ACM} {Conference} on
		  {Information}-{Centric} {Networking} - {ICN} '17},
  publisher	= {ACM Press},
  author	= {Król, Michał and Psaras, Ioannis},
  year		= {2017},
  pages		= {134--144},
  file		= {Król and Psaras - 2017 - NFaaS named function as a
		  service.pdf:/home/prateeks/Zotero/storage/WUEYLHHF/Król
		  and Psaras - 2017 - NFaaS named function as a
		  service.pdf:application/pdf}
}

###InProceedings{ krol_nfaas_2017,
  address	= {Berlin, Germany},
  title		= {{NFaaS}: named function as a service},
  isbn		= {978-1-4503-5122-5},
  shorttitle	= {{NFaaS}},
  url		= {http://dl.acm.org/citation.cfm?doid=3125719.3125727},
  doi		= {10.1145/3125719.3125727},
  abstract	= {In the past, the Information-centric networking (ICN)
		  community has focused on issues mainly pertaining to
		  traditional content delivery (e.g., routing and forwarding
		  scalability, congestion control and in-network caching).
		  However, to keep up with future Internet architectural
		  trends the wider area of future Internet paradigms, there
		  is a pressing need to support edge/fog computing
		  environments, where cloud functionality is available more
		  proximate to where the data is generated and needs
		  processing. With this goal in mind, we propose Named
		  Function as a Service (NFaaS), a framework that extends the
		  Named Data Networking architecture to support in-network
		  function execution. In contrast to existing works,
		  NFaaSbuilds on very lightweight VMs and allows for dynamic
		  execution of custom code. Functions can be downloaded and
		  run by any node in the network. Functions can move between
		  nodes according to user demand, making resolution of moving
		  functions a first-class challenge. NFaaSincludes a Kernel
		  Store component, which is responsible not only for storing
		  functions, but also for making decisions on which functions
		  to run locally. NFaaSincludes a routing protocol and a
		  number of forwarding strategies to deploy and dynamically
		  migrate functions within the network. We validate our
		  design through extensive simulations, which show that
		  delay-sensitive functions are deployed closer to the edge,
		  while less delay-sensitive ones closer to the core.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 4th {ACM} {Conference} on
		  {Information}-{Centric} {Networking} - {ICN} '17},
  publisher	= {ACM Press},
  author	= {Król, Michał and Psaras, Ioannis},
  year		= {2017},
  pages		= {134--144},
  file		= {Król and Psaras - 2017 - NFaaS named function as a
		  service.pdf:/home/prateeks/Zotero/storage/WUEYLHHF/Król
		  and Psaras - 2017 - NFaaS named function as a
		  service.pdf:application/pdf}
}

@InProceedings{	  krzywda_power_2019,
  title		= {Power {Shepherd}: {Application} {Performance} {Aware}
		  {Power} {Shifting}},
  shorttitle	= {Power {Shepherd}},
  doi		= {10.1109/CloudCom.2019.00019},
  abstract	= {Constantly growing power consumption of data centers is a
		  major concern from environmental and economical reasons.
		  Current approaches to reduce negative consequences of high
		  power consumption focus on limiting the peak power
		  consumption. During high workload periods, power
		  consumption of highly utilized servers is throttled to stay
		  within the power budget. However, the peak power reduction
		  affects performance of hosted applications and thus leads
		  to Quality of Service violations. In this paper, we
		  introduce Power Shepherd, a hierarchical system for
		  application performance aware power shifting. Power
		  Shepherd reduces the data center operational costs by
		  redistributing the available power among applications
		  hosted in the cluster. This is achieved by, assigning
		  server power budgets by the cluster controller, enforcing
		  these power budgets using Running Average Power Limit
		  (RAPL), and prioritizing applications within each server by
		  adjusting the CPU scheduling configuration. We implement a
		  prototype of the proposed solution and evaluate it in a
		  real testbed equipped with power meters and using
		  representative cloud applications. Our experiments show
		  that Power Shepherd has potential to manage a cluster
		  consisting of thousands of servers and limit the increase
		  of operational costs by a significant amount when the
		  cluster power budget is limited and the system is
		  overutilized. Finally, we identify some outstanding
		  challenges regarding model sensitivity and the fact that
		  this approach in its current from is not beneficial to be
		  used in all situations, e.g., when the system is
		  underutilized.},
  booktitle	= {2019 {IEEE} {International} {Conference} on {Cloud}
		  {Computing} {Technology} and {Science} ({CloudCom})},
  author	= {Krzywda, Jakub and Ali-Eldin, Ahmed and Wadbro, Eddie and
		  Östberg, Per-Olov and Elmroth, Erik},
  month		= dec,
  year		= {2019},
  note		= {ISSN: 2330-2186},
  keywords	= {cloud computing, quality of service, power budgeting},
  pages		= {45--53},
  annote	= {- Power Shepherd: performance aware shifting. QoS, RAPL.
		  Multi-server ALPACA (single-server perf power model ICAC
		  18) },
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/LVXZIU3H/8968910.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/5AWLGYRN/Krzywda et al. -
		  2019 - Power Shepherd Application Performance Aware
		  Powe.pdf:application/pdf}
}

@InProceedings{	  kulkarni_living_2019,
  title		= {Living on the {Edge}: {Serverless} {Computing} and the
		  {Cost} of {Failure} {Resiliency}},
  shorttitle	= {Living on the {Edge}},
  doi		= {10.1109/LANMAN.2019.8846970},
  abstract	= {Serverless computing platforms have gained popularity
		  because they allow easy deployment of services in a highly
		  scalable and cost-effective manner. By enabling
		  just-in-time startup of container-based services, these
		  platforms can achieve good multiplexing and automatically
		  respond to traffic growth, making them particularly
		  desirable for edge cloud data centers where resources are
		  scarce. Edge cloud data centers are also gaining attention
		  because of their promise to provide responsive, low-latency
		  shared computing and storage resources. Bringing serverless
		  capabilities to edge cloud data centers must continue to
		  achieve the goals of low latency and reliability. The
		  reliability guarantees provided by serverless computing
		  however are weak, with node failures causing requests to be
		  dropped or executed multiple times. Thus serverless
		  computing only provides a best effort infrastructure,
		  leaving application developers responsible for implementing
		  stronger reliability guarantees at a higher level. Current
		  approaches for providing stronger semantics such as
		  “exactly once” guarantees could be integrated into
		  serverless platforms, but they come at high cost in terms
		  of both latency and resource consumption. As edge cloud
		  services move towards applications such as autonomous
		  vehicle control that require strong guarantees for both
		  reliability and performance, these approaches may no longer
		  be sufficient. In this paper we evaluate the latency,
		  throughput, and resource costs of providing different
		  reliability guarantees, with a focus on these emerging edge
		  cloud platforms and applications.},
  booktitle	= {2019 {IEEE} {International} {Symposium} on {Local} and
		  {Metropolitan} {Area} {Networks} ({LANMAN})},
  author	= {Kulkarni, Sameer G and Liu, Guyue and Ramakrishnan, K. K.
		  and Wood, Timothy},
  month		= jul,
  year		= {2019},
  note		= {ISSN: 1944-0367},
  keywords	= {cloud computing, Cloud computing, computer centres,
		  container-based services, Containers, Data centers, edge
		  cloud data centers, edge cloud platforms, edge cloud
		  services, Fasteners, fault tolerant computing, Reliability,
		  reliability guarantees, resource costs, Semantics,
		  serverless computing platforms, serverless platforms,
		  storage resources, Storms},
  pages		= {1--6},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/UNELACFE/8846970.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/23BA479D/Kulkarni et al.
		  - 2019 - Living on the Edge Serverless Computing and the
		  C.pdf:application/pdf}
}

###InProceedings{ kulkarni_living_2019,
  title		= {Living on the {Edge}: {Serverless} {Computing} and the
		  {Cost} of {Failure} {Resiliency}},
  shorttitle	= {Living on the {Edge}},
  doi		= {10.1109/LANMAN.2019.8846970},
  abstract	= {Serverless computing platforms have gained popularity
		  because they allow easy deployment of services in a highly
		  scalable and cost-effective manner. By enabling
		  just-in-time startup of container-based services, these
		  platforms can achieve good multiplexing and automatically
		  respond to traffic growth, making them particularly
		  desirable for edge cloud data centers where resources are
		  scarce. Edge cloud data centers are also gaining attention
		  because of their promise to provide responsive, low-latency
		  shared computing and storage resources. Bringing serverless
		  capabilities to edge cloud data centers must continue to
		  achieve the goals of low latency and reliability. The
		  reliability guarantees provided by serverless computing
		  however are weak, with node failures causing requests to be
		  dropped or executed multiple times. Thus serverless
		  computing only provides a best effort infrastructure,
		  leaving application developers responsible for implementing
		  stronger reliability guarantees at a higher level. Current
		  approaches for providing stronger semantics such as
		  “exactly once” guarantees could be integrated into
		  serverless platforms, but they come at high cost in terms
		  of both latency and resource consumption. As edge cloud
		  services move towards applications such as autonomous
		  vehicle control that require strong guarantees for both
		  reliability and performance, these approaches may no longer
		  be sufficient. In this paper we evaluate the latency,
		  throughput, and resource costs of providing different
		  reliability guarantees, with a focus on these emerging edge
		  cloud platforms and applications.},
  booktitle	= {2019 {IEEE} {International} {Symposium} on {Local} and
		  {Metropolitan} {Area} {Networks} ({LANMAN})},
  author	= {Kulkarni, Sameer G and Liu, Guyue and Ramakrishnan, K. K.
		  and Wood, Timothy},
  month		= jul,
  year		= {2019},
  note		= {ISSN: 1944-0367},
  keywords	= {cloud computing, Cloud computing, computer centres,
		  container-based services, Containers, Data centers, edge
		  cloud data centers, edge cloud platforms, edge cloud
		  services, Fasteners, fault tolerant computing, Reliability,
		  reliability guarantees, resource costs, Semantics,
		  serverless computing platforms, serverless platforms,
		  storage resources, Storms},
  pages		= {1--6},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/UNELACFE/8846970.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/23BA479D/Kulkarni et al.
		  - 2019 - Living on the Edge Serverless Computing and the
		  C.pdf:application/pdf}
}

@Article{	  lagar2011snowflock,
  title		= {Snowflock: Virtual machine cloning as a first-class cloud
		  primitive},
  author	= {Lagar-Cavilla, H Andr{\'e}s and Whitney, Joseph A and
		  Bryant, Roy and Patchin, Philip and Brudno, Michael and de
		  Lara, Eyal and Rumble, Stephen M and Satyanarayanan, M and
		  Scannell, Adin},
  journal	= {ACM Transactions on Computer Systems (TOCS)},
  volume	= {29},
  number	= {1},
  pages		= {1--45},
  year		= {2011},
  publisher	= {ACM New York, NY, USA}
}

###Article{	  lagar2011snowflock,
  title		= {Snowflock: Virtual machine cloning as a first-class cloud
		  primitive},
  author	= {Lagar-Cavilla, H Andr{\'e}s and Whitney, Joseph A and
		  Bryant, Roy and Patchin, Philip and Brudno, Michael and de
		  Lara, Eyal and Rumble, Stephen M and Satyanarayanan, M and
		  Scannell, Adin},
  journal	= {ACM Transactions on Computer Systems (TOCS)},
  volume	= {29},
  number	= {1},
  pages		= {1--45},
  year		= {2011},
  publisher	= {ACM New York, NY, USA}
}

@Misc{		  lambda-edge,
  title		= {AWS Lambda@Edge},
  howpublished	= {\url{https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html}}
}

@Misc{		  lambda-limits,
  title		= {{AWS Lambda Limits}},
  howpublished	= {\url{https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html}}
}

###Misc{	  lambda-limits,
  title		= {{AWS Lambda Limits}},
  howpublished	= {\url{https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html}}
}

@Misc{		  lambda-warm,
  title		= { Cold start / Warm start with AWS Lambda },
  year		= {2018},
  author	= {Erwan Alliaume and Benjamin Le Roux},
  howpublished	= {\url{https://blog.octo.com/en/cold-start-warm-start-with-aws-lambda/}}
}

###Misc{	  lambda-warm,
  title		= { Cold start / Warm start with AWS Lambda },
  year		= {2018},
  author	= {Erwan Alliaume and Benjamin Le Roux},
  howpublished	= {\url{https://blog.octo.com/en/cold-start-warm-start-with-aws-lambda/}}
}

@Misc{		  lambda-warm-hour,
  title		= {{How long does AWS Lambda keep your idle functions around
		  before a cold start?}},
  year		= {2017},
  howpublished	= {\url{https://read.acloud.guru/how-long-does-aws-lambda-keep-your-idle-functions-around-before-a-cold-start-bf715d3b810}}
}

###Misc{	  lambda-warm-hour,
  title		= {{How long does AWS Lambda keep your idle functions around
		  before a cold start?}},
  year		= {2017},
  howpublished	= {\url{https://read.acloud.guru/how-long-does-aws-lambda-keep-your-idle-functions-around-before-a-cold-start-bf715d3b810}}
}

@InProceedings{	  lebeane_watt_2015,
  title		= {Watt {Watcher}: {Fine}-{Grained} {Power} {Estimation} for
		  {Emerging} {Workloads}},
  shorttitle	= {Watt {Watcher}},
  doi		= {10.1109/SBAC-PAD.2015.26},
  abstract	= {Extensive research has focused on estimating power to
		  guide advances in power management schemes, thermal hot
		  spots, and voltage noise. However, simulated power models
		  are slow and struggle with deep software stacks, while
		  direct measurements are typically coarse-grained. This
		  paper introduces Watt Watcher, a multicore power
		  measurement framework that offers fine-grained functional
		  unit breakdowns. Watt Watcher operates by passing event
		  counts and a hardware descriptor file into configurable
		  back-end power models based on McPAT. Researchers and
		  vendors can add other processors to our tool by mapping to
		  the Watt Watcher interface. We show that Watt Watcher, when
		  calibrated, has a MAPE (mean absolute percentage error) of
		  2.67\% aggregated over all benchmarks when compared to
		  measured power consumption on SPEC CPU 2006 and
		  multithreaded PARSEC benchmarks across three different
		  machines of various form factors and manufacturing
		  processes. We present two use cases showing how Watt
		  Watcher can derive insights that are difficult to obtain
		  through other measurement infrastructures. Additionally, we
		  illustrate how Watt Watcher can be used to provide insights
		  into challenging big data and cloud workloads on a server
		  CPU. Through the use of Watt Watcher, it is possible to
		  obtain a detailed power breakdown on real hardware without
		  vendor proprietary models or hardware instrumentation.},
  booktitle	= {2015 27th {International} {Symposium} on {Computer}
		  {Architecture} and {High} {Performance} {Computing}
		  ({SBAC}-{PAD})},
  author	= {LeBeane, Michael and Ryoo, Jee Ho and Panda, Reena and
		  John, Lizy Kurian},
  month		= oct,
  year		= {2015},
  note		= {ISSN: 1550-6533},
  keywords	= {Monitoring, Hardware, Program processors,
		  Microarchitecture, Power measurement, Power demand,
		  Radiation detectors},
  pages		= {106--113},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/QV8685QV/7379840.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/83XT356S/LeBeane et al. -
		  2015 - Watt Watcher Fine-Grained Power Estimation for
		  Em.pdf:application/pdf}
}

###InProceedings{ lebeane_watt_2015,
  title		= {Watt {Watcher}: {Fine}-{Grained} {Power} {Estimation} for
		  {Emerging} {Workloads}},
  shorttitle	= {Watt {Watcher}},
  doi		= {10.1109/SBAC-PAD.2015.26},
  abstract	= {Extensive research has focused on estimating power to
		  guide advances in power management schemes, thermal hot
		  spots, and voltage noise. However, simulated power models
		  are slow and struggle with deep software stacks, while
		  direct measurements are typically coarse-grained. This
		  paper introduces Watt Watcher, a multicore power
		  measurement framework that offers fine-grained functional
		  unit breakdowns. Watt Watcher operates by passing event
		  counts and a hardware descriptor file into configurable
		  back-end power models based on McPAT. Researchers and
		  vendors can add other processors to our tool by mapping to
		  the Watt Watcher interface. We show that Watt Watcher, when
		  calibrated, has a MAPE (mean absolute percentage error) of
		  2.67\% aggregated over all benchmarks when compared to
		  measured power consumption on SPEC CPU 2006 and
		  multithreaded PARSEC benchmarks across three different
		  machines of various form factors and manufacturing
		  processes. We present two use cases showing how Watt
		  Watcher can derive insights that are difficult to obtain
		  through other measurement infrastructures. Additionally, we
		  illustrate how Watt Watcher can be used to provide insights
		  into challenging big data and cloud workloads on a server
		  CPU. Through the use of Watt Watcher, it is possible to
		  obtain a detailed power breakdown on real hardware without
		  vendor proprietary models or hardware instrumentation.},
  booktitle	= {2015 27th {International} {Symposium} on {Computer}
		  {Architecture} and {High} {Performance} {Computing}
		  ({SBAC}-{PAD})},
  author	= {LeBeane, Michael and Ryoo, Jee Ho and Panda, Reena and
		  John, Lizy Kurian},
  month		= oct,
  year		= {2015},
  note		= {ISSN: 1550-6533},
  keywords	= {Monitoring, Hardware, Program processors,
		  Microarchitecture, Power measurement, Power demand,
		  Radiation detectors},
  pages		= {106--113},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/QV8685QV/7379840.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/83XT356S/LeBeane et al. -
		  2015 - Watt Watcher Fine-Grained Power Estimation for
		  Em.pdf:application/pdf}
}

@Article{	  lee_cloudsocket_2018,
  title		= {{CloudSocket}: {Fine}-{Grained} {Power} {Sensing} {System}
		  for {Datacenters}},
  volume	= {6},
  issn		= {2169-3536},
  shorttitle	= {{CloudSocket}},
  doi		= {10.1109/ACCESS.2018.2868469},
  abstract	= {Today’s data centers have various computing and storage
		  devices for processing a myriad of data, and they generally
		  consume a considerable amount of electrical energy. This
		  paper proposes a smart grid-inspired methodology to observe
		  and profile the power consumption of a data center. Based
		  on this technique, our paper provides information that is
		  useful for moderating the peak power consumption of the
		  data centers. Our power measurement platform consists of
		  several devices named CloudSockets, and each CloudSocket
		  unit can measure the power consumption of multiple
		  computing nodes and periodically transmit measurement data
		  wirelessly to the coordinator unit. This data can be used
		  to analyze the relationship between the workload and the
		  power consumption of the data center. We tested our
		  methodology through the application of various algorithms
		  with a 32-node distributed system that runs Apache Spark
		  for large-scale data analytics. An analysis of our
		  experimental results reveals how and where the peak power
		  of each node in the grid overlaps, providing opportunities
		  for informed coordination of the computing components for
		  peak power reduction.},
  journal	= {IEEE Access},
  author	= {Lee, Seil and Kim, Hanjoo and Park, Seongsik and Kim,
		  Seijoon and Choe, Hyeokjun and Yoon, Sungroh},
  year		= {2018},
  note		= {Conference Name: IEEE Access},
  keywords	= {Monitoring, Servers, cloud computing, Machine learning,
		  Smart grids, Power measurement, Power demand, Datacenter,
		  Sensors, smart grid},
  pages		= {49601--49610},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/9M6ZEPLW/8454427.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/LAH5I5HZ/Lee et al. -
		  2018 - CloudSocket Fine-Grained Power Sensing System
		  for.pdf:application/pdf}
}

@InProceedings{	  lee_evaluation_2018,
  title		= {Evaluation of {Production} {Serverless} {Computing}
		  {Environments}},
  doi		= {10.1109/CLOUD.2018.00062},
  abstract	= {Serverless computing provides a small runtime container to
		  execute lines of codes without infrastructure management
		  which is similar to Platform as a Service (PaaS) but a
		  functional level. Amazon started the event-driven compute
		  named Lambda functions in 2014 with a 25 concurrent
		  limitation, but it now supports at least a thousand of
		  concurrent invocation to process event messages generated
		  by resources like databases, storage and system logs. Other
		  providers, i.e., Google, Microsoft, and IBM offer a dynamic
		  scaling manager to handle parallel requests of stateless
		  functions in which additional containers are provisioning
		  on new compute nodes for distribution. However, while
		  functions are often developed for microservices and
		  lightweight workload, they are associated with distributed
		  data processing using the concurrent invocations. We claim
		  that the current serverless computing environments can
		  support dynamic applications in parallel when a partitioned
		  task is executable on a small function instance. We present
		  results of throughput, network bandwidth, a file I/O and
		  compute performance regarding the concurrent invocations.
		  We deployed a series of functions for distributed data
		  processing to address the elasticity and then demonstrated
		  the differences between serverless computing and virtual
		  machines for cost efficiency and resource utilization.},
  booktitle	= {2018 {IEEE} 11th {International} {Conference} on {Cloud}
		  {Computing} ({CLOUD})},
  author	= {Lee, Hyungro and Satyam, Kumar and Fox, Geoffrey},
  month		= jul,
  year		= {2018},
  note		= {ISSN: 2159-6190},
  keywords	= {cloud computing, Cloud computing, concurrent invocation,
		  Containers, Data processing, Databases, distributed data
		  processing, dynamic scaling manager, event-driven compute,
		  FaaS, Serverless, Event-driven Computing, Amazon Lambda,
		  Google Functions, Microsoft Azure Functions, IBM OpenWhisk,
		  Google, infrastructure management, Lambda functions,
		  parallel requests, production serverless computing
		  environments, Runtime, stateless functions, Throughput,
		  virtual machines},
  pages		= {442--450},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/Q9VUFSQY/8457830.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/QABN6LRG/Lee et al. -
		  2018 - Evaluation of Production Serverless Computing
		  Envi.pdf:application/pdf}
}

###InProceedings{ lee_evaluation_2018,
  title		= {Evaluation of {Production} {Serverless} {Computing}
		  {Environments}},
  doi		= {10.1109/CLOUD.2018.00062},
  abstract	= {Serverless computing provides a small runtime container to
		  execute lines of codes without infrastructure management
		  which is similar to Platform as a Service (PaaS) but a
		  functional level. Amazon started the event-driven compute
		  named Lambda functions in 2014 with a 25 concurrent
		  limitation, but it now supports at least a thousand of
		  concurrent invocation to process event messages generated
		  by resources like databases, storage and system logs. Other
		  providers, i.e., Google, Microsoft, and IBM offer a dynamic
		  scaling manager to handle parallel requests of stateless
		  functions in which additional containers are provisioning
		  on new compute nodes for distribution. However, while
		  functions are often developed for microservices and
		  lightweight workload, they are associated with distributed
		  data processing using the concurrent invocations. We claim
		  that the current serverless computing environments can
		  support dynamic applications in parallel when a partitioned
		  task is executable on a small function instance. We present
		  results of throughput, network bandwidth, a file I/O and
		  compute performance regarding the concurrent invocations.
		  We deployed a series of functions for distributed data
		  processing to address the elasticity and then demonstrated
		  the differences between serverless computing and virtual
		  machines for cost efficiency and resource utilization.},
  booktitle	= {2018 {IEEE} 11th {International} {Conference} on {Cloud}
		  {Computing} ({CLOUD})},
  author	= {Lee, Hyungro and Satyam, Kumar and Fox, Geoffrey},
  month		= jul,
  year		= {2018},
  note		= {ISSN: 2159-6190},
  keywords	= {cloud computing, Cloud computing, concurrent invocation,
		  Containers, Data processing, Databases, distributed data
		  processing, dynamic scaling manager, event-driven compute,
		  FaaS, Serverless, Event-driven Computing, Amazon Lambda,
		  Google Functions, Microsoft Azure Functions, IBM OpenWhisk,
		  Google, infrastructure management, Lambda functions,
		  parallel requests, production serverless computing
		  environments, Runtime, stateless functions, Throughput,
		  virtual machines},
  pages		= {442--450},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/Q9VUFSQY/8457830.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/QABN6LRG/Lee et al. -
		  2018 - Evaluation of Production Serverless Computing
		  Envi.pdf:application/pdf}
}

###InProceedings{ lee_evaluation_2018,
  title		= {Evaluation of {Production} {Serverless} {Computing}
		  {Environments}},
  doi		= {10.1109/CLOUD.2018.00062},
  abstract	= {Serverless computing provides a small runtime container to
		  execute lines of codes without infrastructure management
		  which is similar to Platform as a Service (PaaS) but a
		  functional level. Amazon started the event-driven compute
		  named Lambda functions in 2014 with a 25 concurrent
		  limitation, but it now supports at least a thousand of
		  concurrent invocation to process event messages generated
		  by resources like databases, storage and system logs. Other
		  providers, i.e., Google, Microsoft, and IBM offer a dynamic
		  scaling manager to handle parallel requests of stateless
		  functions in which additional containers are provisioning
		  on new compute nodes for distribution. However, while
		  functions are often developed for microservices and
		  lightweight workload, they are associated with distributed
		  data processing using the concurrent invocations. We claim
		  that the current serverless computing environments can
		  support dynamic applications in parallel when a partitioned
		  task is executable on a small function instance. We present
		  results of throughput, network bandwidth, a file I/O and
		  compute performance regarding the concurrent invocations.
		  We deployed a series of functions for distributed data
		  processing to address the elasticity and then demonstrated
		  the differences between serverless computing and virtual
		  machines for cost efficiency and resource utilization.},
  booktitle	= {2018 {IEEE} 11th {International} {Conference} on {Cloud}
		  {Computing} ({CLOUD})},
  author	= {Lee, Hyungro and Satyam, Kumar and Fox, Geoffrey},
  month		= jul,
  year		= {2018},
  note		= {ISSN: 2159-6190},
  keywords	= {Cloud computing, cloud computing, virtual machines,
		  Google, Runtime, Throughput, Containers, concurrent
		  invocation, Data processing, Databases, distributed data
		  processing, dynamic scaling manager, event-driven compute,
		  FaaS, Serverless, Event-driven Computing, Amazon Lambda,
		  Google Functions, Microsoft Azure Functions, IBM OpenWhisk,
		  infrastructure management, Lambda functions, parallel
		  requests, production serverless computing environments,
		  stateless functions},
  pages		= {442--450},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/Q9VUFSQY/8457830.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/QABN6LRG/Lee et al. -
		  2018 - Evaluation of Production Serverless Computing
		  Envi.pdf:application/pdf}
}

@InProceedings{	  leverich_reconciling_2014,
  address	= {Amsterdam, The Netherlands},
  title		= {Reconciling high server utilization and sub-millisecond
		  quality-of-service},
  isbn		= {978-1-4503-2704-6},
  url		= {http://dl.acm.org/citation.cfm?doid=2592798.2592821},
  doi		= {10.1145/2592798.2592821},
  abstract	= {The simplest strategy to guarantee good quality of service
		  (QoS) for a latency-sensitive workload with sub-millisecond
		  latency in a shared cluster environment is to never run
		  other workloads concurrently with it on the same server.
		  Unfortunately, this inevitably leads to low server
		  utilization, reducing both the capability and cost
		  effectiveness of the cluster.},
  language	= {en},
  urldate	= {2022-07-22},
  booktitle	= {Proceedings of the {Ninth} {European} {Conference} on
		  {Computer} {Systems} - {EuroSys} '14},
  publisher	= {ACM Press},
  author	= {Leverich, Jacob and Kozyrakis, Christos},
  year		= {2014},
  pages		= {1--14},
  file		= {Leverich and Kozyrakis - 2014 - Reconciling high server
		  utilization and
		  sub-millis.pdf:/home/prateeks/Zotero/storage/KGWW9M2I/Leverich
		  and Kozyrakis - 2014 - Reconciling high server utilization
		  and sub-millis.pdf:application/pdf}
}

@InProceedings{	  lewis_run-time_2008,
  address	= {USA},
  series	= {{HotPower}'08},
  title		= {Run-time energy consumption estimation based on workload
		  in server systems},
  abstract	= {This paper proposes to develop a system-wide energy
		  consumption model for servers by making use of hardware
		  performance counters and experimental measurements. We
		  develop a real-time energy prediction model that relates
		  server energy consumption to its overall thermal envelope.
		  While previous studies have attempted system-wide modeling
		  of server power consumption through subsystem models, our
		  approach is different in that it uses a small set of
		  tightly correlated parameters to create a model relating
		  system energy input to subsystem energy consumption. We
		  develop a linear regression model that relates processor
		  power, bus activity, and system ambient temperatures into
		  real-time predictions of the power consumption of long jobs
		  and as result controlling their thermal impact. Using the
		  HyperTransport bus model as a case study and through
		  electrical measurements on example server subsystems, we
		  develop a statistical model for estimating run-time power
		  consumption. Our model is accurate within an error of four
		  percent(4\%) as verified using a set of common processor
		  benchmarks.},
  urldate	= {2022-10-11},
  booktitle	= {Proceedings of the 2008 conference on {Power} aware
		  computing and systems},
  publisher	= {USENIX Association},
  author	= {Lewis, Adam and Ghosh, Soumik and Tzeng, N.-F.},
  month		= dec,
  year		= {2008},
  pages		= {4}
}

@Article{	  lin_computation_2019,
  title		= {Computation {Offloading} {Toward} {Edge} {Computing}},
  volume	= {107},
  issn		= {1558-2256},
  doi		= {10.1109/JPROC.2019.2922285},
  abstract	= {We are living in a world where massive end devices perform
		  computing everywhere and everyday. However, these devices
		  are constrained by the battery and computational resources.
		  With the increasing number of intelligent applications
		  (e.g., augmented reality and face recognition) that require
		  much more computational power, they shift to perform
		  computation offloading to the cloud, known as mobile cloud
		  computing (MCC). Unfortunately, the cloud is usually far
		  away from end devices, leading to a high latency as well as
		  the bad quality of experience (QoE) for latency-sensitive
		  applications. In this context, the emergence of edge
		  computing is no coincidence. Edge computing extends the
		  cloud to the edge of the network, close to end users,
		  bringing ultra-low latency and high bandwidth.
		  Consequently, there is a trend of computation offloading
		  toward edge computing. In this paper, we provide a
		  comprehensive perspective on this trend. First, we give an
		  insight into the architecture refactoring in edge
		  computing. Based on that insight, this paper reviews the
		  state-of-the-art research on computation offloading in
		  terms of application partitioning, task allocation,
		  resource management, and distributed execution, with
		  highlighting features for edge computing. Then, we
		  illustrate some disruptive application scenarios that we
		  envision as critical drivers for the flourish of edge
		  computing, such as real-time video analytics, smart
		  “things” (e.g., smart city and smart home), vehicle
		  applications, and cloud gaming. Finally, we discuss the
		  opportunities and future research directions.},
  number	= {8},
  journal	= {Proceedings of the IEEE},
  author	= {Lin, Li and Liao, Xiaofei and Jin, Hai and Li, Peng},
  month		= aug,
  year		= {2019},
  keywords	= {architecture refactoring, Backscatter, Cloud computing,
		  computation offloading, Computation offloading, distributed
		  execution, distributed processing, edge computing, Edge
		  computing, Energy harvesting, Internet of Things, Internet
		  of Things (IoT), mobile cloud computing (MCC), mobile edge
		  computing (MEC), quality of experience, Radio frequency,
		  resource allocation, resource management, Resource
		  management, software architecture, software maintenance,
		  task allocation, Throughput, Wireless communication},
  pages		= {1584--1607},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/JCBZQBQ9/8758310.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/WESJZBRL/Lin et al. -
		  2019 - Computation Offloading Toward Edge
		  Computing.pdf:application/pdf}
}

###Article{	  lin_computation_2019,
  title		= {Computation {Offloading} {Toward} {Edge} {Computing}},
  volume	= {107},
  issn		= {1558-2256},
  doi		= {10.1109/JPROC.2019.2922285},
  abstract	= {We are living in a world where massive end devices perform
		  computing everywhere and everyday. However, these devices
		  are constrained by the battery and computational resources.
		  With the increasing number of intelligent applications
		  (e.g., augmented reality and face recognition) that require
		  much more computational power, they shift to perform
		  computation offloading to the cloud, known as mobile cloud
		  computing (MCC). Unfortunately, the cloud is usually far
		  away from end devices, leading to a high latency as well as
		  the bad quality of experience (QoE) for latency-sensitive
		  applications. In this context, the emergence of edge
		  computing is no coincidence. Edge computing extends the
		  cloud to the edge of the network, close to end users,
		  bringing ultra-low latency and high bandwidth.
		  Consequently, there is a trend of computation offloading
		  toward edge computing. In this paper, we provide a
		  comprehensive perspective on this trend. First, we give an
		  insight into the architecture refactoring in edge
		  computing. Based on that insight, this paper reviews the
		  state-of-the-art research on computation offloading in
		  terms of application partitioning, task allocation,
		  resource management, and distributed execution, with
		  highlighting features for edge computing. Then, we
		  illustrate some disruptive application scenarios that we
		  envision as critical drivers for the flourish of edge
		  computing, such as real-time video analytics, smart
		  “things” (e.g., smart city and smart home), vehicle
		  applications, and cloud gaming. Finally, we discuss the
		  opportunities and future research directions.},
  number	= {8},
  journal	= {Proceedings of the IEEE},
  author	= {Lin, Li and Liao, Xiaofei and Jin, Hai and Li, Peng},
  month		= aug,
  year		= {2019},
  keywords	= {architecture refactoring, Backscatter, Cloud computing,
		  computation offloading, Computation offloading, distributed
		  execution, distributed processing, edge computing, Edge
		  computing, Energy harvesting, Internet of Things, Internet
		  of Things (IoT), mobile cloud computing (MCC), mobile edge
		  computing (MEC), quality of experience, Radio frequency,
		  resource allocation, resource management, Resource
		  management, software architecture, software maintenance,
		  task allocation, Throughput, Wireless communication},
  pages		= {1584--1607},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/JCBZQBQ9/8758310.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/WESJZBRL/Lin et al. -
		  2019 - Computation Offloading Toward Edge
		  Computing.pdf:application/pdf}
}

@Article{	  lin_mitigating_2019,
  title		= {Mitigating {Cold} {Starts} in {Serverless} {Platforms}:
		  {A} {Pool}-{Based} {Approach}},
  shorttitle	= {Mitigating {Cold} {Starts} in {Serverless} {Platforms}},
  url		= {http://arxiv.org/abs/1903.12221},
  abstract	= {Rapid adoption of the ’serverless’ (or
		  Function-as-a-Service, FaaS) paradigm [8], pioneered by
		  Amazon with AWS Lambda and followed by numerous commercial
		  offerings and open source projects, introduces new
		  challenges in designing the cloud infrastructure, balancing
		  between performance and cost. While instant per-request
		  elasticity that FaaS platforms typically offer application
		  developers makes it possible to achieve high performance of
		  bursty workloads without over-provisioning, such elasticity
		  often involves extra latency associated with on-demand
		  provisioning of individual runtime containers that serve
		  the functions. This phenomenon is often called ’cold
		  starts’ [12], as opposed to the situation when a function
		  is served by a pre-provisioned ’warm’ container, ready
		  to serve requests with close to zero overhead. Providers
		  are constantly working on techniques aimed at reducing cold
		  starts. A common approach to reduce cold starts is to
		  maintain a pool of ’warm’ containers, in anticipation
		  of future requests. In this project, we address the cold
		  start problem in serverless architectures, specifically
		  under the Knative Serving FaaS platform. We implemented a
		  pool of function instances and evaluated the latency
		  compared with the original implementation, which resulted
		  in an 85\% reduction of P99 response time for a single
		  instance pool.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1903.12221 [cs]},
  author	= {Lin, Ping-Min and Glikson, Alex},
  month		= mar,
  year		= {2019},
  note		= {arXiv: 1903.12221},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  file		= {Lin and Glikson - 2019 - Mitigating Cold Starts in
		  Serverless Platforms A
		  .pdf:/home/prateeks/Zotero/storage/2T2GVP5J/Lin and Glikson
		  - 2019 - Mitigating Cold Starts in Serverless Platforms A
		  .pdf:application/pdf}
}

###Article{	  lin_mitigating_2019,
  title		= {Mitigating {Cold} {Starts} in {Serverless} {Platforms}:
		  {A} {Pool}-{Based} {Approach}},
  shorttitle	= {Mitigating {Cold} {Starts} in {Serverless} {Platforms}},
  url		= {http://arxiv.org/abs/1903.12221},
  abstract	= {Rapid adoption of the ’serverless’ (or
		  Function-as-a-Service, FaaS) paradigm [8], pioneered by
		  Amazon with AWS Lambda and followed by numerous commercial
		  offerings and open source projects, introduces new
		  challenges in designing the cloud infrastructure, balancing
		  between performance and cost. While instant per-request
		  elasticity that FaaS platforms typically offer application
		  developers makes it possible to achieve high performance of
		  bursty workloads without over-provisioning, such elasticity
		  often involves extra latency associated with on-demand
		  provisioning of individual runtime containers that serve
		  the functions. This phenomenon is often called ’cold
		  starts’ [12], as opposed to the situation when a function
		  is served by a pre-provisioned ’warm’ container, ready
		  to serve requests with close to zero overhead. Providers
		  are constantly working on techniques aimed at reducing cold
		  starts. A common approach to reduce cold starts is to
		  maintain a pool of ’warm’ containers, in anticipation
		  of future requests. In this project, we address the cold
		  start problem in serverless architectures, specifically
		  under the Knative Serving FaaS platform. We implemented a
		  pool of function instances and evaluated the latency
		  compared with the original implementation, which resulted
		  in an 85\% reduction of P99 response time for a single
		  instance pool.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1903.12221 [cs]},
  author	= {Lin, Ping-Min and Glikson, Alex},
  month		= mar,
  year		= {2019},
  note		= {arXiv: 1903.12221},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  file		= {Lin and Glikson - 2019 - Mitigating Cold Starts in
		  Serverless Platforms A
		  .pdf:/home/prateeks/Zotero/storage/2T2GVP5J/Lin and Glikson
		  - 2019 - Mitigating Cold Starts in Serverless Platforms A
		  .pdf:application/pdf}
}

###Article{	  lin_mitigating_2019,
  title		= {Mitigating {Cold} {Starts} in {Serverless} {Platforms}:
		  {A} {Pool}-{Based} {Approach}},
  shorttitle	= {Mitigating {Cold} {Starts} in {Serverless} {Platforms}},
  url		= {http://arxiv.org/abs/1903.12221},
  abstract	= {Rapid adoption of the ’serverless’ (or
		  Function-as-a-Service, FaaS) paradigm [8], pioneered by
		  Amazon with AWS Lambda and followed by numerous commercial
		  offerings and open source projects, introduces new
		  challenges in designing the cloud infrastructure, balancing
		  between performance and cost. While instant per-request
		  elasticity that FaaS platforms typically offer application
		  developers makes it possible to achieve high performance of
		  bursty workloads without over-provisioning, such elasticity
		  often involves extra latency associated with on-demand
		  provisioning of individual runtime containers that serve
		  the functions. This phenomenon is often called ’cold
		  starts’ [12], as opposed to the situation when a function
		  is served by a pre-provisioned ’warm’ container, ready
		  to serve requests with close to zero overhead. Providers
		  are constantly working on techniques aimed at reducing cold
		  starts. A common approach to reduce cold starts is to
		  maintain a pool of ’warm’ containers, in anticipation
		  of future requests. In this project, we address the cold
		  start problem in serverless architectures, specifically
		  under the Knative Serving FaaS platform. We implemented a
		  pool of function instances and evaluated the latency
		  compared with the original implementation, which resulted
		  in an 85\% reduction of P99 response time for a single
		  instance pool.},
  language	= {en},
  urldate	= {2020-01-10},
  journal	= {arXiv:1903.12221 [cs]},
  author	= {Lin, Ping-Min and Glikson, Alex},
  month		= mar,
  year		= {2019},
  note		= {arXiv: 1903.12221},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  file		= {Lin and Glikson - 2019 - Mitigating Cold Starts in
		  Serverless Platforms A
		  .pdf:/home/prateeks/Zotero/storage/2T2GVP5J/Lin and Glikson
		  - 2019 - Mitigating Cold Starts in Serverless Platforms A
		  .pdf:application/pdf}
}

@InProceedings{	  liu2020imitation,
  title		= {An imitation learning approach for cache replacement},
  author	= {Liu, Evan and Hashemi, Milad and Swersky, Kevin and
		  Ranganathan, Parthasarathy and Ahn, Junwhan},
  booktitle	= {International Conference on Machine Learning},
  pages		= {6237--6247},
  year		= {2020},
  organization	= {PMLR}
}

@InProceedings{	  liu_fastcap_2016,
  title		= {{FastCap}: {An} efficient and fair algorithm for power
		  capping in many-core systems},
  shorttitle	= {{FastCap}},
  doi		= {10.1109/ISPASS.2016.7482074},
  abstract	= {Future servers will incorporate many active low-power
		  modes for different system components, such as cores and
		  memory. Though these modes provide flexibility for power
		  management via Dynamic Voltage and Frequency Scaling
		  (DVFS), they must be operated in a coordinated manner. Such
		  coordinated control creates a combinatorial space of
		  possible power mode configurations. Given the rapid growth
		  of the number of cores, it is becoming increasingly
		  challenging to quickly select the configuration that
		  maximizes the performance under a given power budget. Prior
		  power capping techniques do not scale well to large numbers
		  of cores, and none of those works has considered memory
		  DVFS. In this paper, we present FastCap, our optimization
		  approach for system-wide power capping, using both CPU and
		  memory DVFS. Based on a queuing model, FastCap formulates
		  power capping as a non-linear optimization problem where we
		  seek to maximize the system performance under a power
		  budget, while promoting fairness across applications. Our
		  FastCap algorithm solves the optimization online and
		  efficiently (low complexity on the number of cores), using
		  a small set of performance counters as input. To evaluate
		  FastCap, we simulate it for a many-core server running
		  different types of workloads. Our results show that FastCap
		  caps power draw accurately, while producing better
		  application performance and fairness than many existing CPU
		  power capping methods (even after they are extended to use
		  of memory DVFS as well).},
  booktitle	= {2016 {IEEE} {International} {Symposium} on {Performance}
		  {Analysis} of {Systems} and {Software} ({ISPASS})},
  author	= {Liu, Yanpei and Cox, Guilherme and Deng, Qingyuan and
		  Draper, Stark C. and Bianchini, Ricardo},
  month		= apr,
  year		= {2016},
  keywords	= {Optimization, Servers, Resource management, Complexity
		  theory, Memory management, Power demand},
  pages		= {57--68},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/RTW39G62/7482074.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/YQXG79N7/Liu et al. -
		  2016 - FastCap An efficient and fair algorithm for
		  power.pdf:application/pdf}
}

@Article{	  liu_performance_2021,
  title		= {Performance {Characteristics} of the {BlueField}-2
		  {SmartNIC}},
  url		= {http://arxiv.org/abs/2105.06619},
  abstract	= {High-performance computing (HPC) researchers have long
		  envisioned scenarios where application workﬂows could be
		  improved through the use of programmable processing
		  elements embedded in the network fabric. Recently, vendors
		  have introduced programmable Smart Network Interface Cards
		  (SmartNICs) that enable computations to be ofﬂoaded to
		  the edge of the network. There is great interest in both
		  the HPC and high-performance data analytics (HPDA)
		  communities in understanding the roles these devices may
		  play in the data paths of upcoming systems.},
  language	= {en},
  urldate	= {2021-07-21},
  journal	= {arXiv:2105.06619 [cs]},
  author	= {Liu, Jianshen and Maltzahn, Carlos and Ulmer, Craig and
		  Curry, Matthew Leon},
  month		= may,
  year		= {2021},
  note		= {arXiv: 2105.06619},
  keywords	= {Computer Science - Performance, Computer Science -
		  Networking and Internet Architecture, B.8.2, C.2.0},
  annote	= {Comment: 13 pages, 8 figures, 4 tables},
  file		= {Liu et al. - 2021 - Performance Characteristics of the
		  BlueField-2
		  Sma.pdf:/home/prateeks/Zotero/storage/N2ZRB6B5/Liu et al. -
		  2021 - Performance Characteristics of the BlueField-2
		  Sma.pdf:application/pdf}
}

@Misc{		  locust,
  title		= {Locust: A modern load testing framework},
  author	= {Locust},
  howpublished	= {\url{https://locust.io/}}
}

@Misc{		  lookbusy,
  title		= {Lookbusy -- a synthetic load generator},
  author	= {Devin Carraway},
  howpublished	= {\url{http://www.devin.com/lookbusy/}}
}

@InProceedings{	  lynx_asplos20,
  author	= {Tork, Maroun and Maudlej, Lina and Silberstein, Mark},
  title		= {Lynx: A SmartNIC-Driven Accelerator-Centric Architecture
		  for Network Servers},
  year		= {2020},
  isbn		= {9781450371025},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3373376.3378528},
  doi		= {10.1145/3373376.3378528},
  abstract	= {This paper explores new opportunities afforded by the
		  growing deployment of compute and I/O accelerators to
		  improve the performance and efficiency of
		  hardware-accelerated computing services in data centers.We
		  propose Lynx, an accelerator-centric network server
		  architecture that offloads the server data and control
		  planes to the SmartNIC, and enables direct networking from
		  accelerators via a lightweight hardware-friendly I/O
		  mechanism. Lynx enables the design of hardware-accelerated
		  network servers that run without CPU involvement, freeing
		  CPU cores and improving performance isolation for
		  accelerated services. It is portable across accelerator
		  architectures and allows the management of both local and
		  remote accelerators, seamlessly scaling beyond a single
		  physical machine.We implement and evaluate Lynx on GPUs and
		  the Intel Visual Compute Accelerator, as well as two
		  SmartNIC architectures - one with an FPGA, and another with
		  an 8-core ARM processor. Compared to a traditional
		  host-centric approach, Lynx achieves over 4X higher
		  throughput for a GPU-centric face verification server,
		  where it is used for GPU communications with an external
		  database, and 25% higher throughput for a GPU-accelerated
		  neural network inference service. For this workload, we
		  show that a single SmartNIC may drive 4 local and 8 remote
		  GPUs while achieving linear performance scaling without
		  using the host CPU.},
  booktitle	= {Proceedings of the Twenty-Fifth International Conference
		  on Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {117–-131},
  numpages	= {15},
  keywords	= {server architecture, hardware accelerators, smartnics, i/o
		  services for accelerators, operating systems},
  location	= {Lausanne, Switzerland},
  series	= {ASPLOS '20}
}

@Article{	  mahgoub_wisefuse_2022,
  title		= {{WISEFUSE}: {Workload} {Characterization} and {DAG}
		  {Transformation} for {Serverless} {Workflows}},
  volume	= {6},
  issn		= {2476-1249},
  shorttitle	= {{WISEFUSE}},
  url		= {https://dl.acm.org/doi/10.1145/3530892},
  doi		= {10.1145/3530892},
  abstract	= {We characterize production workloads of serverless DAGs at
		  a major cloud provider. Our analysis highlights two major
		  factors that limit performance: (a) lack of efficient
		  communication methods between the serverless functions in
		  the DAG, and (b) stragglers when a DAG stage invokes a set
		  of parallel functions that must complete before starting
		  the next DAG stage. To address these limitations, we
		  propose WISEFUSE, an automated approach to generate an
		  optimized execution plan for serverless DAGs for a
		  user-specified latency objective or budget. We introduce
		  three optimizations: (1) Fusion combines in-series
		  functions together in a single VM to reduce the
		  communication overhead between cascaded functions. (2)
		  Bundling executes a group of parallel invocations of a
		  function in one VM to improve resource sharing among the
		  parallel workers to reduce skew. (3) Resource Allocation
		  assigns the right VM size to each function or function
		  bundle in the DAG to reduce the E2E latency and cost. We
		  implement WISEFUSE to evaluate it experimentally using
		  three popular serverless applications with different DAG
		  structures, memory footprints, and intermediate data sizes.
		  Compared to competing approaches and other alternatives,
		  WISEFUSE shows significant improvements in E2E latency and
		  cost. Specifically, for a machine learning pipeline,
		  WISEFUSE achieves P95 latency that is 67\% lower than
		  Photons, 39\% lower than Faastlane, and 90\% lower than
		  SONIC without increasing the cost.},
  language	= {en},
  number	= {2},
  urldate	= {2022-07-29},
  journal	= {Proceedings of the ACM on Measurement and Analysis of
		  Computing Systems},
  author	= {Mahgoub, Ashraf and Yi, Edgardo Barsallo and Shankar,
		  Karthick and Minocha, Eshaan and Elnikety, Sameh and
		  Bagchi, Saurabh and Chaterji, Somali},
  month		= may,
  year		= {2022},
  pages		= {1--28},
  file		= {Mahgoub et al. - 2022 - WISEFUSE Workload Characterization
		  and DAG
		  Transf.pdf:/home/prateeks/Zotero/storage/YA4HWBFN/Mahgoub
		  et al. - 2022 - WISEFUSE Workload Characterization and DAG
		  Transf.pdf:application/pdf}
}

@InProceedings{	  maissen2020faasdom,
  title		= {FaaSdom: A benchmark suite for serverless computing},
  author	= {Maissen, Pascal and Felber, Pascal and Kropf, Peter and
		  Schiavoni, Valerio},
  booktitle	= {Proceedings of the 14th ACM International Conference on
		  Distributed and Event-based Systems},
  pages		= {73--84},
  year		= {2020}
}

@Article{	  makonin2015nonintrusive,
  title		= {Nonintrusive load monitoring (NILM) performance
		  evaluation},
  author	= {Makonin, Stephen and Popowich, Fred},
  journal	= {Energy Efficiency},
  volume	= {8},
  number	= {4},
  pages		= {809--814},
  year		= {2015},
  publisher	= {Springer}
}

@Article{	  mampage2021holistic,
  title		= {A Holistic View on Resource Management in Serverless
		  Computing Environments: Taxonomy, and Future Directions},
  author	= {Mampage, Anupama and Karunasekera, Shanika and Buyya,
		  Rajkumar},
  journal	= {arXiv preprint arXiv:2105.11592},
  year		= {2021}
}

###Article{	  mampage2021holistic,
  title		= {A Holistic View on Resource Management in Serverless
		  Computing Environments: Taxonomy, and Future Directions},
  author	= {Mampage, Anupama and Karunasekera, Shanika and Buyya,
		  Rajkumar},
  journal	= {arXiv preprint arXiv:2105.11592},
  year		= {2021}
}

@InProceedings{	  manner_cold_2018,
  address	= {Zurich},
  title		= {Cold {Start} {Influencing} {Factors} in {Function} as a
		  {Service}},
  isbn		= {978-1-72810-359-4},
  url		= {https://ieeexplore.ieee.org/document/8605777/},
  doi		= {10.1109/UCC-Companion.2018.00054},
  language	= {en},
  urldate	= {2020-08-15},
  booktitle	= {2018 {IEEE}/{ACM} {International} {Conference} on
		  {Utility} and {Cloud} {Computing} {Companion} ({UCC}
		  {Companion})},
  publisher	= {IEEE},
  author	= {Manner, Johannes and EndreB, Martin and Heckel, Tobias and
		  Wirtz, Guido},
  month		= dec,
  year		= {2018},
  pages		= {181--188},
  file		= {Manner et al. - 2018 - Cold Start Influencing Factors in
		  Function as a
		  Se.pdf:/home/prateeks/Zotero/storage/MEHUS7HU/Manner et al.
		  - 2018 - Cold Start Influencing Factors in Function as a
		  Se.pdf:application/pdf}
}

@Article{	  masanet2020recalibrating,
  title		= {Recalibrating global data center energy-use estimates},
  author	= {Masanet, Eric and Shehabi, Arman and Lei, Nuoa and Smith,
		  Sarah and Koomey, Jonathan},
  journal	= {Science},
  volume	= {367},
  number	= {6481},
  pages		= {984--986},
  year		= {2020},
  publisher	= {American Association for the Advancement of Science}
}

@Article{	  mccullough_evaluating_2011,
  title		= {Evaluating the {Effectiveness} of {Model}-{Based} {Power}
		  {Characterization}},
  abstract	= {Accurate power characterization is important in computing
		  platforms for several reasons ranging from poweraware
		  adaptation to power provisioning. Power characterization is
		  typically obtained through either direct measurements
		  enabled by physical instrumentation or modeling based on
		  hardware performance counters. We show, however, that
		  linear-regression based modeling techniques commonly used
		  in the literature work well only in restricted settings.
		  These techniques frequently exhibit high prediction error
		  in modern computing platforms due to inherent complexities
		  such as multiple cores, hidden device states, and large
		  dynamic power components.},
  language	= {en},
  journal	= {USENIX ATC},
  author	= {McCullough, John C and Agarwal, Yuvraj and Chandrashekar,
		  Jaideep and Kuppuswamy, Sathyanarayan and Snoeren, Alex C
		  and Gupta, Rajesh K},
  year		= {2011},
  pages		= {14},
  file		= {McCullough et al. - Evaluating the Effectiveness of
		  Model-Based Power
		  .pdf:/home/prateeks/Zotero/storage/73CE9ZJE/McCullough et
		  al. - Evaluating the Effectiveness of Model-Based Power
		  .pdf:application/pdf}
}

@InProceedings{	  medes2022,
  title		= {Memory deduplication for serverless computing with Medes},
  author	= {Saxena, Divyanshu and Ji, Tao and Singhvi, Arjun and
		  Khalid, Junaid and Akella, Aditya},
  booktitle	= {Proceedings of the Seventeenth European Conference on
		  Computer Systems},
  pages		= {714--729},
  year		= {2022}
}

@InProceedings{	  megiddo2003arc,
  title		= {ARC: A Self-Tuning, Low Overhead Replacement Cache.},
  author	= {Megiddo, Nimrod and Modha, Dharmendra S},
  booktitle	= {{USENIX FAST}},
  volume	= {3},
  pages		= {115--130},
  year		= {2003}
}

@Article{	  meinhold1983understanding,
  title		= {{Understanding the Kalman filter}},
  author	= {Meinhold, Richard J and Singpurwalla, Nozer D},
  journal	= {The American Statistician},
  volume	= {37},
  number	= {2},
  pages		= {123--127},
  year		= {1983},
  publisher	= {Taylor \& Francis}
}

@InProceedings{	  mirrokni2018consistent,
  title		= {Consistent hashing with bounded loads},
  author	= {Mirrokni, Vahab and Thorup, Mikkel and Zadimoghaddam,
		  Morteza},
  booktitle	= {Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium
		  on Discrete Algorithms},
  pages		= {587--604},
  year		= {2018},
  organization	= {SIAM}
}

@InProceedings{	  mishra_caloree_2018,
  address	= {Williamsburg VA USA},
  title		= {{CALOREE}: {Learning} {Control} for {Predictable}
		  {Latency} and {Low} {Energy}},
  isbn		= {978-1-4503-4911-6},
  shorttitle	= {{CALOREE}},
  url		= {https://dl.acm.org/doi/10.1145/3173162.3173184},
  doi		= {10.1145/3173162.3173184},
  abstract	= {Many modern computing systems must provide reliable
		  latency with minimal energy. Two central challenges arise
		  when allocating system resources to meet these conlicting
		  goals: (1) complexityÐmodern hardware exposes diverse
		  resources with complicated interactionsÐand (2) dynamicsÐ
		  latency must be maintained despite unpredictable changes in
		  operating environment or input. Machine learning accurately
		  models the latency of complex, interacting resources, but
		  does not address system dynamics; control theory adjusts to
		  dynamic changes, but struggles with complex resource
		  interaction. We therefore propose CALOREE, a resource
		  manager that learns key control parameters to meet latency
		  requirements with minimal energy in complex, dynamic
		  environments. CALOREE breaks resource allocation into two
		  sub-tasks: learning how interacting resources afect
		  speedup, and controlling speedup to meet latency
		  requirements with minimal energy. CALOREE deines a general
		  control systemÐ whose parameters are customized by a
		  learning frameworkÐ while maintaining control-theoretic
		  formal guarantees that the latency goal will be met. We
		  test CALOREE’s ability to deliver reliable latency on
		  heterogeneous ARM big.LITTLE architectures in both single
		  and multi-application scenarios. Compared to the best prior
		  learning and control solutions, CALOREE reduces deadline
		  misses by 60\% and energy consumption by 13\%.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the {Twenty}-{Third} {International}
		  {Conference} on {Architectural} {Support} for {Programming}
		  {Languages} and {Operating} {Systems}},
  publisher	= {ACM},
  author	= {Mishra, Nikita and Imes, Connor and Lafferty, John D. and
		  Hoffmann, Henry},
  month		= mar,
  year		= {2018},
  pages		= {184--198},
  file		= {Mishra et al. - 2018 - CALOREE Learning Control for
		  Predictable Latency
		  .pdf:/home/prateeks/Zotero/storage/N2TH65IH/Mishra et al. -
		  2018 - CALOREE Learning Control for Predictable Latency
		  .pdf:application/pdf}
}

@InCollection{	  mocskos_faaster_2018,
  address	= {Cham},
  title		= {{FaaSter}, {Better}, {Cheaper}: {The} {Prospect} of
		  {Serverless} {Scientific} {Computing} and {HPC}},
  volume	= {796},
  isbn		= {978-3-319-73352-4 978-3-319-73353-1},
  shorttitle	= {{FaaSter}, {Better}, {Cheaper}},
  url		= {http://link.springer.com/10.1007/978-3-319-73353-1_11},
  abstract	= {The adoption of cloud computing facilities and programming
		  models diﬀers vastly between diﬀerent application
		  domains. Scalable web applications, low-latency mobile
		  backends and on-demand provisioned databases are typical
		  cases for which cloud services on the platform or
		  infrastructure level exist and are convincing when
		  considering technical and economical arguments.
		  Applications with speciﬁc processing demands, including
		  high-performance computing, high-throughput computing and
		  certain ﬂavours of scientiﬁc computing, have
		  historically required special conﬁgurations such as
		  compute- or memory-optimised virtual machine instances.
		  With the rise of function-level compute instances through
		  Function-as-a-Service (FaaS) models, the ﬁtness of
		  generic conﬁgurations needs to be re-evaluated for these
		  applications. We analyse several demanding computing tasks
		  with regards to how FaaS models compare against
		  conventional monolithic algorithm execution. Beside the
		  comparison, we contribute a reﬁned FaaSiﬁcation process
		  for legacy software and provide a roadmap for future
		  work.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {High {Performance} {Computing}},
  publisher	= {Springer International Publishing},
  author	= {Spillner, Josef and Mateos, Cristian and Monge, David A.},
  editor	= {Mocskos, Esteban and Nesmachnow, Sergio},
  year		= {2018},
  doi		= {10.1007/978-3-319-73353-1_11},
  pages		= {154--168},
  file		= {Spillner et al. - 2018 - FaaSter, Better, Cheaper The
		  Prospect of
		  Serverle.pdf:/home/prateeks/Zotero/storage/4X87DHXU/Spillner
		  et al. - 2018 - FaaSter, Better, Cheaper The Prospect of
		  Serverle.pdf:application/pdf}
}

###InCollection{  mocskos_faaster_2018,
  address	= {Cham},
  title		= {{FaaSter}, {Better}, {Cheaper}: {The} {Prospect} of
		  {Serverless} {Scientific} {Computing} and {HPC}},
  volume	= {796},
  isbn		= {978-3-319-73352-4 978-3-319-73353-1},
  shorttitle	= {{FaaSter}, {Better}, {Cheaper}},
  url		= {http://link.springer.com/10.1007/978-3-319-73353-1_11},
  abstract	= {The adoption of cloud computing facilities and programming
		  models diﬀers vastly between diﬀerent application
		  domains. Scalable web applications, low-latency mobile
		  backends and on-demand provisioned databases are typical
		  cases for which cloud services on the platform or
		  infrastructure level exist and are convincing when
		  considering technical and economical arguments.
		  Applications with speciﬁc processing demands, including
		  high-performance computing, high-throughput computing and
		  certain ﬂavours of scientiﬁc computing, have
		  historically required special conﬁgurations such as
		  compute- or memory-optimised virtual machine instances.
		  With the rise of function-level compute instances through
		  Function-as-a-Service (FaaS) models, the ﬁtness of
		  generic conﬁgurations needs to be re-evaluated for these
		  applications. We analyse several demanding computing tasks
		  with regards to how FaaS models compare against
		  conventional monolithic algorithm execution. Beside the
		  comparison, we contribute a reﬁned FaaSiﬁcation process
		  for legacy software and provide a roadmap for future
		  work.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {High {Performance} {Computing}},
  publisher	= {Springer International Publishing},
  author	= {Spillner, Josef and Mateos, Cristian and Monge, David A.},
  editor	= {Mocskos, Esteban and Nesmachnow, Sergio},
  year		= {2018},
  doi		= {10.1007/978-3-319-73353-1_11},
  pages		= {154--168},
  file		= {Spillner et al. - 2018 - FaaSter, Better, Cheaper The
		  Prospect of
		  Serverle.pdf:/home/prateeks/Zotero/storage/4X87DHXU/Spillner
		  et al. - 2018 - FaaSter, Better, Cheaper The Prospect of
		  Serverle.pdf:application/pdf}
}

###InCollection{  mocskos_faaster_2018,
  address	= {Cham},
  title		= {{FaaSter}, {Better}, {Cheaper}: {The} {Prospect} of
		  {Serverless} {Scientific} {Computing} and {HPC}},
  volume	= {796},
  isbn		= {978-3-319-73352-4 978-3-319-73353-1},
  shorttitle	= {{FaaSter}, {Better}, {Cheaper}},
  url		= {http://link.springer.com/10.1007/978-3-319-73353-1_11},
  abstract	= {The adoption of cloud computing facilities and programming
		  models diﬀers vastly between diﬀerent application
		  domains. Scalable web applications, low-latency mobile
		  backends and on-demand provisioned databases are typical
		  cases for which cloud services on the platform or
		  infrastructure level exist and are convincing when
		  considering technical and economical arguments.
		  Applications with speciﬁc processing demands, including
		  high-performance computing, high-throughput computing and
		  certain ﬂavours of scientiﬁc computing, have
		  historically required special conﬁgurations such as
		  compute- or memory-optimised virtual machine instances.
		  With the rise of function-level compute instances through
		  Function-as-a-Service (FaaS) models, the ﬁtness of
		  generic conﬁgurations needs to be re-evaluated for these
		  applications. We analyse several demanding computing tasks
		  with regards to how FaaS models compare against
		  conventional monolithic algorithm execution. Beside the
		  comparison, we contribute a reﬁned FaaSiﬁcation process
		  for legacy software and provide a roadmap for future
		  work.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {High {Performance} {Computing}},
  publisher	= {Springer International Publishing},
  author	= {Spillner, Josef and Mateos, Cristian and Monge, David A.},
  editor	= {Mocskos, Esteban and Nesmachnow, Sergio},
  year		= {2018},
  doi		= {10.1007/978-3-319-73353-1_11},
  pages		= {154--168},
  file		= {Spillner et al. - 2018 - FaaSter, Better, Cheaper The
		  Prospect of
		  Serverle.pdf:/home/prateeks/Zotero/storage/4X87DHXU/Spillner
		  et al. - 2018 - FaaSter, Better, Cheaper The Prospect of
		  Serverle.pdf:application/pdf}
}

@Article{	  mohan_agile_2019,
  title		= {Agile {Cold} {Starts} for {Scalable} {Serverless}},
  abstract	= {The Serverless or Function-as-a-Service (FaaS) model
		  capitalizes on lightweight execution by packaging code and
		  dependencies together for just-in-time dispatch. Often a
		  container environment has to be set up afresh– a
		  condition called “cold start", and in such cases,
		  performance suffers and overheads mount, both deteriorating
		  rapidly under high concurrency. Caching and reusing
		  previously employed containers ties up memory and risks
		  information leakage. Latency for cold starts is frequently
		  due to work and wait-times in setting up various
		  dependencies – such as in initializing networking
		  elements. This paper proposes a solution that pre-crafts
		  such resources and then dynamically reassociates them with
		  baseline containers. Applied to networking, this approach
		  demonstrates an order of magnitude gain in cold starts,
		  negligible memory consumption, and ﬂat startup time under
		  rising concurrency.},
  language	= {en},
  journal	= {USENIX Workshop on Hot Topics in Cloud Computing
		  (HotCloud)},
  author	= {Mohan, Anup and Sane, Harshad and Doshi, Kshitij and
		  Edupuganti, Saikrishna and Sukhomlinov, Vadim and Nayak,
		  Naren},
  year		= {2019},
  pages		= {6},
  file		= {Mohan et al. - Agile Cold Starts for Scalable
		  Serverless.pdf:/home/prateeks/Zotero/storage/KKUKA8EB/Mohan
		  et al. - Agile Cold Starts for Scalable
		  Serverless.pdf:application/pdf}
}

###Article{	  mohan_agile_2019,
  title		= {Agile {Cold} {Starts} for {Scalable} {Serverless}},
  abstract	= {The Serverless or Function-as-a-Service (FaaS) model
		  capitalizes on lightweight execution by packaging code and
		  dependencies together for just-in-time dispatch. Often a
		  container environment has to be set up afresh– a
		  condition called “cold start", and in such cases,
		  performance suffers and overheads mount, both deteriorating
		  rapidly under high concurrency. Caching and reusing
		  previously employed containers ties up memory and risks
		  information leakage. Latency for cold starts is frequently
		  due to work and wait-times in setting up various
		  dependencies – such as in initializing networking
		  elements. This paper proposes a solution that pre-crafts
		  such resources and then dynamically reassociates them with
		  baseline containers. Applied to networking, this approach
		  demonstrates an order of magnitude gain in cold starts,
		  negligible memory consumption, and ﬂat startup time under
		  rising concurrency.},
  language	= {en},
  journal	= {USENIX Workshop on Hot Topics in Cloud Computing
		  (HotCloud)},
  author	= {Mohan, Anup and Sane, Harshad and Doshi, Kshitij and
		  Edupuganti, Saikrishna and Sukhomlinov, Vadim and Nayak,
		  Naren},
  year		= {2019},
  pages		= {6},
  file		= {Mohan et al. - Agile Cold Starts for Scalable
		  Serverless.pdf:/home/prateeks/Zotero/storage/KKUKA8EB/Mohan
		  et al. - Agile Cold Starts for Scalable
		  Serverless.pdf:application/pdf}
}

@InProceedings{	  mtibaa_towards_2018,
  title		= {Towards {Edge} {Computing} over {Named} {Data}
		  {Networking}},
  doi		= {10.1109/EDGE.2018.00023},
  abstract	= {This paper discusses leveraging the Named Data Networking
		  (NDN) architecture and Named Function Networking (NFN) to
		  facilitate in-network edge computing. In the NDN context,
		  we consider a the Augmented Reality (AR) use-case-a
		  challenging application-to discuss how NDN functionalities
		  can be leveraged for addressing inherent edge computing
		  challenges, such as efficient resource discovery, compute
		  re-use, mobility management, and security. We present
		  several options to tackle the highlighted challenges and
		  where possible provide solutions.},
  booktitle	= {2018 {IEEE} {International} {Conference} on {Edge}
		  {Computing} ({EDGE})},
  author	= {Mtibaa, Abderrahmen and Tourani, Reza and Misra,
		  Satyajayant and Burke, Jeff and Zhang, Lixia},
  month		= jul,
  year		= {2018},
  note		= {ISSN: null},
  keywords	= {augmented reality, Augmented reality, Computer
		  architecture, distributed processing, edge, Edge computing,
		  in-network edge computing, inherent edge computing
		  challenges, IP networks, Mobile handsets, named data
		  networking architecture, named function networking, NDN,
		  NDN context, NDN functionalities, NFN, Quality of
		  experience, security, Task analysis},
  pages		= {117--120},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/SYXAKTNC/8473385.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/4EA226ZY/Mtibaa et al. -
		  2018 - Towards Edge Computing over Named Data
		  Networking.pdf:application/pdf}
}

###InProceedings{ mtibaa_towards_2018,
  title		= {Towards {Edge} {Computing} over {Named} {Data}
		  {Networking}},
  doi		= {10.1109/EDGE.2018.00023},
  abstract	= {This paper discusses leveraging the Named Data Networking
		  (NDN) architecture and Named Function Networking (NFN) to
		  facilitate in-network edge computing. In the NDN context,
		  we consider a the Augmented Reality (AR) use-case-a
		  challenging application-to discuss how NDN functionalities
		  can be leveraged for addressing inherent edge computing
		  challenges, such as efficient resource discovery, compute
		  re-use, mobility management, and security. We present
		  several options to tackle the highlighted challenges and
		  where possible provide solutions.},
  booktitle	= {2018 {IEEE} {International} {Conference} on {Edge}
		  {Computing} ({EDGE})},
  author	= {Mtibaa, Abderrahmen and Tourani, Reza and Misra,
		  Satyajayant and Burke, Jeff and Zhang, Lixia},
  month		= jul,
  year		= {2018},
  note		= {ISSN: null},
  keywords	= {augmented reality, Augmented reality, Computer
		  architecture, distributed processing, edge, Edge computing,
		  in-network edge computing, inherent edge computing
		  challenges, IP networks, Mobile handsets, named data
		  networking architecture, named function networking, NDN,
		  NDN context, NDN functionalities, NFN, Quality of
		  experience, security, Task analysis},
  pages		= {117--120},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/SYXAKTNC/8473385.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/4EA226ZY/Mtibaa et al. -
		  2018 - Towards Edge Computing over Named Data
		  Networking.pdf:application/pdf}
}

@Article{	  mukhanov_alea_2017,
  title		= {{ALEA}: {A} {Fine}-{Grained} {Energy} {Profiling} {Tool}},
  volume	= {14},
  issn		= {1544-3566, 1544-3973},
  shorttitle	= {{ALEA}},
  url		= {https://dl.acm.org/doi/10.1145/3050436},
  doi		= {10.1145/3050436},
  abstract	= {Energy efficiency is becoming increasingly important, yet
		  few developers understand how source code changes affect
		  the energy and power consumption of their programs. To
		  enable them to achieve energy savings, we must associate
		  energy consumption with software structures, especially at
		  the fine-grained level of functions and loops. Most
		  research in the field relies on direct power/energy
		  measurements taken from on-board sensors or performance
		  counters. However, this coarse granularity does not
		  directly provide the needed fine-grained measurements. This
		  article presents ALEA, a novel fine-grained energy
		  profiling tool based on probabilistic analysis for
		  fine-grained energy accounting. ALEA overcomes the
		  limitations of coarse-grained power-sensing instruments to
		  associate energy information effectively with source code
		  at a fine-grained level. We demonstrate and validate that
		  ALEA can perform accurate energy profiling at various
		  granularity levels on two different architectures: Intel
		  Sandy Bridge and ARM big.LITTLE. ALEA achieves a worst-case
		  error of only 2\% for coarse-grained code structures and
		  6\% for fine-grained ones, with less than 1\% runtime
		  overhead. Our use cases demonstrate that ALEA supports
		  energy optimizations, with energy savings of up to 2.87
		  times for a latency-critical option pricing workload under
		  a given power budget.},
  language	= {en},
  number	= {1},
  urldate	= {2022-04-11},
  journal	= {ACM Transactions on Architecture and Code Optimization},
  author	= {Mukhanov, Lev and Petoumenos, Pavlos and Wang, Zheng and
		  Parasyris, Nikos and Nikolopoulos, Dimitrios S. and De
		  Supinski, Bronis R. and Leather, Hugh},
  month		= apr,
  year		= {2017},
  pages		= {1--25},
  file		= {Mukhanov et al. - 2017 - ALEA A Fine-Grained Energy
		  Profiling
		  Tool.pdf:/home/prateeks/Zotero/storage/ME4DKH8J/Mukhanov et
		  al. - 2017 - ALEA A Fine-Grained Energy Profiling
		  Tool.pdf:application/pdf}
}

###Article{	  mukhanov_alea_2017,
  title		= {{ALEA}: {A} {Fine}-{Grained} {Energy} {Profiling} {Tool}},
  volume	= {14},
  issn		= {1544-3566, 1544-3973},
  shorttitle	= {{ALEA}},
  url		= {https://dl.acm.org/doi/10.1145/3050436},
  doi		= {10.1145/3050436},
  abstract	= {Energy efficiency is becoming increasingly important, yet
		  few developers understand how source code changes affect
		  the energy and power consumption of their programs. To
		  enable them to achieve energy savings, we must associate
		  energy consumption with software structures, especially at
		  the fine-grained level of functions and loops. Most
		  research in the field relies on direct power/energy
		  measurements taken from on-board sensors or performance
		  counters. However, this coarse granularity does not
		  directly provide the needed fine-grained measurements. This
		  article presents ALEA, a novel fine-grained energy
		  profiling tool based on probabilistic analysis for
		  fine-grained energy accounting. ALEA overcomes the
		  limitations of coarse-grained power-sensing instruments to
		  associate energy information effectively with source code
		  at a fine-grained level. We demonstrate and validate that
		  ALEA can perform accurate energy profiling at various
		  granularity levels on two different architectures: Intel
		  Sandy Bridge and ARM big.LITTLE. ALEA achieves a worst-case
		  error of only 2\% for coarse-grained code structures and
		  6\% for fine-grained ones, with less than 1\% runtime
		  overhead. Our use cases demonstrate that ALEA supports
		  energy optimizations, with energy savings of up to 2.87
		  times for a latency-critical option pricing workload under
		  a given power budget.},
  language	= {en},
  number	= {1},
  urldate	= {2022-07-22},
  journal	= {ACM Transactions on Architecture and Code Optimization},
  author	= {Mukhanov, Lev and Petoumenos, Pavlos and Wang, Zheng and
		  Parasyris, Nikos and Nikolopoulos, Dimitrios S. and De
		  Supinski, Bronis R. and Leather, Hugh},
  month		= apr,
  year		= {2017},
  pages		= {1--25},
  file		= {Mukhanov et al. - 2017 - ALEA A Fine-Grained Energy
		  Profiling
		  Tool.pdf:/home/prateeks/Zotero/storage/IAT7BJLF/Mukhanov et
		  al. - 2017 - ALEA A Fine-Grained Energy Profiling
		  Tool.pdf:application/pdf}
}

@InProceedings{	  mvondo2021ofc,
  title		= {OFC: an opportunistic caching system for FaaS platforms},
  author	= {Mvondo, Djob and Bacou, Mathieu and Nguetchouang, Kevin
		  and Ngale, Lucien and Pouget, St{\'e}phane and Kouam,
		  Josiane and Lachaize, Renaud and Hwang, Jinho and Wood, Tim
		  and Hagimont, Daniel and others},
  booktitle	= {Proceedings of the Sixteenth European Conference on
		  Computer Systems},
  pages		= {228--244},
  year		= {2021}
}

@InProceedings{	  mytkowicz2009producing,
  title		= {Producing Wrong Data without Doing Anything Obviously
		  Wrong!},
  author	= {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias
		  and Sweeney, Peter F.},
  booktitle	= {Proceedings of the 14th International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {265–276},
  year		= {2009},
  isbn		= {9781605584065},
  publisher	= {Association for Computing Machinery},
  organization	= {ACM},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/1508244.1508275},
  doi		= {10.1145/1508244.1508275},
  keywords	= {measurement, performance, bias},
  location	= {Washington, DC, USA},
  series	= {ASPLOS XIV}
}

###Article{	  mytkowicz2009producing,
  title		= {Producing wrong data without doing anything obviously
		  wrong!},
  author	= {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias
		  and Sweeney, Peter F},
  journal	= {ACM Sigplan Notices},
  volume	= {44},
  number	= {3},
  pages		= {265--276},
  year		= {2009},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  narayan2013power,
  title		= {Power-aware cloud metering},
  author	= {Narayan, Akshay and Rao, Shrisha},
  journal	= {IEEE Transactions on Services Computing},
  volume	= {7},
  number	= {3},
  pages		= {440--451},
  year		= {2013},
  publisher	= {IEEE}
}

@InProceedings{	  narayanan2018deepcache,
  title		= {Deepcache: A deep learning based framework for content
		  caching},
  author	= {Narayanan, Arvind and Verma, Saurabh and Ramadan, Eman and
		  Babaie, Pariya and Zhang, Zhi-Li},
  booktitle	= {Proceedings of the 2018 Workshop on Network Meets AI \&
		  ML},
  pages		= {48--53},
  year		= {2018}
}

@InProceedings{	  narayanan_mediating_2020,
  title		= {Mediating {Power} {Struggles} on a {Shared} {Server}},
  doi		= {10.1109/ISPASS48437.2020.00030},
  abstract	= {Most of today's servers, with numerous CPU cores and other
		  plentiful direct resources, host co-located workloads using
		  mechanisms to reduce hardware resource contention. However,
		  power is an equally important indirect resource in a server
		  that is shared between the co-located applications, for
		  which they can contend, especially when power budgets are
		  tight. We refer to this as a "power struggle". While there
		  is a considerable amount of prior effort on server power
		  capping, they are largely oblivious to power as an
		  indirectly shared resource. This indirect resource exhibits
		  a unique set of properties - spatial non-multiplexing,
		  non-convex, fluidic, time-shifting and dynamic capacity -
		  which have not been explicitly tackled so far. We propose
		  policies that explicitly consider these properties to
		  mediate power struggles, implement these on real hardware,
		  and provide experimental results to show the performance
		  benefits of our solution.},
  booktitle	= {2020 {IEEE} {International} {Symposium} on {Performance}
		  {Analysis} of {Systems} and {Software} ({ISPASS})},
  author	= {Narayanan, Iyswarya and Sivasubramaniam, Anand},
  month		= aug,
  year		= {2020},
  keywords	= {Hardware, Servers, Software, Dynamic scheduling,
		  Performance analysis, Fluid dynamics},
  pages		= {149--159},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/9E2Y4W2U/9238623.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/BDDU9K7N/Narayanan and
		  Sivasubramaniam - 2020 - Mediating Power Struggles on a
		  Shared Server.pdf:application/pdf}
}

@Article{	  nitu2018working,
  title		= {Working set size estimation techniques in virtualized
		  environments: One size does not fit all},
  author	= {Nitu, Vlad and Kocharyan, Aram and Yaya, Hannas and
		  Tchana, Alain and Hagimont, Daniel and Astsatryan,
		  Hrachya},
  journal	= {Proceedings of the ACM on Measurement and Analysis of
		  Computing Systems},
  volume	= {2},
  number	= {1},
  pages		= {1--22},
  year		= {2018},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  noureddine2013review,
  title		= {A review of energy measurement approaches},
  author	= {Noureddine, Adel and Rouvoy, Romain and Seinturier,
		  Lionel},
  journal	= {ACM SIGOPS Operating Systems Review},
  volume	= {47},
  number	= {3},
  pages		= {42--49},
  year		= {2013},
  publisher	= {ACM New York, NY, USA}
}

@InProceedings{	  novak_cloud_2019,
  title		= {Cloud {Functions} for {Fast} and {Robust} {Resource}
		  {Auto}-{Scaling}},
  doi		= {10.1109/COMSNETS.2019.8711058},
  abstract	= {We design and build FEAT, a new scaling approach that uses
		  (1) cloud functions as interim processing resources to
		  compensate for VM launch delays and (2) a reactive,
		  knobless, auto-scaling algorithm that requires no
		  pre-specified thresholds or parameters, making it robust
		  against changing load. We implement FEAT on Amazon Web
		  Services (AWS) and Microsoft Azure. Our evaluations clearly
		  demonstrate the higher performance and robustness of FEAT
		  in comparison to existing approaches.},
  booktitle	= {2019 11th {International} {Conference} on {Communication}
		  {Systems} {Networks} ({COMSNETS})},
  author	= {Novak, Joe H. and Kasera, Sneha Kumar and Stutsman, Ryan},
  month		= jan,
  year		= {2019},
  note		= {ISSN: 2155-2487},
  keywords	= {Cloud computing, Servers, cloud computing, resource
		  allocation, virtual machines, Web services, robustness,
		  Runtime, Delays, Load modeling, Amazon Web Services, cloud
		  functions, Current measurement, fast robust resource
		  autoscaling, FEAT, interim processing resources, Microsoft
		  Azure, reactive knobless autoscaling algorithm, scaling
		  approach, VM launch delays},
  pages		= {133--140},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/3XCH46NG/8711058.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/S4WN76WY/Novak et al. -
		  2019 - Cloud Functions for Fast and Robust Resource
		  Auto-.pdf:application/pdf}
}

@Misc{		  nuclio,
  title		= {{Automate the Data Science Pipeline with Serverless
		  Functions}},
  howpublished	= {\url{https://nuclio.io/}},
  year		= {2023}
}

@Article{	  nygren2010akamai,
  title		= {The akamai network: a platform for high-performance
		  internet applications},
  author	= {Nygren, Erik and Sitaraman, Ramesh K and Sun, Jennifer},
  journal	= {ACM SIGOPS Operating Systems Review},
  volume	= {44},
  number	= {3},
  pages		= {2--19},
  year		= {2010},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  o1993lru,
  title		= {The LRU-K page replacement algorithm for database disk
		  buffering},
  author	= {O'neil, Elizabeth J and O'neil, Patrick E and Weikum,
		  Gerhard},
  journal	= {Acm Sigmod Record},
  volume	= {22},
  number	= {2},
  pages		= {297--306},
  year		= {1993},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  oakes_sock_2018,
  title		= {{SOCK}: {Rapid} {Task} {Provisioning} with
		  {Serverless}-{Optimized} {Containers}},
  volume	= {USENIX ATC},
  abstract	= {Serverless computing promises to provide applications with
		  cost savings and extreme elasticity. Unfortunately, slow
		  application and container initialization can hurt
		  common-case latency on serverless platforms. In this work,
		  we analyze Linux container primitives, identifying
		  scalability bottlenecks related to storage and network
		  isolation. We also analyze Python applications from GitHub
		  and show that importing many popular libraries adds about
		  100 ms to startup. Based on these ﬁndings, we implement
		  SOCK, a container system optimized for serverless
		  workloads. Careful avoidance of kernel scalability
		  bottlenecks gives SOCK an 18× speedup over Docker. A
		  generalized-Zygote provisioning strategy yields an
		  additional 3× speedup. A more sophisticated three-tier
		  caching strategy based on Zygotes provides a 45× speedup
		  over SOCK without Zygotes. Relative to AWS Lambda and
		  OpenWhisk, OpenLambda with SOCK reduces platform overheads
		  by 2.8× and 5.3× respectively in an image processing case
		  study.},
  language	= {en},
  author	= {Oakes, Edward and Yang, Leon and Zhou, Dennis and Houck,
		  Kevin and Harter, Tyler and Arpaci-Dusseau, Andrea C and
		  Arpaci-Dusseau, Remzi H},
  year		= {2018},
  pages		= {14},
  file		= {Oakes et al. - SOCK Rapid Task Provisioning with
		  Serverless-Opti.pdf:/home/prateeks/Zotero/storage/GT5DXNVP/Oakes
		  et al. - SOCK Rapid Task Provisioning with
		  Serverless-Opti.pdf:application/pdf}
}

###Article{	  oakes_sock_2018,
  title		= {{SOCK}: {Rapid} {Task} {Provisioning} with
		  {Serverless}-{Optimized} {Containers}},
  volume	= {USENIX ATC},
  abstract	= {Serverless computing promises to provide applications with
		  cost savings and extreme elasticity. Unfortunately, slow
		  application and container initialization can hurt
		  common-case latency on serverless platforms. In this work,
		  we analyze Linux container primitives, identifying
		  scalability bottlenecks related to storage and network
		  isolation. We also analyze Python applications from GitHub
		  and show that importing many popular libraries adds about
		  100 ms to startup. Based on these ﬁndings, we implement
		  SOCK, a container system optimized for serverless
		  workloads. Careful avoidance of kernel scalability
		  bottlenecks gives SOCK an 18× speedup over Docker. A
		  generalized-Zygote provisioning strategy yields an
		  additional 3× speedup. A more sophisticated three-tier
		  caching strategy based on Zygotes provides a 45× speedup
		  over SOCK without Zygotes. Relative to AWS Lambda and
		  OpenWhisk, OpenLambda with SOCK reduces platform overheads
		  by 2.8× and 5.3× respectively in an image processing case
		  study.},
  language	= {en},
  author	= {Oakes, Edward and Yang, Leon and Zhou, Dennis and Houck,
		  Kevin and Harter, Tyler and Arpaci-Dusseau, Andrea C and
		  Arpaci-Dusseau, Remzi H},
  year		= {2018},
  pages		= {14},
  file		= {Oakes et al. - SOCK Rapid Task Provisioning with
		  Serverless-Opti.pdf:/home/prateeks/Zotero/storage/GT5DXNVP/Oakes
		  et al. - SOCK Rapid Task Provisioning with
		  Serverless-Opti.pdf:application/pdf}
}

@Misc{		  oci,
  title		= {{Open Container Initiative}},
  howpublished	= {\url{https://opencontainers.org/}},
  year		= {2015}
}

###Misc{	  oci,
  title		= {Open Container Initiative},
  howpublished	= {\url{https://opencontainers.org/}}
}

@InProceedings{	  oliner2013carat,
  title		= {Carat: Collaborative energy diagnosis for mobile devices},
  author	= {Oliner, Adam J and Iyer, Anand P and Stoica, Ion and
		  Lagerspetz, Eemil and Tarkoma, Sasu},
  booktitle	= {Proceedings of the 11th ACM conference on embedded
		  networked sensor systems},
  pages		= {1--14},
  year		= {2013}
}

@Misc{		  openfaas,
  title		= {{OpenFaaS : Server Functions, Made Simple.}},
  howpublished	= {\url{https://www.openfaas.com}},
  year		= {2020}
}

###Misc{	  openfaas,
  title		= {{OpenFaaS : Server Functions, Made Simple.}},
  howpublished	= {\url{https://www.openfaas.com}},
  year		= {2020}
}

###Misc{	  openfaas,
  title		= {{OpenFaaS : Server Functions, Made Simple.}},
  howpublished	= {\url{https://www.openfaas.com}},
  year		= {2020}
}

@Misc{		  openwhisk,
  title		= {{Apache OpenWhisk: Open Source Serverless Cloud
		  Platform}},
  howpublished	= {\url{https://openwhisk.apache.org/}},
  year		= {2020}
}

###Misc{	  openwhisk,
  title		= {{Apache OpenWhisk: Open Source Serverless Cloud
		  Platform}},
  howpublished	= {\url{https://openwhisk.apache.org/}},
  year		= {2020}
}

@InProceedings{	  osca_atc20,
  author	= {Yu Zhang and Ping Huang and Ke Zhou and Hua Wang and
		  Jianying Hu and Yongguang Ji and Bin Cheng},
  title		= {{OSCA}: An Online-Model Based Cache Allocation Scheme in
		  Cloud Block Storage Systems},
  booktitle	= {2020 {USENIX} Annual Technical Conference ({USENIX} {ATC}
		  20)},
  year		= {2020},
  isbn		= {978-1-939133-14-4},
  pages		= {785--798},
  url		= {https://www.usenix.org/conference/atc20/presentation/zhang-yu},
  publisher	= {{USENIX} Association},
  month		= jul
}

@InProceedings{	  ournani2020taming,
  title		= {Taming energy consumption variations in systems
		  benchmarking},
  author	= {Ournani, Zakaria and Belgaid, Mohammed Chakib and Rouvoy,
		  Romain and Rust, Pierre and Penhoat, Joel and Seinturier,
		  Lionel},
  booktitle	= {Proceedings of the ACM/SPEC International Conference on
		  Performance Engineering},
  pages		= {36--47},
  year		= {2020}
}

@InProceedings{	  ow-qos-2022,
  author	= {Russo, Gabriele Russo and Milani, Alfredo and Iannucci,
		  Stefano and Cardellini, Valeria},
  booktitle	= {2022 IEEE International Conference on Pervasive Computing
		  and Communications Workshops and other Affiliated Events
		  (PerCom Workshops)},
  title		= {Towards QoS-Aware Function Composition Scheduling in
		  Apache OpenWhisk},
  year		= {2022},
  volume	= {},
  number	= {},
  pages		= {693-698},
  doi		= {10.1109/PerComWorkshops53856.2022.}
}
@inproceedings{abdi2023palette,
  title     = {Palette load balancing: Locality hints for serverless functions},
  author    = {Abdi, Mania and Ginzburg, Samuel and Lin, Xiayue Charles and Faleiro, Jose and Chaudhry, Gohar Irfan and Goiri, Inigo and Bianchini, Ricardo and Berger, Daniel S and Fonseca, Rodrigo},
  booktitle = {Proceedings of the Eighteenth European Conference on Computer Systems},
  pages     = {365--380},
  year      = {2023}
}

@InProceedings{	  package-cristina-19,
  author	= {Aumala, Gabriel and Boza, Edwin and Ortiz-Avilés, Luis
		  and Totoy, Gustavo and Abad, Cristina},
  booktitle	= {2019 19th IEEE/ACM International Symposium on Cluster,
		  Cloud and Grid Computing (CCGRID)},
  title		= {Beyond Load Balancing: Package-Aware Scheduling for
		  Serverless Platforms},
  year		= {2019},
  volume	= {},
  number	= {},
  pages		= {282-291},
  doi		= {10.1109/CCGRID.2019.00042}
}

@InProceedings{	  palade-edge-22,
  author	= {Palade, Andrei and Kazmi, Aqeel and Clarke, Siobhán},
  booktitle	= {2019 IEEE World Congress on Services (SERVICES)},
  title		= {An Evaluation of Open Source Serverless Computing
		  Frameworks Support at the Edge},
  year		= {2019},
  volume	= {2642-939X},
  number	= {},
  pages		= {206-211},
  doi		= {10.1109/SERVICES.2019.00057}
}

@article{wen2023rise,
  title     = {Rise of the planet of serverless computing: A systematic review},
  author    = {Wen, Jinfeng and Chen, Zhenpeng and Jin, Xin and Liu, Xuanzhe},
  journal   = {ACM Transactions on Software Engineering and Methodology},
  volume    = {32},
  number    = {5},
  pages     = {1--61},
  year      = {2023},
  publisher = {ACM New York, NY, USA}
}

@InProceedings{	  pfandzelter_tinyfaas_2020,
  address	= {Sydney, Australia},
  title		= {{tinyFaaS}: {A} {Lightweight} {FaaS} {Platform} for {Edge}
		  {Environments}},
  isbn		= {978-1-72811-086-8},
  shorttitle	= {{tinyFaaS}},
  url		= {https://ieeexplore.ieee.org/document/9103476/},
  doi		= {10.1109/ICFC49376.2020.00011},
  abstract	= {The Function-as-a-Service (FaaS) model is a great ﬁt for
		  data and event processing in the Internet of Things (IoT).
		  Sending all data to a cloud-based FaaS platform, however,
		  may cause performance and privacy issues. While these
		  issues could be mitigated using edge computing, existing
		  FaaS approaches, designed for the cloud, are too
		  heavyweight to run on small, constrained edge nodes.},
  language	= {en},
  urldate	= {2023-01-09},
  booktitle	= {2020 {IEEE} {International} {Conference} on {Fog}
		  {Computing} ({ICFC})},
  publisher	= {IEEE},
  author	= {Pfandzelter, Tobias and Bermbach, David},
  month		= apr,
  year		= {2020},
  pages		= {17--24},
  file		= {Pfandzelter and Bermbach - 2020 - tinyFaaS A Lightweight
		  FaaS Platform for Edge
		  Env.pdf:/home/prateeks/Zotero/storage/H2BVTP8A/Pfandzelter
		  and Bermbach - 2020 - tinyFaaS A Lightweight FaaS Platform
		  for Edge Env.pdf:application/pdf}
}

@Misc{		  pid-wiki,
  title		= {{PID} Controllers},
  howpublished	= {\url{https://en.wikipedia.org/wiki/PID\_controller}}
}

@InProceedings{	  pitchumani2015realistic,
  title		= {Realistic request arrival generation in storage
		  benchmarks},
  author	= {Pitchumani, Rekha and Frank, Shayna and Miller, Ethan L},
  booktitle	= {2015 31st Symposium on Mass Storage Systems and
		  Technologies (MSST)},
  pages		= {1--10},
  year		= {2015},
  organization	= {IEEE}
}

###InProceedings{ pitchumani2015realistic,
  title		= {Realistic request arrival generation in storage
		  benchmarks},
  author	= {Pitchumani, Rekha and Frank, Shayna and Miller, Ethan L},
  booktitle	= {2015 31st Symposium on Mass Storage Systems and
		  Technologies (MSST)},
  pages		= {1--10},
  year		= {2015},
  organization	= {IEEE}
}

@InProceedings{	  popa_http_2010,
  address	= {Monterey California},
  title		= {{HTTP} as the narrow waist of the future internet},
  isbn		= {978-1-4503-0409-2},
  url		= {https://dl.acm.org/doi/10.1145/1868447.1868453},
  doi		= {10.1145/1868447.1868453},
  abstract	= {Over the past decade a variety of network architectures
		  have been proposed to address IP’s limitations in terms
		  of ﬂexible forwarding, security, and data distribution.
		  Meanwhile, fueled by the explosive growth of video trafﬁc
		  and HTTP infrastructure (e.g., CDNs, web caches), HTTP has
		  became the de-facto protocol for deploying new services and
		  applications. Given these developments, we argue that these
		  architectures should be evaluated not only with respect to
		  IP, but also with respect to HTTP, and that HTTP could be a
		  fertile ground (more so than IP) for deploying the newly
		  proposed functionalities. In this paper, we take a step in
		  this direction, and ﬁnd that HTTP already provides many
		  of the desired properties for new Internet architectures.
		  HTTP is a content centric protocol, provides middlebox
		  support in the form of reverse and forward proxies, and
		  leverages DNS to decouple names from addresses. We then
		  investigate HTTP’s limitations, and propose an extension,
		  called S-GET that provides support for low-latency
		  applications, such as VoIP and chat.},
  language	= {en},
  urldate	= {2023-01-24},
  booktitle	= {Proceedings of the 9th {ACM} {SIGCOMM} {Workshop} on {Hot}
		  {Topics} in {Networks}},
  publisher	= {ACM},
  author	= {Popa, Lucian and Ghodsi, Ali and Stoica, Ion},
  month		= oct,
  year		= {2010},
  pages		= {1--6},
  file		= {Popa et al. - 2010 - HTTP as the narrow waist of the
		  future
		  internet.pdf:/home/prateeks/Zotero/storage/X4ZCJSZD/Popa et
		  al. - 2010 - HTTP as the narrow waist of the future
		  internet.pdf:application/pdf}
}

@Misc{		  powerapi-bug,
  title		= {{PowerAPI-NG github issue: RAPL\_ENERGY\_PKG is invalid or
		  unsupported}},
  howpublished	= {\url{https://github.com/powerapi-ng/powerapi/issues/125}}
}

@InProceedings{	  prakash2021optimizing,
  title		= {Optimizing Goodput of Real-time Serverless Functions using
		  Dynamic Slicing with vGPUs},
  author	= {Prakash, Chandra and Garg, Anshuj and Bellur, Umesh and
		  Kulkarni, Purushottam and Kurkure, Uday and Sivaraman, Hari
		  and Vu, Lan},
  booktitle	= {2021 IEEE International Conference on Cloud Engineering
		  (IC2E)},
  pages		= {60--70},
  year		= {2021},
  organization	= {IEEE}
}

@InProceedings{	  prekas2017zygos,
  title		= {Zygos: Achieving low tail latency for microsecond-scale
		  networked tasks},
  author	= {Prekas, George and Kogias, Marios and Bugnion, Edouard},
  booktitle	= {Proceedings of the 26th Symposium on Operating Systems
		  Principles},
  pages		= {325--341},
  year		= {2017}
}

@Article{	  przybylski2021data,
  title		= {Data-driven scheduling in serverless computing to reduce
		  response time},
  author	= {Przybylski, Bart{\l}omiej and {\.Z}uk, Pawe{\l} and
		  Rzadca, Krzysztof},
  journal	= {arXiv preprint arXiv:2105.03217},
  year		= {2021}
}

@Article{	  pu_shufing_2019,
  title		= {Shufﬂing, {Fast} and {Slow}: {Scalable} {Analytics} on
		  {Serverless} {Infrastructure}},
  abstract	= {Serverless computing is poised to fulﬁll the long-held
		  promise of transparent elasticity and millisecond-level
		  pricing. To achieve this goal, service providers impose a
		  ﬁnegrained computational model where every function has a
		  maximum duration, a ﬁxed amount of memory and no
		  persistent local storage. We observe that the ﬁne-grained
		  elasticity of serverless is key to achieve high utilization
		  for general computations such as analytics workloads, but
		  that resource limits make it challenging to implement such
		  applications as they need to move large amounts of data
		  between functions that don’t overlap in time. In this
		  paper, we present Locus, a serverless analytics system that
		  judiciously combines (1) cheap but slow storage with (2)
		  fast but expensive storage, to achieve good performance
		  while remaining cost-efﬁcient. Locus applies a
		  performance model to guide users in selecting the type and
		  the amount of storage to achieve the desired
		  cost-performance trade-off. We evaluate Locus on a number
		  of analytics applications including TPC-DS, CloudSort, Big
		  Data Benchmark and show that Locus can navigate the
		  costperformance trade-off, leading to 4⇥-500⇥
		  performance improvements over slow storage-only baseline
		  and reducing resource usage by up to 59\% while achieving
		  comparable performance with running Apache Spark on a
		  cluster of virtual machines, and within 2⇥ slower
		  compared to Redshift.},
  language	= {en},
  journal	= {USENIC NSDI},
  author	= {Pu, Qifan},
  year		= {2019},
  pages		= {15},
  file		= {Pu - Shufﬂing, Fast and Slow Scalable Analytics on
		  Ser.pdf:/home/prateeks/Zotero/storage/CCL9FASY/Pu -
		  Shufﬂing, Fast and Slow Scalable Analytics on
		  Ser.pdf:application/pdf}
}

###Article{	  pu_shufing_2019,
  title		= {Shufﬂing, {Fast} and {Slow}: {Scalable} {Analytics} on
		  {Serverless} {Infrastructure}},
  abstract	= {Serverless computing is poised to fulﬁll the long-held
		  promise of transparent elasticity and millisecond-level
		  pricing. To achieve this goal, service providers impose a
		  ﬁnegrained computational model where every function has a
		  maximum duration, a ﬁxed amount of memory and no
		  persistent local storage. We observe that the ﬁne-grained
		  elasticity of serverless is key to achieve high utilization
		  for general computations such as analytics workloads, but
		  that resource limits make it challenging to implement such
		  applications as they need to move large amounts of data
		  between functions that don’t overlap in time. In this
		  paper, we present Locus, a serverless analytics system that
		  judiciously combines (1) cheap but slow storage with (2)
		  fast but expensive storage, to achieve good performance
		  while remaining cost-efﬁcient. Locus applies a
		  performance model to guide users in selecting the type and
		  the amount of storage to achieve the desired
		  cost-performance trade-off. We evaluate Locus on a number
		  of analytics applications including TPC-DS, CloudSort, Big
		  Data Benchmark and show that Locus can navigate the
		  costperformance trade-off, leading to 4⇥-500⇥
		  performance improvements over slow storage-only baseline
		  and reducing resource usage by up to 59\% while achieving
		  comparable performance with running Apache Spark on a
		  cluster of virtual machines, and within 2⇥ slower
		  compared to Redshift.},
  language	= {en},
  journal	= {USENIC NSDI},
  author	= {Pu, Qifan},
  year		= {2019},
  pages		= {15},
  file		= {Pu - Shufﬂing, Fast and Slow Scalable Analytics on
		  Ser.pdf:/home/prateeks/Zotero/storage/CCL9FASY/Pu -
		  Shufﬂing, Fast and Slow Scalable Analytics on
		  Ser.pdf:application/pdf}
}

###Article{	  pu_shufing_2019,
  title		= {Shuffling, {Fast} and {Slow}: {Scalable} {Analytics} on
		  {Serverless} {Infrastructure}},
  abstract	= {Serverless computing is poised to fulﬁll the long-held
		  promise of transparent elasticity and millisecond-level
		  pricing. To achieve this goal, service providers impose a
		  ﬁnegrained computational model where every function has a
		  maximum duration, a ﬁxed amount of memory and no
		  persistent local storage. We observe that the ﬁne-grained
		  elasticity of serverless is key to achieve high utilization
		  for general computations such as analytics workloads, but
		  that resource limits make it challenging to implement such
		  applications as they need to move large amounts of data
		  between functions that don’t overlap in time. In this
		  paper, we present Locus, a serverless analytics system that
		  judiciously combines (1) cheap but slow storage with (2)
		  fast but expensive storage, to achieve good performance
		  while remaining cost-efﬁcient. Locus applies a
		  performance model to guide users in selecting the type and
		  the amount of storage to achieve the desired
		  cost-performance trade-off. We evaluate Locus on a number
		  of analytics applications including TPC-DS, CloudSort, Big
		  Data Benchmark and show that Locus can navigate the
		  costperformance trade-off, leading to 4⇥-500⇥
		  performance improvements over slow storage-only baseline
		  and reducing resource usage by up to 59\% while achieving
		  comparable performance with running Apache Spark on a
		  cluster of virtual machines, and within 2⇥ slower
		  compared to Redshift.},
  language	= {en},
  journal	= {USENIC NSDI},
  author	= {Pu, Qifan, Venkataraman, Shivaram, Stoica, Ian},
  year		= {2019},
  pages		= {15},
  file		= {Pu - Shufﬂing, Fast and Slow Scalable Analytics on
		  Ser.pdf:/home/prateeks/Zotero/storage/CCL9FASY/Pu -
		  Shufﬂing, Fast and Slow Scalable Analytics on
		  Ser.pdf:application/pdf}
}

@InProceedings{	  puru_xanadu_20,
  author	= {Daw, Nilanjan and Bellur, Umesh and Kulkarni,
		  Purushottam},
  title		= {Xanadu: Mitigating Cascading Cold Starts in Serverless
		  Function Chain Deployments},
  year		= {2020},
  isbn		= {9781450381536},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3423211.3425690},
  doi		= {10.1145/3423211.3425690},
  abstract	= {Organization of tasks as workflows are an essential
		  feature to expand the applicability of the serverless
		  computing framework. Existing serverless platforms are
		  either agnostic to function chains (workflows as a
		  composition of functions) or rely on naive provisioning and
		  management mechanisms of the serverless framework---an
		  example is that they provision resources after the trigger
		  to each function in a workflow arrives thereby forcing a
		  setup latency for each function in the workflow. In this
		  work, we focus on mitigating the cascading cold start
		  problem--- the latency overheads in triggering a sequence
		  of serverless functions according to a workflow
		  specification. We first establish the nature and extent of
		  the cascading effects in cold start situations across
		  multiple commercial server platforms and cloud providers.
		  Towards mitigating these cascading overheads, we design and
		  develop several optimizations, that are built into our tool
		  Xanadu. Xanadu offers multiple instantiation options based
		  on the desired runtime isolation requirements and supports
		  function chaining with or without explicit workflow
		  specifications. Xanadu's optimizations to address the
		  cascading cold start problem are built on speculative and
		  just-in-time provisioning of resources. Our evaluation of
		  the Xanadu system reveals almost complete elimination of
		  cascading cold starts at minimal cost overheads,
		  outperforming the available state of the art platforms. For
		  even relatively short workflows, Xanadu reduces platform
		  overheads by almost 18x compared to Knative and 10x
		  compared to Apache Openwhisk.},
  booktitle	= {Proceedings of the 21st International Middleware
		  Conference},
  pages		= {356-–370},
  numpages	= {15},
  keywords	= {Just-in-time scheduling, Serverless workflows, Speculative
		  deployment},
  location	= {Delft, Netherlands},
  series	= {Middleware '20}
}

@InProceedings{	  qiu2020clara,
  title		= {Clara: Performance clarity for smartnic offloading},
  author	= {Qiu, Yiming and Kang, Qiao and Liu, Ming and Chen, Ang},
  booktitle	= {Proceedings of the 19th ACM Workshop on Hot Topics in
		  Networks},
  pages		= {16--22},
  year		= {2020}
}

@InProceedings{	  quevedo_evaluating_2019,
  title		= {Evaluating {Apache} {OpenWhisk} - {FaaS}},
  doi		= {10.1109/ETCM48019.2019.9014867},
  abstract	= {Function-as-a-Service (FaaS) platforms enable users to
		  execute user-defined functions without worrying about
		  operational issues such as the management of infrastructure
		  resources. In order to improve performance, different FaaS
		  platforms are implementing optimizations and improvements,
		  but it's not clear how good these implementations are. In
		  this work, Apache OpenWhisk platform is evaluated from an
		  approach that allows to determinate and characterize the
		  performance under different configuration options; it was
		  found that under certain premises an improvement of the
		  performance in cold-booting latencies up to 38\% is
		  obtain.},
  booktitle	= {2019 {IEEE} {Fourth} {Ecuador} {Technical} {Chapters}
		  {Meeting} ({ETCM})},
  author	= {Quevedo, Sebastián and Merchán, Freddy and Rivadeneira,
		  Rafael and Dominguez, Federico X.},
  month		= nov,
  year		= {2019},
  keywords	= {Cloud computing, Containers, FAA, IEEE Sections, Java,
		  Optimization, Programming},
  pages		= {1--5},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/IWUJHRW2/stamp.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/K5RJ8H24/Quevedo et al. -
		  2019 - Evaluating Apache OpenWhisk -
		  FaaS.pdf:application/pdf}
}

@Article{	  raghavendra_no_2008,
  title		= {No “{Power}” {Struggles}: {Coordinated} {Multi}-level
		  {Power} {Management} for the {Data} {Center}},
  abstract	= {Power delivery, electricity consumption, and heat
		  management are becoming key challenges in data center
		  environments. Several past solutions have individually
		  evaluated different techniques to address separate aspects
		  of this problem, in hardware and software, and at local and
		  global levels. Unfortunately, there has been no
		  corresponding work on coordinating all these solutions. In
		  the absence of such coordination, these solutions are
		  likely to interfere with one another, in unpredictable (and
		  potentially dangerous) ways. This paper seeks to address
		  this problem. We make two key contributions. First, we
		  propose and validate a power management solution that
		  coordinates different individual approaches. Using
		  simulations based on 180 server traces from nine different
		  real-world enterprises, we demonstrate the correctness,
		  stability, and efficiency advantages of our solution.
		  Second, using our unified architecture as the base, we
		  perform a detailed quantitative sensitivity analysis and
		  draw conclusions about the impact of different
		  architectures, implementations, workloads, and system
		  design choices.},
  language	= {en},
  journal	= {ASPLOS},
  author	= {Raghavendra, Ramya and Ranganathan, Parthasarathy and
		  Talwar, Vanish and Wang, Zhikui and Zhu, Xiaoyun},
  year		= {2008},
  pages		= {12},
  file		= {Raghavendra et al. - No “Power” Struggles Coordinated
		  Multi-level
		  Powe.pdf:/home/prateeks/Zotero/storage/3W8H5W2F/Raghavendra
		  et al. - No “Power” Struggles Coordinated Multi-level
		  Powe.pdf:application/pdf}
}

@Article{	  raza2021sok,
  title		= {SoK: Function-As-A-Service: From An Application
		  Developer’s Perspective},
  author	= {Raza, Ali and Matta, Ibrahim and Akhtar, Nabeel and
		  Kalavri, Vasiliki and Isahagian, Vatche},
  journal	= {Journal of Systems Research},
  volume	= {1},
  number	= {1},
  year		= {2021}
}

@InProceedings{	  reddy_empirical_2017,
  title		= {Empirical {CPU} power modelling and estimation in the gem5
		  simulator},
  doi		= {10.1109/PATMOS.2017.8106988},
  abstract	= {Power modelling is important for modern CPUs to inform
		  power management approaches and allow design space
		  exploration. Power simulators, combined with a full-system
		  architectural simulator such as gem5, enable
		  power-performance trade-offs to be investigated early in
		  the design of a system with different configurations (e.g
		  number of cores, cache size, etc.). However, the accuracy
		  of existing power simulators, such as McPAT, is known to be
		  low due to the abstraction and specification errors, and
		  this can lead to incorrect research conclusions. In this
		  paper, we present an accurate power model, built from
		  measured data, integrated into gem5 for estimating the
		  power consumption of a simulated quad-core ARM Cortex-A15.
		  A power modelling methodology based on Performance
		  Monitoring Counters (PMCs) is used to build and evaluate
		  the integrated model in gem5. We first validate this
		  methodology on the real hardware with 60 workloads at nine
		  Dynamic Voltage and Frequency Scaling (DVFS) levels and
		  four core mappings (2,160 samples), showing an average
		  error between estimated and real measured power of less
		  than 6\%. Correlation between gem5 activity statistics and
		  hardware PMCs is investigated to build a gem5 model
		  representing a quad-core ARM Cortex-A15. Experimental
		  validation with 15 workloads at four DVFS levels on real
		  hardware and gem5 has been conducted to understand how the
		  difference between the gem5 simulated activity statistics
		  and the hardware PMCs affects the estimated power
		  consumption.},
  booktitle	= {2017 27th {International} {Symposium} on {Power} and
		  {Timing} {Modeling}, {Optimization} and {Simulation}
		  ({PATMOS})},
  author	= {Reddy, Basireddy Karunakar and Walker, Matthew J. and
		  Balsamo, Domenico and Diestelhorst, Stephan and Al-Hashimi,
		  Bashir M. and Merrett, Geoff V.},
  month		= sep,
  year		= {2017},
  keywords	= {Data models, Hardware, Microarchitecture, Power demand,
		  Power measurement, Timing, Tools},
  pages		= {1--8},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/4SU5EUSM/8106988.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/CUIMH2JS/Reddy et al. -
		  2017 - Empirical CPU power modelling and estimation in
		  th.pdf:application/pdf}
}

@InCollection{	  riis_nielson_no_2019,
  address	= {Cham},
  title		= {No {More}, {No} {Less}: {A} {Formal} {Model} for
		  {Serverless} {Computing}},
  volume	= {11533},
  isbn		= {978-3-030-22396-0 978-3-030-22397-7},
  shorttitle	= {No {More}, {No} {Less}},
  url		= {http://link.springer.com/10.1007/978-3-030-22397-7_9},
  abstract	= {Serverless computing, also known as
		  Functions-as-a-Service, is a recent paradigm aimed at
		  simplifying the programming of cloud applications. The idea
		  is that developers design applications in terms of
		  functions, which are then deployed on a cloud
		  infrastructure. The infrastructure takes care of executing
		  the functions whenever requested by remote clients, dealing
		  automatically with distribution and scaling with respect to
		  inbound traﬃc.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Coordination {Models} and {Languages}},
  publisher	= {Springer International Publishing},
  author	= {Gabbrielli, Maurizio and Giallorenzo, Saverio and Lanese,
		  Ivan and Montesi, Fabrizio and Peressotti, Marco and
		  Zingaro, Stefano Pio},
  editor	= {Riis Nielson, Hanne and Tuosto, Emilio},
  year		= {2019},
  doi		= {10.1007/978-3-030-22397-7_9},
  pages		= {148--157},
  file		= {Gabbrielli et al. - 2019 - No More, No Less A Formal Model
		  for Serverless
		  Co.pdf:/home/prateeks/Zotero/storage/2WD849R9/Gabbrielli et
		  al. - 2019 - No More, No Less A Formal Model for Serverless
		  Co.pdf:application/pdf}
}

###InCollection{  riis_nielson_no_2019,
  address	= {Cham},
  title		= {No {More}, {No} {Less}: {A} {Formal} {Model} for
		  {Serverless} {Computing}},
  volume	= {11533},
  isbn		= {978-3-030-22396-0 978-3-030-22397-7},
  shorttitle	= {No {More}, {No} {Less}},
  url		= {http://link.springer.com/10.1007/978-3-030-22397-7_9},
  abstract	= {Serverless computing, also known as
		  Functions-as-a-Service, is a recent paradigm aimed at
		  simplifying the programming of cloud applications. The idea
		  is that developers design applications in terms of
		  functions, which are then deployed on a cloud
		  infrastructure. The infrastructure takes care of executing
		  the functions whenever requested by remote clients, dealing
		  automatically with distribution and scaling with respect to
		  inbound traﬃc.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Coordination {Models} and {Languages}},
  publisher	= {Springer International Publishing},
  author	= {Gabbrielli, Maurizio and Giallorenzo, Saverio and Lanese,
		  Ivan and Montesi, Fabrizio and Peressotti, Marco and
		  Zingaro, Stefano Pio},
  editor	= {Riis Nielson, Hanne and Tuosto, Emilio},
  year		= {2019},
  doi		= {10.1007/978-3-030-22397-7_9},
  pages		= {148--157},
  file		= {Gabbrielli et al. - 2019 - No More, No Less A Formal Model
		  for Serverless
		  Co.pdf:/home/prateeks/Zotero/storage/2WD849R9/Gabbrielli et
		  al. - 2019 - No More, No Less A Formal Model for Serverless
		  Co.pdf:application/pdf}
}

@InProceedings{	  ristov_colder_warmer,
  author	= {Ristov, Sashko and Hollaus, Christian and Hautz, Mika},
  title		= {Colder Than the Warm Start and Warmer Than the Cold Start!
		  Experience the Spawn Start in FaaS Providers},
  year		= {2022},
  isbn		= {9781450392808},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3524053.3542751},
  doi		= {10.1145/3524053.3542751},
  abstract	= {Many researchers reported considerable delay of up to a
		  few seconds when invoking serverless functions for the
		  first time. This phenomenon, which is known as a cold
		  start, affects even more when users are running multiple
		  serverless functions orchestrated in a workflow. However,
		  in many cases users need to instantly spawn numerous
		  serverless functions, usually as a part of parallel loops.
		  In this paper, we introduce the spawn start and analyze the
		  behavior of three Function-as-a-Service (FaaS) providers
		  AWS Lambda, Google Cloud Functions, and IBM Cloud Functions
		  when running parallel loops, both as warm and cold starts.
		  We conducted a series of experiments and observed three
		  insights that are beneficial for the research community.
		  Firstly, cold start on IBM Cloud Functions, which is up to
		  2s delay compared to the warm start, is negligible compared
		  to the spawn start because the latter generates additional
		  20s delay. Secondly, Google Cloud Functions' cold start is
		  "warmer" than the warm start of the same serverless
		  function. Finally, while Google Cloud Functions and IBM
		  Cloud Functions run the same serverless function with low
		  concurrency faster than AWS Lambda, the spawn start effect
		  on Google Cloud Functions and IBM Cloud Functions makes AWS
		  the preferred provider when spawning numerous serverless
		  functions.},
  booktitle	= {Proceedings of the 2022 Workshop on Advanced Tools,
		  Programming Languages, and PLatforms for Implementing and
		  Evaluating Algorithms for Distributed Systems},
  pages		= {35–39},
  numpages	= {5},
  keywords	= {cold start, overhead, serverless, performance, spawn
		  start, function-as-a-service},
  location	= {Salerno, Italy},
  series	= {ApPLIED '22}
}

@Article{	  romero2021faa,
  title		= {Faa\$T: A Transparent Auto-Scaling Cache for Serverless
		  Applications},
  author	= {Romero, Francisco and Chaudhry, Gohar Irfan and Goiri,
		  {\'I}{\~n}igo and Gopa, Pragna and Batum, Paul and
		  Yadwadkar, Neeraja J and Fonseca, Rodrigo and Kozyrakis,
		  Christos and Bianchini, Ricardo},
  journal	= {arXiv preprint arXiv:2104.13869},
  year		= {2021}
}

@Article{	  rotem_power-management_2012,
  title		= {Power-{Management} {Architecture} of the {Intel}
		  {Microarchitecture} {Code}-{Named} {Sandy} {Bridge}},
  volume	= {32},
  issn		= {0272-1732},
  url		= {http://ieeexplore.ieee.org/document/6148200/},
  doi		= {10.1109/MM.2012.12},
  language	= {en},
  number	= {2},
  urldate	= {2022-07-22},
  journal	= {IEEE Micro},
  author	= {Rotem, Efraim and Naveh, Alon and Ananthakrishnan, Avinash
		  and Weissmann, Eliezer and Rajwan, Doron},
  month		= mar,
  year		= {2012},
  pages		= {20--27},
  file		= {Rotem et al. - 2012 - Power-Management Architecture of the
		  Intel
		  Microar.pdf:/home/prateeks/Zotero/storage/6EA2KTN5/Rotem et
		  al. - 2012 - Power-Management Architecture of the Intel
		  Microar.pdf:application/pdf}
}
@inproceedings{luo2021characterizing,
  title={Characterizing microservice dependency and performance: Alibaba trace analysis},
  author={Luo, Shutian and Xu, Huanle and Lu, Chengzhi and Ye, Kejiang and Xu, Guoyao and Zhang, Liping and Ding, Yu and He, Jian and Xu, Chengzhong},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={412--426},
  year={2021}
}

@InProceedings{	  roy2022icebreaker,
  title		= {Icebreaker: Warming serverless functions better with
		  heterogeneity},
  author	= {Roy, Rohan Basu and Patel, Tirthak and Tiwari, Devesh},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {753--767},
  year		= {2022}
}

###InProceedings{ roy2022icebreaker,
  title		= {IceBreaker: warming serverless functions better with
		  heterogeneity},
  author	= {Roy, Rohan Basu and Patel, Tirthak and Tiwari, Devesh},
  booktitle	= {Proceedings of the 27th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {753--767},
  year		= {2022}
}

@InProceedings{	  roy2022mashup,
  title		= {Mashup: making serverless computing useful for hpc
		  workflows via hybrid execution},
  author	= {Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay
		  and Tiwari, Devesh},
  booktitle	= {Proceedings of the 27th ACM SIGPLAN Symposium on
		  Principles and Practice of Parallel Programming},
  pages		= {46--60},
  year		= {2022}
}

@Article{	  saltzer1984end,
  title		= {End-to-end arguments in system design},
  author	= {Saltzer, Jerome H and Reed, David P and Clark, David D},
  journal	= {ACM Transactions on Computer Systems (TOCS)},
  volume	= {2},
  number	= {4},
  pages		= {277--288},
  year		= {1984},
  publisher	= {Acm New York, NY, USA}
}

@InProceedings{	  satzke2020efficient,
  title		= {Efficient GPU Sharing for Serverless Workflows},
  author	= {Satzke, Klaus and Akkus, Istemi Ekin and Chen, Ruichuan
		  and Rimac, Ivica and Stein, Manuel and Beck, Andre and
		  Aditya, Paarijaat and Vanga, Manohar and Hilt, Volker},
  booktitle	= {Proceedings of the 1st Workshop on High Performance
		  Serverless Computing},
  pages		= {17--24},
  year		= {2020}
}

@Misc{		  scheuner_lets_2022,
  title		= {Let's {Trace} {It}: {Fine}-{Grained} {Serverless}
		  {Benchmarking} using {Synchronous} and {Asynchronous}
		  {Orchestrated} {Applications}},
  shorttitle	= {Let's {Trace} {It}},
  url		= {http://arxiv.org/abs/2205.07696},
  abstract	= {Making serverless computing widely applicable requires
		  detailed performance understanding. Although contemporary
		  benchmarking approaches exist, they report only coarse
		  results, do not apply distributed tracing, do not consider
		  asynchronous applications, and provide limited capabilities
		  for (root cause) analysis. Addressing this gap, we design
		  and implement ServiBench, a serverless benchmarking suite.
		  ServiBench (i) leverages synchronous and asynchronous
		  serverless applications representative of production usage,
		  (ii) extrapolates cloud-provider data to generate realistic
		  workloads, (iii) conducts comprehensive, end-to-end
		  experiments to capture application-level performance, (iv)
		  analyzes results using a novel approach based on
		  (distributed) serverless tracing, and (v) supports
		  comprehensively serverless performance analysis. With
		  ServiBench, we conduct comprehensive experiments on AWS,
		  covering ﬁve common performance factors: median latency,
		  cold starts, tail latency, scalability, and dynamic
		  workloads. We ﬁnd that the median end-to-end latency of
		  serverless applications is often dominated not by function
		  computation but by external service calls, orchestration,
		  or trigger-based coordination. We release collected
		  experimental data under FAIR principles and ServiBench as a
		  tested, extensible open-source tool.},
  language	= {en},
  urldate	= {2023-01-09},
  publisher	= {arXiv},
  author	= {Scheuner, Joel and Eismann, Simon and Talluri, Sacheendra
		  and van Eyk, Erwin and Abad, Cristina and Leitner, Philipp
		  and Iosup, Alexandru},
  month		= may,
  year		= {2022},
  note		= {arXiv:2205.07696 [cs]},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Software Engineering},
  file		= {Scheuner et al. - 2022 - Let's Trace It Fine-Grained
		  Serverless
		  Benchmarki.pdf:/home/prateeks/Zotero/storage/PAGCS2SP/Scheuner
		  et al. - 2022 - Let's Trace It Fine-Grained Serverless
		  Benchmarki.pdf:application/pdf}
}

@Article{	  schwartz_green_2020,
  title		= {Green {AI}},
  volume	= {63},
  issn		= {0001-0782, 1557-7317},
  url		= {https://dl.acm.org/doi/10.1145/3381831},
  doi		= {10.1145/3381831},
  abstract	= {Creating efficiency in AI research will decrease its
		  carbon footprint and increase its inclusivity as deep
		  learning study should not require the deepest pockets.},
  language	= {en},
  number	= {12},
  urldate	= {2022-09-30},
  journal	= {Communications of the ACM},
  author	= {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and
		  Etzioni, Oren},
  month		= nov,
  year		= {2020},
  pages		= {54--63},
  file		= {Schwartz et al. - 2020 - Green
		  AI.pdf:/home/prateeks/Zotero/storage/49FLG5XV/Schwartz et
		  al. - 2020 - Green AI.pdf:application/pdf}
}

@Article{	  scikit-learn,
  title		= {Scikit-learn: Machine Learning in {P}ython},
  author	= {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and
		  Michel, V. and Thirion, B. and Grisel, O. and Blondel, M.
		  and Prettenhofer, P. and Weiss, R. and Dubourg, V. and
		  Vanderplas, J. and Passos, A. and Cournapeau, D. and
		  Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal	= {Journal of Machine Learning Research},
  volume	= {12},
  pages		= {2825--2830},
  year		= {2011}
}

@Article{	  scully2022bounding,
  title		= {Bounding Mean Slowdown in Multiserver Systems},
  author	= {Scully, Ziv},
  journal	= {ACM SIGMETRICS Performance Evaluation Review},
  volume	= {49},
  number	= {2},
  pages		= {36--38},
  year		= {2022},
  publisher	= {ACM New York, NY, USA}
}

@Article{	  sen_pareto_2017,
  title		= {Pareto {Governors} for {Energy}-{Optimal} {Computing}},
  volume	= {14},
  issn		= {1544-3566, 1544-3973},
  url		= {https://dl.acm.org/doi/10.1145/3046682},
  doi		= {10.1145/3046682},
  abstract	= {The original definition of energy-proportional computing
		  does not characterize the energy efficiency of recent
		  reconfigurable computers, resulting in nonintuitive
		  “super-proportional” behavior. This article introduces
		  a new definition of ideal energy-proportional computing,
		  new metrics to quantify computational energy waste, and new
		  SLA-aware OS governors that seek Pareto optimality to
		  achieve power-efficient performance.},
  language	= {en},
  number	= {1},
  urldate	= {2022-04-21},
  journal	= {ACM Transactions on Architecture and Code Optimization},
  author	= {Sen, Rathijit and Wood, David A.},
  month		= apr,
  year		= {2017},
  pages		= {1--25},
  file		= {Sen and Wood - 2017 - Pareto Governors for Energy-Optimal
		  Computing.pdf:/home/prateeks/Zotero/storage/364HJ3VM/Sen
		  and Wood - 2017 - Pareto Governors for Energy-Optimal
		  Computing.pdf:application/pdf}
}

@InProceedings{	  seo2007energy,
  title		= {An energy consumption framework for distributed java-based
		  systems},
  author	= {Seo, Chiyoung and Malek, Sam and Medvidovic, Nenad},
  booktitle	= {Proceedings of the twenty-second IEEE/ACM international
		  conference on Automated software engineering},
  pages		= {421--424},
  year		= {2007}
}

@Article{	  serverless-cacm-21,
  author	= {Schleier-Smith, Johann and Sreekanti, Vikram and
		  Khandelwal, Anurag and Carreira, Joao and Yadwadkar,
		  Neeraja J. and Popa, Raluca Ada and Gonzalez, Joseph E. and
		  Stoica, Ion and Patterson, David A.},
  title		= {What Serverless Computing is and Should Become: The next
		  Phase of Cloud Computing},
  year		= {2021},
  issue_date	= {May 2021},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {64},
  number	= {5},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3406011},
  doi		= {10.1145/3406011},
  abstract	= {The evolution that serverless computing represents, the
		  economic forces that shape it, why it could fail, and how
		  it might fulfill its potential.},
  journal	= {Commun. ACM},
  month		= apr,
  pages		= {76–84},
  numpages	= {9}
}

@InProceedings{	  serverless-harvest-sosp21,
  author	= {Zhang, Yanqi and Goiri, \'{I}\~{n}igo and Chaudhry, Gohar
		  Irfan and Fonseca, Rodrigo and Elnikety, Sameh and
		  Delimitrou, Christina and Bianchini, Ricardo},
  title		= {Faster and Cheaper Serverless Computing on Harvested
		  Resources},
  year		= {2021},
  isbn		= {9781450387095},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3477132.3483580},
  doi		= {10.1145/3477132.3483580},
  abstract	= {Serverless computing is becoming increasingly popular due
		  to its ease of programming, fast elasticity, and
		  fine-grained billing. However, the serverless provider
		  still needs to provision, manage, and pay the IaaS provider
		  for the virtual machines (VMs) hosting its platform. This
		  ties the cost of the serverless platform to the cost of the
		  underlying VMs. One way to significantly reduce cost is to
		  use spare resources, which cloud providers rent at a
		  massive discount. Harvest VMs offer such cheap resources:
		  they grow and shrink to harvest all the unallocated CPU
		  cores in their host servers, but may be evicted to make
		  room for more expensive VMs. Thus, using Harvest VMs to run
		  the serverless platform comes with two main challenges that
		  must be carefully managed: VM evictions and dynamically
		  varying resources in each VM.In this work, we explore the
		  challenges and benefits of hosting serverless (Function as
		  a Service or simply FaaS) platforms on Harvest VMs. We
		  characterize the serverless workloads and Harvest VMs of
		  Microsoft Azure, and design a serverless load balancer that
		  is aware of evictions and resource variations in Harvest
		  VMs. We modify OpenWhisk, a widely-used open-source
		  serverless platform, to monitor harvested resources and
		  balance the load accordingly, and evaluate it
		  experimentally. Our results show that adopting harvested
		  resources improves efficiency and reduces cost. Under the
		  same cost budget, running serverless platforms on harvested
		  resources achieves 2.2x to 9.0x higher throughput compared
		  to using dedicated resources. When using the same amount of
		  resources, running serverless platforms on harvested
		  resources achieves 48% to 89% cost savings with lower
		  latency due to better load balancing.},
  booktitle	= {Proceedings of the ACM SIGOPS 28th Symposium on Operating
		  Systems Principles},
  pages		= {724–739},
  numpages	= {16},
  keywords	= {Serverless computing, harvested resources},
  location	= {Virtual Event, Germany},
  series	= {SOSP '21}
}

@Article{	  serverless-survey-21,
  author	= {Eismann, Simon and Scheuner, Joel and van Eyk, Erwin and
		  Schwinger, Maximilian and Grohmann, Johannes and Herbst,
		  Nikolas and Abad, Cristina L. and Iosup, Alexandru},
  journal	= {IEEE Software},
  title		= {Serverless Applications: Why, When, and How?},
  year		= {2021},
  volume	= {38},
  number	= {1},
  pages		= {32--39},
  doi		= {10.1109/MS.2020.3023302}
}

@Article{	  serverless_cacm_19,
  author	= {Castro, Paul and Ishakian, Vatche and Muthusamy, Vinod and
		  Slominski, Aleksander},
  title		= {The Rise of Serverless Computing},
  year		= {2019},
  issue_date	= {December 2019},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  volume	= {62},
  number	= {12},
  issn		= {0001-0782},
  url		= {https://doi.org/10.1145/3368454},
  doi		= {10.1145/3368454},
  journal	= {Commun. ACM},
  month		= nov,
  pages		= {44–54},
  numpages	= {11}
}

@Book{		  sewak2019deep,
  title		= {Deep reinforcement learning},
  author	= {Sewak, Mohit},
  year		= {2019},
  publisher	= {Springer}
}

@InProceedings{	  shahrad_architectural_2019,
  address	= {Columbus OH USA},
  title		= {Architectural {Implications} of {Function}-as-a-{Service}
		  {Computing}},
  isbn		= {978-1-4503-6938-1},
  url		= {https://dl.acm.org/doi/10.1145/3352460.3358296},
  doi		= {10.1145/3352460.3358296},
  abstract	= {Serverless computing is a rapidly growing cloud
		  application model, popularized by Amazon’s Lambda
		  platform. Serverless cloud services provide fine-grained
		  provisioning of resources, which scale automatically with
		  user demand. Function-as-a-Service (FaaS) applications
		  follow this serverless model, with the developer providing
		  their application as a set of functions which are executed
		  in response to a user- or system-generated event. Functions
		  are designed to be short-lived and execute inside
		  containers or virtual machines, introducing a range of
		  system-level overheads. This paper studies the
		  architectural implications of this emerging paradigm. Using
		  the commercial-grade Apache OpenWhisk FaaS platform on real
		  servers, this work investigates and identifies the
		  architectural implications of FaaS serverless computing.
		  The workloads, along with the way that FaaS inherently
		  interleaves short functions from many tenants frustrates
		  many of the locality-preserving architectural structures
		  common in modern processors. In particular, we find that:
		  FaaS containerization brings up to 20x slowdown compared to
		  native execution, cold-start can be over 10x a short
		  function’s execution time, branch mispredictions per
		  kilo-instruction are 20x higher for short functions, memory
		  bandwidth increases by 6x due to the invocation pattern,
		  and IPC decreases by as much as 35\% due to inter-function
		  interference. We open-source FaaSProfiler, the FaaS testing
		  and profiling platform that we developed for this work.},
  language	= {en},
  urldate	= {2023-01-09},
  booktitle	= {Proceedings of the 52nd {Annual} {IEEE}/{ACM}
		  {International} {Symposium} on {Microarchitecture}},
  publisher	= {ACM},
  author	= {Shahrad, Mohammad and Balkind, Jonathan and Wentzlaff,
		  David},
  month		= oct,
  year		= {2019},
  pages		= {1063--1075},
  file		= {Shahrad et al. - 2019 - Architectural Implications of
		  Function-as-a-Servic.pdf:/home/prateeks/Zotero/storage/2DMEKWR7/Shahrad
		  et al. - 2019 - Architectural Implications of
		  Function-as-a-Servic.pdf:application/pdf}
}

@inproceedings{shahrad2020serverless,
  title={Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider},
  author={Shahrad, Mohammad and Fonseca, Rodrigo and Goiri, Inigo and Chaudhry, Gohar and Batum, Paul and Cooke, Jason and Laureano, Eduardo and Tresness, Colby and Russinovich, Mark and Bianchini, Ricardo},
  booktitle={2020 USENIX annual technical conference (USENIX ATC 20)},
  pages={205--218},
  year={2020}
}

@Article{	  shankar2018numpywren,
  title		= {Numpywren: Serverless linear algebra},
  author	= {Shankar, Vaishaal and Krauth, Karl and Pu, Qifan and
		  Jonas, Eric and Venkataraman, Shivaram and Stoica, Ion and
		  Recht, Benjamin and Ragan-Kelley, Jonathan},
  journal	= {arXiv preprint arXiv:1810.09679},
  year		= {2018}
}

@Misc{		  shao_your_2020,
  title		= {Your {Noise}, {My} {Signal}: {Exploiting} {Switching}
		  {Noise} for {Stealthy} {Data} {Exfiltration} from {Desktop}
		  {Computers}},
  shorttitle	= {Your {Noise}, {My} {Signal}},
  url		= {http://arxiv.org/abs/2001.06729},
  abstract	= {Attacks based on power analysis have been long existing
		  and studied, with some recent works focused on data
		  exfiltration from victim systems without using conventional
		  communications (e.g., WiFi). Nonetheless, prior works
		  typically rely on intrusive direct power measurement,
		  either by implanting meters in the power outlet or tapping
		  into the power cable, thus jeopardizing the stealthiness of
		  attacks. In this paper, we propose NoDE (Noise for Data
		  Exfiltration), a new system for stealthy data exfiltration
		  from enterprise desktop computers. Specifically, NoDE
		  achieves data exfiltration over a building’s power
		  network by exploiting high-frequency voltage ripples (i.e.,
		  switching noises) generated by power factor correction
		  circuits built into today’s computers. Located at a
		  distance and even from a different room, the receiver can
		  non-intrusively measure the voltage of a power outlet to
		  capture the high-frequency switching noises for online
		  information decoding without supervised training/learning.
		  To evaluate NoDE, we run experiments on seven different
		  computers from top vendors and using top-brand power supply
		  units. Our results show that for a single transmitter, NoDE
		  achieves a rate of up to 28.48 bits/second with a distance
		  of 90 feet (27.4 meters) without the line of sight,
		  demonstrating a practically stealthy threat. Based on the
		  orthogonality of switching noise frequencies of different
		  computers, we also demonstrate simultaneous data
		  exfiltration from four computers using only one receiver.
		  Finally, we present a few possible defenses, such as
		  installing noise filters, and discuss their limitations.},
  language	= {en},
  urldate	= {2022-09-28},
  publisher	= {arXiv},
  author	= {Shao, Zhihui and Islam, Mohammad A. and Ren, Shaolei},
  month		= jan,
  year		= {2020},
  note		= {arXiv:2001.06729 [cs]},
  keywords	= {Computer Science - Cryptography and Security},
  annote	= {Comment: 23 pages, 40 figures, to be presented at ACM
		  SIGMETRICS, June 08{\textasciitilde}12, 2020, Boston, MA},
  file		= {Shao et al. - 2020 - Your Noise, My Signal Exploiting
		  Switching Noise
		  .pdf:/home/prateeks/Zotero/storage/XVBJPWM3/Shao et al. -
		  2020 - Your Noise, My Signal Exploiting Switching Noise
		  .pdf:application/pdf}
}

@InProceedings{	  shards,
  title		= {Efficient {MRC} Construction with {SHARDS}},
  author	= {Waldspurger, Carl A and Park, Nohhyun and Garthwaite,
		  Alexander and Ahmad, Irfan},
  booktitle	= {13th {USENIX} Conference on File and Storage Technologies
		  ({FAST} 15)},
  pages		= {95--110},
  year		= {2015}
}

@InProceedings{	  sharma2011blink,
  title		= {Blink: managing server clusters on intermittent power},
  author	= {Sharma, Navin and Barker, Sean and Irwin, David and
		  Shenoy, Prashant},
  booktitle	= {Proceedings of the sixteenth international conference on
		  Architectural support for programming languages and
		  operating systems},
  pages		= {185--198},
  year		= {2011}
}

@InProceedings{	  sharma2012singleton,
  title		= {Singleton: system-wide page deduplication in virtual
		  environments},
  author	= {Sharma, Prateek and Kulkarni, Purushottam},
  booktitle	= {Proceedings of the 21st international symposium on
		  High-Performance Parallel and Distributed Computing},
  pages		= {15--26},
  year		= {2012}
}

###InProceedings{ sharma2012singleton,
  title		= {Singleton: system-wide page deduplication in virtual
		  environments},
  author	= {Sharma, Prateek and Kulkarni, Purushottam},
  booktitle	= {Proceedings of the 21st international symposium on
		  High-Performance Parallel and Distributed Computing},
  pages		= {15--26},
  year		= {2012}
}

@Article{	  sharma2017design,
  title		= {Design and operational analysis of a green data center},
  author	= {Sharma, Prateek and Pegus II, Patrick and Irwin, David and
		  Shenoy, Prashant and Goodhue, John and Culbert, James},
  journal	= {IEEE Internet Computing},
  volume	= {21},
  number	= {4},
  pages		= {16--24},
  year		= {2017},
  publisher	= {IEEE}
}

@InProceedings{	  shen2021defuse,
  title		= {Defuse: A Dependency-Guided Function Scheduler to Mitigate
		  Cold Starts on FaaS Platforms},
  author	= {Shen, Jiacheng and Yang, Tianyi and Su, Yuxin and Zhou,
		  Yangfan and Lyu, Michael R},
  booktitle	= {2021 IEEE 41st International Conference on Distributed
		  Computing Systems (ICDCS)},
  pages		= {194--204},
  year		= {2021},
  organization	= {IEEE}
}

@InProceedings{	  shen_defuse_2021,
  title		= {Defuse: {A} {Dependency}-{Guided} {Function} {Scheduler}
		  to {Mitigate} {Cold} {Starts} on {FaaS} {Platforms}},
  shorttitle	= {Defuse},
  doi		= {10.1109/ICDCS51616.2021.00027},
  abstract	= {Function-as-a-Service (FaaS) is becoming a prevalent
		  paradigm in developing cloud applications. With FaaS,
		  clients can develop applications as serverless functions,
		  leaving the burden of resource management to cloud
		  providers. However, FaaS platforms suffer from the
		  performance degradation caused by the cold starts of
		  serverless functions. Cold starts happen when serverless
		  functions are invoked before they have been loaded into the
		  memory. The problem is unavoidable because the memory in
		  datacenters is typically too limited to hold all serverless
		  functions simultaneously. The latency of cold function
		  invocations will greatly degenerate the performance of FaaS
		  platforms. Currently, FaaS platforms employ various
		  scheduling methods to reduce the occurrences of cold
		  starts. However, they do not consider the ubiquitous
		  dependencies between serverless functions. Observing the
		  potential of using dependencies to mitigate cold starts, we
		  propose Defuse, a Dependency-guided Function Scheduler on
		  FaaS platforms. Specifically, Defuse identifies two types
		  of dependencies between serverless functions, i.e., strong
		  dependencies and weak ones. It uses frequent pattern mining
		  and positive point-wise mutual information to mine such
		  dependencies respectively from function invocation
		  histories. In this way, Defuse constructs a function
		  dependency graph. The connected components (i.e., dependent
		  functions) on the graph can be scheduled to diminish the
		  occurrences of cold starts. We evaluate the effectiveness
		  of Defuse by applying it to an industrial serverless
		  dataset. The experimental results show that Defuse can
		  reduce 22\% of memory usage while having a 35\% decrease in
		  function cold-start rates compared with the
		  state-of-the-art method.},
  booktitle	= {2021 {IEEE} 41st {International} {Conference} on
		  {Distributed} {Computing} {Systems} ({ICDCS})},
  author	= {Shen, Jiacheng and Yang, Tianyi and Su, Yuxin and Zhou,
		  Yangfan and Lyu, Michael R.},
  month		= jul,
  year		= {2021},
  note		= {ISSN: 2575-8411},
  keywords	= {Cloud Computing, Cold Start, FAA, FaaS, Job shop
		  scheduling, Memory management, Processor scheduling,
		  Resource management, Schedules, Serverless, Service
		  Dependency, System improvement},
  pages		= {194--204},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/PD6AUEGI/stamp.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/GXFFUGLA/Shen et al. -
		  2021 - Defuse A Dependency-Guided Function Scheduler to
		  .pdf:application/pdf}
}

@Article{	  shen_power_2013,
  title		= {Power containers: an {OS} facility for fine-grained power
		  and energy management on multicore servers},
  language	= {en},
  journal	= {ASPLOS},
  author	= {Shen, Kai and Shriraman, Arrvindh and Dwarkadas, Sandhya
		  and Zhang, Xiao and Chen, Zhuan},
  year		= {2013},
  pages		= {12},
  annote	= {- Power containers. Coarse-grained offline model, updated
		  frequently. Interesting tricks for request tracing using
		  OS/TCP support. Not applicable for modern Go/js apps? },
  file		= {Shen et al. - Power containers an OS facility for
		  fine-grained
		  .pdf:/home/prateeks/Zotero/storage/Y3Q67MFS/Shen et al. -
		  Power containers an OS facility for fine-grained
		  .pdf:application/pdf}
}

@InProceedings{	  shillaker2020faasm,
  title		= {Faasm: Lightweight isolation for efficient stateful
		  serverless computing},
  author	= {Shillaker, Simon and Pietzuch, Peter},
  booktitle	= {2020 USENIX Annual Technical Conference (USENIX$\{$ATC$\}$
		  20)},
  pages		= {419--433},
  year		= {2020}
}

###InProceedings{ shillaker2020faasm,
  title		= {Faasm: Lightweight isolation for efficient stateful
		  serverless computing},
  author	= {Shillaker, Simon and Pietzuch, Peter},
  booktitle	= {2020 USENIX Annual Technical Conference (USENIX ATC 20)},
  pages		= {419--433},
  year		= {2020}
}

@Article{	  siddik_2021,
  doi		= {10.1088/1748-9326/abfba1},
  url		= {https://doi.org/10.1088/1748-9326/abfba1},
  year		= 2021,
  month		= {may},
  publisher	= {{IOP} Publishing},
  volume	= {16},
  number	= {6},
  pages		= {064017},
  author	= {Md Abu Bakar Siddik and Arman Shehabi and Landon Marston},
  title		= {The environmental footprint of data centers in the United
		  States},
  journal	= {Environmental Research Letters},
  abstract	= {Much of the world’s data are stored, managed, and
		  distributed by data centers. Data centers require a
		  tremendous amount of energy to operate, accounting for
		  around 1.8}
}

@InProceedings{	  silva2020prebaking,
  title		= {Prebaking Functions to Warm the Serverless Cold Start},
  author	= {Silva, Paulo and Fireman, Daniel and Pereira, Thiago
		  Emmanuel},
  booktitle	= {Proceedings of the 21st International Middleware
		  Conference},
  pages		= {1--13},
  year		= {2020}
}

@InProceedings{	  silva_prebaking_2020,
  address	= {Delft Netherlands},
  title		= {Prebaking {Functions} to {Warm} the {Serverless} {Cold}
		  {Start}},
  isbn		= {978-1-4503-8153-6},
  url		= {https://dl.acm.org/doi/10.1145/3423211.3425682},
  doi		= {10.1145/3423211.3425682},
  language	= {en},
  urldate	= {2023-01-20},
  booktitle	= {Proceedings of the 21st {International} {Middleware}
		  {Conference}},
  publisher	= {ACM},
  author	= {Silva, Paulo and Fireman, Daniel and Pereira, Thiago
		  Emmanuel},
  month		= dec,
  year		= {2020},
  pages		= {1--13},
  file		= {Silva et al. - 2020 - Prebaking Functions to Warm the
		  Serverless Cold
		  St.pdf:/home/prateeks/Zotero/storage/D2LX4SJZ/Silva et al.
		  - 2020 - Prebaking Functions to Warm the Serverless Cold
		  St.pdf:application/pdf}
}

@InProceedings{	  singhvi2021atoll,
  title		= {Atoll: A scalable low-latency serverless platform},
  author	= {Singhvi, Arjun and Balasubramanian, Arjun and Houck, Kevin
		  and Shaikh, Mohammed Danish and Venkataraman, Shivaram and
		  Akella, Aditya},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {138--152},
  year		= {2021}
}

@Misc{		  singularity-net,
  title		= {SingularityNET},
  howpublished	= {\url{https://public.singularitynet.io/whitepaper.pdf}}
}

@Article{	  sinha_jouletrack_2001,
  title		= {{JouleTrack} - {A} {Web} {Based} {Tool} for {Software}
		  {Energy} {Profiling}},
  abstract	= {A software energy estimation methodology is presented that
		  avoids explicit characterization of instruction energy
		  consumption and predicts energy consumption to within 3\%
		  accuracy for a set of benchmark programs evaluated on the
		  StrongARM SA-1100 and Hitachi SH-4 microprocessors. The
		  tool, JouleTrack, is available as an online resource and
		  has various estimation levels. It also isolates the
		  switching and leakage components of the energy
		  consumption.},
  language	= {en},
  journal	= {DAC},
  author	= {Sinha, Amit and Chandrakasan, Anantha P},
  year		= {2001},
  pages		= {6},
  file		= {Sinha and Chandrakasan - JouleTrack - A Web Based Tool for
		  Software Energy
		  .pdf:/home/prateeks/Zotero/storage/K8MN6IT4/Sinha and
		  Chandrakasan - JouleTrack - A Web Based Tool for Software
		  Energy .pdf:application/pdf}
}

@InProceedings{	  snowdon_koala_2009,
  address	= {Nuremberg, Germany},
  title		= {Koala: a platform for {OS}-level power management},
  isbn		= {978-1-60558-482-9},
  shorttitle	= {Koala},
  url		= {http://portal.acm.org/citation.cfm?doid=1519065.1519097},
  doi		= {10.1145/1519065.1519097},
  abstract	= {Managing the power consumption of computing platforms is a
		  complicated problem thanks to a multitude of hardware
		  conﬁguration options and characteristics. Much of the
		  academic research is based on unrealistic assumptions, and
		  has, therefore, seen little practical uptake. We provide an
		  overview of the difﬁculties facing power management
		  schemes when used in real systems.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the fourth {ACM} european conference on
		  {Computer} systems - {EuroSys} '09},
  publisher	= {ACM Press},
  author	= {Snowdon, David C. and Le Sueur, Etienne and Petters,
		  Stefan M. and Heiser, Gernot},
  year		= {2009},
  pages		= {289},
  file		= {Snowdon et al. - 2009 - Koala a platform for OS-level
		  power
		  management.pdf:/home/prateeks/Zotero/storage/E26GA78G/Snowdon
		  et al. - 2009 - Koala a platform for OS-level power
		  management.pdf:application/pdf}
}

@InProceedings{	  solaiman2020wlec,
  title		= {WLEC: A Not So Cold Architecture to Mitigate Cold Start
		  Problem in Serverless Computing},
  author	= {Solaiman, Khondokar and Adnan, Muhammad Abdullah},
  booktitle	= {2020 IEEE International Conference on Cloud Engineering
		  (IC2E)},
  pages		= {144--153},
  year		= {2020},
  organization	= {IEEE}
}

@InProceedings{	  sorber_eon_2007,
  address	= {Sydney, Australia},
  title		= {Eon: a language and runtime system for perpetual systems},
  isbn		= {978-1-59593-763-6},
  shorttitle	= {Eon},
  url		= {http://portal.acm.org/citation.cfm?doid=1322263.1322279},
  doi		= {10.1145/1322263.1322279},
  abstract	= {Embedded systems can operate perpetually without being
		  connected to a power source by harvesting environmental
		  energy from motion, the sun, wind, or heat differentials.
		  However, programming these perpetual systems is
		  challenging. In response to changing energy levels,
		  programmers can adjust the execution frequency of
		  energy-intensive tasks, or provide higher service levels
		  when energy is plentiful and lower service levels when
		  energy is scarce. However, it is often difﬁcult for
		  programmers to predict the energy consumption resulting
		  from these adjustments. Worse, explicit energy management
		  can tie a program to a particular hardware platform,
		  limiting portability.},
  language	= {en},
  urldate	= {2022-07-08},
  booktitle	= {Proceedings of the 5th international conference on
		  {Embedded} networked sensor systems - {SenSys} '07},
  publisher	= {ACM Press},
  author	= {Sorber, Jacob and Kostadinov, Alexander and Garber,
		  Matthew and Brennan, Matthew and Corner, Mark D. and
		  Berger, Emery D.},
  year		= {2007},
  pages		= {161},
  file		= {Sorber et al. - 2007 - Eon a language and runtime system
		  for perpetual
		  s.pdf:/home/prateeks/Zotero/storage/WQG9VHFW/Sorber et al.
		  - 2007 - Eon a language and runtime system for perpetual
		  s.pdf:application/pdf}
}

@InProceedings{	  spotcheck,
  title		= {Spotcheck: Designing a derivative iaas cloud on the spot
		  market},
  author	= {Prateek Sharma and Stephen Lee and Tian Guo and David
		  Irwin and Prashant Shenoy},
  booktitle	= {Proceedings of the Tenth European Conference on Computer
		  Systems (EuroSys)},
  year		= {2015},
  organization	= {ACM}
}

@Article{	  sreekanti2020cloudburst,
  title		= {Cloudburst: Stateful functions-as-a-service},
  author	= {Sreekanti, Vikram and Wu, Chenggang and Lin, Xiayue
		  Charles and Schleier-Smith, Johann and Faleiro, Jose M and
		  Gonzalez, Joseph E and Hellerstein, Joseph M and Tumanov,
		  Alexey},
  journal	= {arXiv preprint arXiv:2001.04592},
  year		= {2020}
}

@Article{	  stanley-marbell_exploiting_2021,
  title		= {Exploiting {Errors} for {Efficiency}: {A} {Survey} from
		  {Circuits} to {Applications}},
  volume	= {53},
  issn		= {0360-0300, 1557-7341},
  shorttitle	= {Exploiting {Errors} for {Efficiency}},
  url		= {https://dl.acm.org/doi/10.1145/3394898},
  doi		= {10.1145/3394898},
  abstract	= {When a computational task tolerates a relaxation of its
		  specification or when an algorithm tolerates the effects of
		  noise in its execution, hardware, system software, and
		  programming language compilers or their runtime systems can
		  trade deviations from correct behavior for lower resource
		  usage. We present, for the first time, a synthesis of
		  research results on computing systems that only make as
		  many errors as their end-to-end applications can tolerate.
		  The results span the disciplines of computer-aided design
		  of circuits, digital system design, computer architecture,
		  programming languages, operating systems, and information
		  theory. Rather than over-provisioning the resources
		  controlled by each of these layers of abstraction to avoid
		  errors, it can be more efficient to exploit the masking of
		  errors occurring at one layer and thereby prevent those
		  errors from propagating to a higher layer. We demonstrate
		  the potential benefits of end-to-end approaches using two
		  illustrative examples. We introduce a formalization of
		  terminology that allows us to present a coherent view
		  across the techniques traditionally used by different
		  research communities in their individual layer of focus.
		  Using this formalization, we survey tradeoffs for
		  individual layers of computing systems at the circuit,
		  architecture, operating system, and programming language
		  levels as well as fundamental information-theoretic limits
		  to tradeoffs between resource usage and correctness.},
  language	= {en},
  number	= {3},
  urldate	= {2022-07-08},
  journal	= {ACM Computing Surveys},
  author	= {Stanley-Marbell, Phillip and Alaghi, Armin and Carbin,
		  Michael and Darulova, Eva and Dolecek, Lara and Gerstlauer,
		  Andreas and Gillani, Ghayoor and Jevdjic, Djordje and
		  Moreau, Thierry and Cacciotti, Mattia and Daglis,
		  Alexandros and Jerger, Natalie Enright and Falsafi, Babak
		  and Misailovic, Sasa and Sampson, Adrian and Zufferey,
		  Damien},
  month		= may,
  year		= {2021},
  pages		= {1--39},
  file		= {Stanley-Marbell et al. - 2021 - Exploiting Errors for
		  Efficiency A Survey from
		  Ci.pdf:/home/prateeks/Zotero/storage/D5YAH2MS/Stanley-Marbell
		  et al. - 2021 - Exploiting Errors for Efficiency A Survey
		  from Ci.pdf:application/pdf}
}

@InProceedings{	  sundarrajan2017footprint,
  title		= {Footprint descriptors: Theory and practice of cache
		  provisioning in a global cdn},
  author	= {Sundarrajan, Aditya and Feng, Mingdong and Kasbekar,
		  Mangesh and Sitaraman, Ramesh K},
  booktitle	= {Proceedings of the 13th International Conference on
		  emerging Networking EXperiments and Technologies},
  pages		= {55--67},
  year		= {2017}
}

@InProceedings{	  suresh2019fnsched,
  title		= {Fnsched: An efficient scheduler for serverless functions},
  author	= {Suresh, Amoghvarsha and Gandhi, Anshul},
  booktitle	= {Proceedings of the 5th International Workshop on
		  Serverless Computing},
  pages		= {19--24},
  year		= {2019}
}

###InProceedings{ suresh2019fnsched,
  title		= {Fnsched: An efficient scheduler for serverless functions},
  author	= {Suresh, Amoghvarsha and Gandhi, Anshul},
  booktitle	= {Proceedings of the 5th International Workshop on
		  Serverless Computing},
  pages		= {19--24},
  year		= {2019}
}

@InProceedings{	  suresh2021servermore,
  title		= {ServerMore: Opportunistic Execution of Serverless
		  Functions in the Cloud},
  author	= {Suresh, Amoghavarsha and Gandhi, Anshul},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {570--584},
  year		= {2021}
}

###InProceedings{ suresh2021servermore,
  title		= {ServerMore: Opportunistic Execution of Serverless
		  Functions in the Cloud},
  author	= {Suresh, Amoghavarsha and Gandhi, Anshul},
  booktitle	= {Proceedings of the ACM Symposium on Cloud Computing},
  pages		= {570--584},
  year		= {2021}
}

@Article{	  tang_nipd_2017,
  title		= {{NIPD}: {Non}-{Intrusive} {Power} {Disaggregation} in
		  {Legacy} {Datacenters}},
  volume	= {66},
  issn		= {1557-9956},
  shorttitle	= {{NIPD}},
  doi		= {10.1109/TC.2016.2582163},
  abstract	= {Fine-grained power monitoring, which refers to power
		  monitoring at the server level, is critical to the
		  efficient operation and energy saving of datacenters.
		  Fined-grained power monitoring, however, is extremely
		  challenging in legacy datacenters that host server systems
		  not equipped with power monitoring sensors. Installing
		  power monitoring hardware at the server level not only
		  incurs high costs but also complicates the maintenance of
		  high-density server clusters and enclosures. In this paper,
		  we present a zero-cost, purely software-based solution to
		  this challenging problem. We use a novel technique of
		  non-intrusive power disaggregation (NIPD) that establishes
		  power mapping functions (PMFs) between the states of
		  servers and their power consumption, and infer the power
		  consumption of each server with the aggregated power of the
		  entire datacenter. The PMFs that we have developed can
		  support both linear and nonlinear power models via the
		  state feature transformation. To reduce the training
		  overhead, we further develop adaptive PMFs update
		  strategies and ensure that the training data and state
		  features are appropriately selected. We implement and
		  evaluate NIPD over a real-world datacenter with 326 nodes.
		  The results show that our solution can provide high
		  precision power estimation at both rack level and server
		  level. In specific, with PMFs including only two nonlinear
		  terms, our power estimation i) at rack level has mean
		  relative error of 2.18 percent, and ii) at server level has
		  mean relative errors of 9.61 and 7.53 percent corresponding
		  to the idle and peak power, respectively.},
  number	= {2},
  journal	= {IEEE Transactions on Computers},
  author	= {Tang, Guoming and Jiang, Weixiang and Xu, Zhifeng and Liu,
		  Fangming and Wu, Kui},
  month		= feb,
  year		= {2017},
  note		= {Conference Name: IEEE Transactions on Computers},
  keywords	= {Monitoring, Hardware, Servers, Training, Power
		  measurement, Power demand, Datacenter, Sensors,
		  non-intrusive power disaggregation, power monitoring,
		  servers},
  pages		= {312--325},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/I8YCCC3Y/7494629.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/VGVEFXTD/Tang et al. -
		  2017 - NIPD Non-Intrusive Power Disaggregation in
		  Legacy.pdf:application/pdf}
}

###Article{	  tang_nipd_2017,
  title		= {{NIPD}: {Non}-{Intrusive} {Power} {Disaggregation} in
		  {Legacy} {Datacenters}},
  volume	= {66},
  issn		= {1557-9956},
  shorttitle	= {{NIPD}},
  doi		= {10.1109/TC.2016.2582163},
  abstract	= {Fine-grained power monitoring, which refers to power
		  monitoring at the server level, is critical to the
		  efficient operation and energy saving of datacenters.
		  Fined-grained power monitoring, however, is extremely
		  challenging in legacy datacenters that host server systems
		  not equipped with power monitoring sensors. Installing
		  power monitoring hardware at the server level not only
		  incurs high costs but also complicates the maintenance of
		  high-density server clusters and enclosures. In this paper,
		  we present a zero-cost, purely software-based solution to
		  this challenging problem. We use a novel technique of
		  non-intrusive power disaggregation (NIPD) that establishes
		  power mapping functions (PMFs) between the states of
		  servers and their power consumption, and infer the power
		  consumption of each server with the aggregated power of the
		  entire datacenter. The PMFs that we have developed can
		  support both linear and nonlinear power models via the
		  state feature transformation. To reduce the training
		  overhead, we further develop adaptive PMFs update
		  strategies and ensure that the training data and state
		  features are appropriately selected. We implement and
		  evaluate NIPD over a real-world datacenter with 326 nodes.
		  The results show that our solution can provide high
		  precision power estimation at both rack level and server
		  level. In specific, with PMFs including only two nonlinear
		  terms, our power estimation i) at rack level has mean
		  relative error of 2.18 percent, and ii) at server level has
		  mean relative errors of 9.61 and 7.53 percent corresponding
		  to the idle and peak power, respectively.},
  number	= {2},
  journal	= {IEEE Transactions on Computers},
  author	= {Tang, Guoming and Jiang, Weixiang and Xu, Zhifeng and Liu,
		  Fangming and Wu, Kui},
  month		= feb,
  year		= {2017},
  note		= {Conference Name: IEEE Transactions on Computers},
  keywords	= {Monitoring, Hardware, Servers, Training, Power
		  measurement, Power demand, Datacenter, Sensors,
		  non-intrusive power disaggregation, power monitoring,
		  servers},
  pages		= {312--325},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/I8YCCC3Y/7494629.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/VGVEFXTD/Tang et al. -
		  2017 - NIPD Non-Intrusive Power Disaggregation in
		  Legacy.pdf:application/pdf}
}

@InProceedings{	  tariq_sequoia_2020,
  address	= {Virtual Event USA},
  title		= {Sequoia: enabling quality-of-service in serverless
		  computing},
  isbn		= {978-1-4503-8137-6},
  shorttitle	= {Sequoia},
  url		= {https://dl.acm.org/doi/10.1145/3419111.3421306},
  doi		= {10.1145/3419111.3421306},
  abstract	= {Serverless computing is a rapidly growing paradigm that
		  easily harnesses the power of the cloud. With serverless
		  computing, developers simply provide an event-driven
		  function to cloud providers, and the provider seamlessly
		  scales function invocations to meet demands as
		  event-triggers occur. As current and future serverless
		  o�erings support a wide variety of serverless
		  applications, e�ective techniques to manage serverless
		  workloads becomes an important issue. This work examines
		  current management and scheduling practices in cloud
		  providers, uncovering many issues including in�ated
		  application run times, function drops, ine�cient
		  allocations, and other undocumented and unexpected
		  behavior. To �x these issues, a new quality-of-service
		  function scheduling and allocation framework, called
		  Sequoia, is designed. Sequoia allows developers or
		  administrators to easily de�ne how serverless functions
		  and applications should be deployed, capped, prioritized,
		  or altered based on easily con�gured, �exible policies.
		  Results with controlled and realistic workloads show
		  Sequoia seamlessly adapts to policies, eliminates mid-chain
		  drops, reduces queuing times by up to 6.4⇥, enforces
		  tight chain-level fairness, and improves run-time
		  performance up to 25⇥.},
  language	= {en},
  urldate	= {2021-07-16},
  booktitle	= {Proceedings of the 11th {ACM} {Symposium} on {Cloud}
		  {Computing}},
  publisher	= {ACM},
  author	= {Tariq, Ali and Pahl, Austin and Nimmagadda, Sharat and
		  Rozner, Eric and Lanka, Siddharth},
  month		= oct,
  year		= {2020},
  pages		= {311--327},
  file		= {Tariq et al. - 2020 - Sequoia enabling quality-of-service
		  in
		  serverless.pdf:/home/prateeks/Zotero/storage/J5GF66VY/Tariq
		  et al. - 2020 - Sequoia enabling quality-of-service in
		  serverless.pdf:application/pdf}
}

@InProceedings{	  tian_owl_2022,
  address	= {San Francisco California},
  title		= {Owl: performance-aware scheduling for resource-efficient
		  function-as-a-service cloud},
  isbn		= {978-1-4503-9414-7},
  shorttitle	= {Owl},
  url		= {https://dl.acm.org/doi/10.1145/3542929.3563470},
  doi		= {10.1145/3542929.3563470},
  language	= {en},
  urldate	= {2023-01-20},
  booktitle	= {Proceedings of the 13th {Symposium} on {Cloud}
		  {Computing}},
  publisher	= {ACM},
  author	= {Tian, Huangshi and Li, Suyi and Wang, Ao and Wang, Wei and
		  Wu, Tianlong and Yang, Haoran},
  month		= nov,
  year		= {2022},
  pages		= {78--93},
  file		= {Tian et al. - 2022 - Owl performance-aware scheduling for
		  resource-eff.pdf:/home/prateeks/Zotero/storage/LYKBHWZY/Tian
		  et al. - 2022 - Owl performance-aware scheduling for
		  resource-eff.pdf:application/pdf}
}

@InProceedings{	  tpu-datacenter,
  title		= {In-datacenter performance analysis of a tensor processing
		  unit},
  author	= {Jouppi, Norman P and Young, Cliff and Patil, Nishant and
		  Patterson, David and Agrawal, Gaurav and Bajwa, Raminder
		  and Bates, Sarah and Bhatia, Suresh and Boden, Nan and
		  Borchers, Al and others},
  booktitle	= {Proceedings of the 44th annual international symposium on
		  computer architecture},
  pages		= {1--12},
  year		= {2017}
}

@Article{	  transient,
  author	= {R. Singh and P. Sharma and D. Irwin and P. Shenoy and K.K.
		  Ramakrishnan},
  title		= {Here {T}oday, {G}one {T}omorrow: Exploiting {T}ransient
		  {S}ervers in {D}ata {C}enters},
  journal	= {IEEE Internet Computing},
  month		= {July/August},
  year		= {2014},
  volume	= {18},
  number	= {4}
}

@Article{	  treehouse_hotc22,
  title		= {A Case For Carbon-Aware Datacenter Software},
  author	= {Thomas Anderson, Adam Belay, Mosharaf Chowdhury, Asaf
		  Cidon, Irene Zhang},
  year		= {2022},
  journal	= {HotCarbon: Workshop on Sustainable Computer Systems Design
		  and Implementation}
}

@InProceedings{	  unikernels,
  author	= {Madhavapeddy, Anil and Mortier, Richard and Rotsos,
		  Charalampos and Scott, David and Singh, Balraj and
		  Gazagnaire, Thomas and Smith, Steven and Hand, Steven and
		  Crowcroft, Jon},
  title		= {Unikernels: Library Operating Systems for the Cloud},
  booktitle	= {Proceedings of the Eighteenth International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  series	= {ASPLOS '13},
  year		= {2013},
  isbn		= {978-1-4503-1870-9},
  location	= {Houston, Texas, USA},
  pages		= {461--472},
  numpages	= {12},
  url		= {http://doi.acm.org/10.1145/2451116.2451167},
  doi		= {10.1145/2451116.2451167},
  acmid		= {2451167},
  publisher	= {ACM},
  address	= {New York, NY, USA},
  keywords	= {functional programming, hypervisor, microkernel}
}

###InProceedings{ unikernels,
  author	= {Madhavapeddy, Anil and Mortier, Richard and Rotsos,
		  Charalampos and Scott, David and Singh, Balraj and
		  Gazagnaire, Thomas and Smith, Steven and Hand, Steven and
		  Crowcroft, Jon},
  title		= {Unikernels: Library Operating Systems for the Cloud},
  booktitle	= {Proceedings of the Eighteenth International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  series	= {ASPLOS '13},
  year		= {2013},
  isbn		= {978-1-4503-1870-9},
  location	= {Houston, Texas, USA},
  pages		= {461--472},
  numpages	= {12},
  url		= {http://doi.acm.org/10.1145/2451116.2451167},
  doi		= {10.1145/2451116.2451167},
  acmid		= {2451167},
  publisher	= {ACM},
  address	= {New York, NY, USA},
  keywords	= {functional programming, hypervisor, microkernel}
}

@Article{	  urgaonkar2005analytical,
  title		= {An analytical model for multi-tier internet services and
		  its applications},
  author	= {Urgaonkar, Bhuvan and Pacifici, Giovanni and Shenoy,
		  Prashant and Spreitzer, Mike and Tantawi, Asser},
  journal	= {ACM SIGMETRICS Performance Evaluation Review},
  volume	= {33},
  number	= {1},
  pages		= {291--302},
  year		= {2005},
  publisher	= {ACM New York, NY, USA}
}

@InProceedings{	  ustiugov2021benchmarking,
  title		= {Benchmarking, analysis, and optimization of serverless
		  function snapshots},
  author	= {Ustiugov, Dmitrii and Petrov, Plamen and Kogias, Marios
		  and Bugnion, Edouard and Grot, Boris},
  booktitle	= {Proceedings of the 26th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {559--572},
  year		= {2021}
}

@InProceedings{	  ustiugov_analyzing_2021,
  address	= {Storrs, CT, USA},
  title		= {Analyzing {Tail} {Latency} in {Serverless} {Clouds} with
		  {STeLLAR}},
  isbn		= {978-1-66544-173-5},
  url		= {https://ieeexplore.ieee.org/document/9668286/},
  doi		= {10.1109/IISWC53511.2021.00016},
  abstract	= {Serverless computing has seen rapid adoption because of
		  its instant scalability, ﬂexible billing model, and
		  economies of scale. In serverless, developers structure
		  their applications as a collection of functions invoked by
		  various events like clicks, and cloud providers take
		  responsibility for cloud infrastructure management. As with
		  other cloud services, serverless deployments require
		  responsiveness and performance predictability manifested
		  through low average and tail latencies. While the average
		  end-to-end latency has been extensively studied in prior
		  works, existing papers lack a detailed characterization of
		  the effects of tail latency in real-world serverless
		  scenarios and their root causes.},
  language	= {en},
  urldate	= {2023-01-09},
  booktitle	= {2021 {IEEE} {International} {Symposium} on {Workload}
		  {Characterization} ({IISWC})},
  publisher	= {IEEE},
  author	= {Ustiugov, Dmitrii and Amariucai, Theodor and Grot, Boris},
  month		= nov,
  year		= {2021},
  pages		= {51--62},
  file		= {Ustiugov et al. - 2021 - Analyzing Tail Latency in
		  Serverless Clouds with
		  S.pdf:/home/prateeks/Zotero/storage/J7BNQJB9/Ustiugov et
		  al. - 2021 - Analyzing Tail Latency in Serverless Clouds
		  with S.pdf:application/pdf}
}

###InProceedings{ ustiugov_analyzing_2021,
  address	= {Storrs, CT, USA},
  title		= {Analyzing {Tail} {Latency} in {Serverless} {Clouds} with
		  {STeLLAR}},
  isbn		= {978-1-66544-173-5},
  url		= {https://ieeexplore.ieee.org/document/9668286/},
  doi		= {10.1109/IISWC53511.2021.00016},
  abstract	= {Serverless computing has seen rapid adoption because of
		  its instant scalability, ﬂexible billing model, and
		  economies of scale. In serverless, developers structure
		  their applications as a collection of functions invoked by
		  various events like clicks, and cloud providers take
		  responsibility for cloud infrastructure management. As with
		  other cloud services, serverless deployments require
		  responsiveness and performance predictability manifested
		  through low average and tail latencies. While the average
		  end-to-end latency has been extensively studied in prior
		  works, existing papers lack a detailed characterization of
		  the effects of tail latency in real-world serverless
		  scenarios and their root causes.},
  language	= {en},
  urldate	= {2023-01-09},
  booktitle	= {2021 {IEEE} {International} {Symposium} on {Workload}
		  {Characterization} ({IISWC})},
  publisher	= {IEEE},
  author	= {Ustiugov, Dmitrii and Amariucai, Theodor and Grot, Boris},
  month		= nov,
  year		= {2021},
  pages		= {51--62},
  file		= {Ustiugov et al. - 2021 - Analyzing Tail Latency in
		  Serverless Clouds with
		  S.pdf:/home/prateeks/Zotero/storage/J7BNQJB9/Ustiugov et
		  al. - 2021 - Analyzing Tail Latency in Serverless Clouds
		  with S.pdf:application/pdf}
}

@InProceedings{	  van_eyk_spec_2017,
  address	= {Las Vegas, Nevada},
  title		= {The {SPEC} cloud group's research vision on {FaaS} and
		  serverless architectures},
  isbn		= {978-1-4503-5434-9},
  url		= {http://dl.acm.org/citation.cfm?doid=3154847.3154848},
  doi		= {10.1145/3154847.3154848},
  abstract	= {Cloud computing enables an entire ecosystem of developing,
		  composing, and providing IT services. An emerging class of
		  cloud-based software architectures, serverless, focuses on
		  providing software architects the ability to execute
		  arbitrary functions with small overhead in server
		  management, as Function-as-a-service (FaaS). However
		  useful, serverless and FaaS suffer from a community problem
		  that faces every emerging technology, which has indeed also
		  hampered cloud computing a decade ago: lack of clear
		  terminology, and scattered vision about the field. In this
		  work, we address this community problem. We clarify the
		  term serverless, by reducing it to cloud functions as
		  programming units, and a model of executing simple and
		  complex (e.g., workflows of) functions with operations
		  managed primarily by the cloud provider. We propose a
		  research vision, where 4 key directions (perspectives)
		  present 17 technical opportunities and challenges.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 2nd {International} {Workshop} on
		  {Serverless} {Computing} - {WoSC} '17},
  publisher	= {ACM Press},
  author	= {van Eyk, Erwin and Iosup, Alexandru and Seif, Simon and
		  Thömmes, Markus},
  year		= {2017},
  pages		= {1--4},
  file		= {van Eyk et al. - 2017 - The SPEC cloud group's research
		  vision on FaaS
		  and.pdf:/home/prateeks/Zotero/storage/QTXZKT4I/van Eyk et
		  al. - 2017 - The SPEC cloud group's research vision on FaaS
		  and.pdf:application/pdf}
}

###InProceedings{ van_eyk_spec_2017,
  address	= {Las Vegas, Nevada},
  title		= {The {SPEC} cloud group's research vision on {FaaS} and
		  serverless architectures},
  isbn		= {978-1-4503-5434-9},
  url		= {http://dl.acm.org/citation.cfm?doid=3154847.3154848},
  doi		= {10.1145/3154847.3154848},
  abstract	= {Cloud computing enables an entire ecosystem of developing,
		  composing, and providing IT services. An emerging class of
		  cloud-based software architectures, serverless, focuses on
		  providing software architects the ability to execute
		  arbitrary functions with small overhead in server
		  management, as Function-as-a-service (FaaS). However
		  useful, serverless and FaaS suffer from a community problem
		  that faces every emerging technology, which has indeed also
		  hampered cloud computing a decade ago: lack of clear
		  terminology, and scattered vision about the field. In this
		  work, we address this community problem. We clarify the
		  term serverless, by reducing it to cloud functions as
		  programming units, and a model of executing simple and
		  complex (e.g., workflows of) functions with operations
		  managed primarily by the cloud provider. We propose a
		  research vision, where 4 key directions (perspectives)
		  present 17 technical opportunities and challenges.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the 2nd {International} {Workshop} on
		  {Serverless} {Computing} - {WoSC} '17},
  publisher	= {ACM Press},
  author	= {van Eyk, Erwin and Iosup, Alexandru and Seif, Simon and
		  Thömmes, Markus},
  year		= {2017},
  pages		= {1--4},
  file		= {van Eyk et al. - 2017 - The SPEC cloud group's research
		  vision on FaaS
		  and.pdf:/home/prateeks/Zotero/storage/QTXZKT4I/van Eyk et
		  al. - 2017 - The SPEC cloud group's research vision on FaaS
		  and.pdf:application/pdf}
}

@InProceedings{	  vergara_sharing_2015,
  address	= {Cancun Mexico},
  title		= {Sharing the {Cost} of {Lunch}: {Energy} {Apportionment}
		  {Policies}},
  isbn		= {978-1-4503-3757-1},
  shorttitle	= {Sharing the {Cost} of {Lunch}},
  url		= {https://dl.acm.org/doi/10.1145/2815317.2815338},
  doi		= {10.1145/2815317.2815338},
  abstract	= {Energy consumption has become a hot topic in computer and
		  communication technologies pinpointing the need to
		  carefully analyse system eﬃciency. The energy consumption
		  of a system is determined by the usage patterns of system
		  components and complex interactions between the coexisting
		  entities and resources. Providing transparency of a
		  system’s consumption by breaking down the total
		  consumption is vital to evaluate and provide
		  energy-eﬃcient design and operation.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the 11th {ACM} {Symposium} on {QoS} and
		  {Security} for {Wireless} and {Mobile} {Networks}},
  publisher	= {ACM},
  author	= {Vergara, Ekhiotz Jon and Nadjm-Tehrani, Simin and Asplund,
		  Mikael},
  month		= nov,
  year		= {2015},
  pages		= {91--97},
  file		= {Vergara et al. - 2015 - Sharing the Cost of Lunch Energy
		  Apportionment
		  Po.pdf:/home/prateeks/Zotero/storage/3ZYHTEQN/Vergara et
		  al. - 2015 - Sharing the Cost of Lunch Energy Apportionment
		  Po.pdf:application/pdf}
}

@InProceedings{	  vhive-asplos21,
  title		= {Benchmarking, analysis, and optimization of serverless
		  function snapshots},
  author	= {Ustiugov, Dmitrii and Petrov, Plamen and Kogias, Marios
		  and Bugnion, Edouard and Grot, Boris},
  booktitle	= {Proceedings of the 26th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {559--572},
  year		= {2021}
}

###InProceedings{ vhive-asplos21,
  title		= {Benchmarking, analysis, and optimization of serverless
		  function snapshots},
  author	= {Ustiugov, Dmitrii and Petrov, Plamen and Kogias, Marios
		  and Bugnion, Edouard and Grot, Boris},
  booktitle	= {Proceedings of the 26th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems},
  pages		= {559--572},
  year		= {2021}
}

@InProceedings{	  vid-edge-serverless-mmsys21,
  author	= {Zhang, Miao and Wang, Fangxin and Zhu, Yifei and Liu,
		  Jiangchuan and Wang, Zhi},
  title		= {Towards Cloud-Edge Collaborative Online Video Analytics
		  with Fine-Grained Serverless Pipelines},
  year		= {2021},
  isbn		= {9781450384346},
  publisher	= {Association for Computing Machinery},
  address	= {New York, NY, USA},
  url		= {https://doi.org/10.1145/3458305.3463377},
  doi		= {10.1145/3458305.3463377},
  abstract	= {The ever-growing deployment scale of surveillance cameras
		  and the users' increasing appetite for real-time queries
		  have urged online video analytics. Synergizing the
		  virtually unlimited cloud resources with agile edge
		  processing would deliver an ideal online video analytics
		  system; yet, given the complex interaction and dependency
		  within and across video query pipelines, it is easier said
		  than done. This paper starts with a measurement study to
		  acquire a deep understanding of video query pipelines on
		  real-world camera streams. We identify the potentials and
		  practical challenges towards cloud-edge collaborative video
		  analytics. We then argue that the newly emerged serverless
		  computing paradigm is the key to achieve fine-grained
		  resource partitioning with minimum dependency. We
		  accordingly propose CEVAS, a Cloud-Edge collaborative Video
		  Analytics system empowered by fine-grained Serverless
		  pipelines. It builds flexible serverless-based
		  infrastructures to facilitate fine-grained and adaptive
		  partitioning of cloud-edge workloads for multiple
		  concurrent query pipelines. With the optimized design of
		  individual modules and their integration, CEVAS achieves
		  real-time responses to highly dynamic input workloads. We
		  have developed a prototype of CEVAS over Amazon Web
		  Services (AWS) and conducted extensive experiments with
		  real-world video streams and queries. The results show that
		  by judiciously coordinating the fine-grained serverless
		  resources in the cloud and at the edge, CEVAS reduces 86.9%
		  cloud expenditure and 74.4% data transfer overhead of a
		  pure cloud scheme and improves the analysis throughput of a
		  pure edge scheme by up to 20.6%. Thanks to the fine-grained
		  video content-aware forecasting, CEVAS is also more
		  adaptive than the state-of-the-art cloud-edge collaborative
		  scheme.},
  booktitle	= {Proceedings of the 12th ACM Multimedia Systems
		  Conference},
  pages		= {80–93},
  numpages	= {14},
  keywords	= {cloud-edge collaboration, serverless computing, video
		  analytics},
  location	= {Istanbul, Turkey},
  series	= {MMSys '21}
}

@InProceedings{	  wang2018peeking,
  title		= {Peeking behind the curtains of serverless platforms},
  author	= {Wang, Liang and Li, Mengyuan and Zhang, Yinqian and
		  Ristenpart, Thomas and Swift, Michael},
  booktitle	= {2018 {USENIX} Annual Technical Conference},
  pages		= {133--146},
  year		= {2018}
}

@InProceedings{	  wang2020infinicache,
  title		= {Infinicache: Exploiting ephemeral serverless functions to
		  build a cost-effective memory cache},
  author	= {Wang, Ao and Zhang, Jingyuan and Ma, Xiaolong and Anwar,
		  Ali and Rupprecht, Lukas and Skourtis, Dimitrios and
		  Tarasov, Vasily and Yan, Feng and Cheng, Yue},
  booktitle	= {18th USENIX Conference on File and Storage Technologies
		  ($\{$FAST$\}$ 20)},
  pages		= {267--281},
  year		= {2020}
}

@InProceedings{	  wang2021lass,
  title		= {Lass: running latency sensitive serverless computations at
		  the edge},
  author	= {Wang, Bin and Ali-Eldin, Ahmed and Shenoy, Prashant},
  booktitle	= {Proceedings of the 30th International Symposium on
		  High-Performance Parallel and Distributed Computing},
  pages		= {239--251},
  year		= {2021}
}

@InProceedings{	  wang2021smartharvest,
  title		= {SmartHarvest: harvesting idle CPUs safely and efficiently
		  in the cloud},
  author	= {Wang, Yawen and Arya, Kapil and Kogias, Marios and Vanga,
		  Manohar and Bhandari, Aditya and Yadwadkar, Neeraja J and
		  Sen, Siddhartha and Elnikety, Sameh and Kozyrakis, Christos
		  and Bianchini, Ricardo},
  booktitle	= {Proceedings of the Sixteenth European Conference on
		  Computer Systems},
  pages		= {1--16},
  year		= {2021}
}

@Article{	  wang_distributed_2010,
  title		= {Distributed {Systems} {Meet} {Economics}: {Pricing} in the
		  {Cloud}},
  abstract	= {Cloud computing allows users to perform computation in a
		  public cloud with a pricing scheme typically based on
		  incurred resource consumption. While cloud computing is
		  often considered as merely a new application for classic
		  distributed systems, we argue that, by decoupling users
		  from cloud providers with a pricing scheme as the bridge,
		  cloud computing has fundamentally changed the landscape of
		  system design and optimization. Our preliminary studies on
		  Amazon EC2 cloud service and on a local cloud computing
		  testbed, have revealed an interesting interplay between
		  distributed systems and economics related to pricing. We
		  believe that this new angle of looking at distributed
		  systems potentially fosters new insights into cloud
		  computing.},
  language	= {en},
  journal	= {HotCloud},
  author	= {Wang, Hongyi and Jing, Qingfeng and Chen, Rishan and He,
		  Bingsheng and Qian, Zhengping and Zhou, Lidong},
  year		= {2010},
  pages		= {7},
  file		= {Wang et al. - Distributed Systems Meet Economics Pricing
		  in the.pdf:/home/prateeks/Zotero/storage/PMT85EWN/Wang et
		  al. - Distributed Systems Meet Economics Pricing in
		  the.pdf:application/pdf}
}

@InProceedings{	  wang_real-time_2020,
  address	= {Virtual Event Japan},
  title		= {Real-{Time} {Cooling} {Power} {Attribution} for
		  {Co}-{Located} {Data} {Center} {Rooms} with {Distinct}
		  {Temperatures}},
  isbn		= {978-1-4503-8061-4},
  url		= {https://dl.acm.org/doi/10.1145/3408308.3427607},
  doi		= {10.1145/3408308.3427607},
  abstract	= {At present, a co-location data center often applies an
		  identical and low temperature setpoint for its all server
		  rooms. Although increasing the temperature setpoint is a
		  rule-of-thumb approach to reducing the cooling energy
		  usage, the tenants may have different mentalities and
		  technical constraints in accepting higher temperature
		  setpoints. Thus, supporting distinct temperature setpoints
		  is desirable for a co-location data center in pursuing
		  higher energy efficiency. This support calls for a new
		  cooling power attribution scheme to address the inter-room
		  heat transfers that can be up to 9\% of server load as
		  shown in our real experiments. This paper describes our
		  approaches to estimating the inter-room heat transfers,
		  using the estimates to rectify the metered power usages of
		  the rooms’ air handling units, and fairly attributing the
		  power usage of the shared cooling infrastructure (i.e.,
		  chiller and cooling tower) to server rooms by following the
		  Shapley value principle. Extensive numeric experiments
		  based on a widely accepted cooling system model are
		  conducted to evaluate the effectiveness of the proposed
		  cooling power attribution scheme.},
  language	= {en},
  urldate	= {2022-09-11},
  booktitle	= {Proceedings of the 7th {ACM} {International} {Conference}
		  on {Systems} for {Energy}-{Efficient} {Buildings},
		  {Cities}, and {Transportation}},
  publisher	= {ACM},
  author	= {Wang, Rongrong and Van Le, Duc and Tan, Rui and Wong,
		  Yew-Wah and Wen, Yonggang},
  month		= nov,
  year		= {2020},
  pages		= {190--199},
  file		= {Wang et al. - 2020 - Real-Time Cooling Power Attribution
		  for
		  Co-Located.pdf:/home/prateeks/Zotero/storage/8SX6TM89/Wang
		  et al. - 2020 - Real-Time Cooling Power Attribution for
		  Co-Located.pdf:application/pdf}
}

@Misc{		  warm1,
  title		= {{Keeping Functions Warm - How To Fix AWS Lambda Cold Start
		  Issues}},
  year		= {},
  howpublished	= {\url{https://serverless.com/blog/keep-your-lambdas-warm/}}
}

###Misc{	  warm1,
  title		= {{Keeping Functions Warm - How To Fix AWS Lambda Cold Start
		  Issues}},
  year		= {},
  howpublished	= {\url{https://serverless.com/blog/keep-your-lambdas-warm/}}
}

@Misc{		  warm2,
  title		= {{Lambda Warmer: Optimize AWS Lambda Function Cold
		  Starts}},
  year		= {2018},
  howpublished	= {\url{https://www.jeremydaly.com/lambda-warmer-optimize-aws-lambda-function-cold-starts/}}
}

###Misc{	  warm2,
  title		= {{Lambda Warmer: Optimize AWS Lambda Function Cold
		  Starts}},
  year		= {2018},
  howpublished	= {\url{https://www.jeremydaly.com/lambda-warmer-optimize-aws-lambda-function-cold-starts/}}
}

@Misc{		  wasi,
  title		= {WebAssembly System Interface},
  howpublished	= {\url{https://wasi.dev/}}
}

@Book{		  welch1995introduction,
  title		= {An introduction to the Kalman filter},
  author	= {Welch, Greg and Bishop, Gary and others},
  year		= {1995},
  publisher	= {Chapel Hill, NC, USA}
}

@Article{	  wiesner_lets_2021,
  title		= {Let's {Wait} {Awhile}: {How} {Temporal} {Workload}
		  {Shifting} {Can} {Reduce} {Carbon} {Emissions} in the
		  {Cloud}},
  shorttitle	= {Let's {Wait} {Awhile}},
  url		= {http://arxiv.org/abs/2110.13234},
  doi		= {10.1145/3464298.3493399},
  abstract	= {Depending on energy sources and demand, the carbon
		  intensity of the public power grid fluctuates over time.
		  Exploiting this variability is an important factor in
		  reducing the emissions caused by data centers. However,
		  regional differences in the availability of lowcarbon
		  energy sources make it hard to provide general best
		  practices for when to consume electricity. Moreover,
		  existing research in this domain focuses mostly on
		  carbon-aware workload migration across geo-distributed data
		  centers, or addresses demand response purely from the
		  perspective of power grid stability and costs.},
  language	= {en},
  urldate	= {2022-04-22},
  journal	= {Proceedings of the 22nd International Middleware
		  Conference},
  author	= {Wiesner, Philipp and Behnke, Ilja and Scheinert, Dominik
		  and Gontarska, Kordian and Thamsen, Lauritz},
  month		= dec,
  year		= {2021},
  note		= {arXiv: 2110.13234},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing},
  pages		= {260--272},
  annote	= {Comment: To be published in the proceedings of the 22nd
		  International Middleware Conference (Middleware '21),
		  December 6-10, 2021, Virtual Event, Canada},
  file		= {Wiesner et al. - 2021 - Let's Wait Awhile How Temporal
		  Workload Shifting
		  .pdf:/home/prateeks/Zotero/storage/YM5KS7ID/Wiesner et al.
		  - 2021 - Let's Wait Awhile How Temporal Workload Shifting
		  .pdf:application/pdf}
}

@Article{	  winter2002shapley,
  title		= {The shapley value},
  author	= {Winter, Eyal},
  journal	= {Handbook of game theory with economic applications},
  volume	= {3},
  pages		= {2025--2054},
  year		= {2002},
  publisher	= {Elsevier}
}

@InProceedings{	  wu2020transactional,
  title		= {Transactional causal consistency for serverless
		  computing},
  author	= {Wu, Chenggang and Sreekanti, Vikram and Hellerstein,
		  Joseph M},
  booktitle	= {Proceedings of the 2020 ACM SIGMOD International
		  Conference on Management of Data},
  pages		= {83--97},
  year		= {2020}
}

@Article{	  wu2022container,
  title		= {Container lifecycle-aware scheduling for serverless
		  computing},
  author	= {Wu, Song and Tao, Zhiheng and Fan, Hao and Huang, Zhuo and
		  Zhang, Xinmin and Jin, Hai and Yu, Chen and Cao, Chun},
  journal	= {Software: Practice and Experience},
  volume	= {52},
  number	= {2},
  pages		= {337--352},
  year		= {2022},
  publisher	= {Wiley Online Library}
}

@Article{	  xu2021lambda,
  title		= {$\lambda$-DNN : Achieving Predictable Distributed DNN
		  Training with Serverless Architectures},
  author	= {Xu, Fei and Qin, Yiling and Chen, Li and Zhou, Zhi and
		  Liu, Fangming},
  journal	= {IEEE Transactions on Computers},
  year		= {2021},
  publisher	= {IEEE}
}

@Article{	  xylomenos_named_2019,
  title		= {Named {Functions} at the {Edge}},
  abstract	= {As end-user and edge-network devices are becoming ever
		  more powerful, they are producing ever increasing amounts
		  of data. Pulling all this data into the cloud for
		  processing is impossible, not only due to its enormous
		  volume, but also due to the stringent latency requirements
		  of many applications. Instead, we argue that end-user and
		  edge-network devices should collectively form edge
		  computing swarms and complement the cloud with their
		  storage and processing resources. This shift from
		  centralized to edge clouds has the potential to open new
		  horizons for application development, supporting new
		  low-latency services and, ultimately, creating new markets
		  for storage and processing resources. To realize this
		  vision, we propose Named Functions at the Edge (NFE), a
		  platform where functions can i) be identiﬁed through a
		  routable name, ii) be requested and moved (as data objects)
		  to process data on demand at edge nodes, iii) pull raw or
		  anonymized data from sensors and devices, iv) securely and
		  privately return their results to the invoker and v)
		  compensate each party for use of their data, storage,
		  communication or computing resources via tracking and
		  accountability mechanisms. We use an emergency evacuation
		  application to motivate the need for NFE and demonstrate
		  its potential.},
  language	= {en},
  author	= {Xylomenos, George and Pavlou, George and Psaras, Ioannis
		  and Karakonstantis, Ioannis},
  year		= {2019},
  pages		= {6},
  file		= {Xylomenos et al. - 2019 - Named Functions at the
		  Edge.pdf:/home/prateeks/Zotero/storage/2W7DJBP3/Xylomenos
		  et al. - 2019 - Named Functions at the
		  Edge.pdf:application/pdf}
}

###Article{	  xylomenos_named_2019,
  title		= {Named {Functions} at the {Edge}},
  abstract	= {As end-user and edge-network devices are becoming ever
		  more powerful, they are producing ever increasing amounts
		  of data. Pulling all this data into the cloud for
		  processing is impossible, not only due to its enormous
		  volume, but also due to the stringent latency requirements
		  of many applications. Instead, we argue that end-user and
		  edge-network devices should collectively form edge
		  computing swarms and complement the cloud with their
		  storage and processing resources. This shift from
		  centralized to edge clouds has the potential to open new
		  horizons for application development, supporting new
		  low-latency services and, ultimately, creating new markets
		  for storage and processing resources. To realize this
		  vision, we propose Named Functions at the Edge (NFE), a
		  platform where functions can i) be identiﬁed through a
		  routable name, ii) be requested and moved (as data objects)
		  to process data on demand at edge nodes, iii) pull raw or
		  anonymized data from sensors and devices, iv) securely and
		  privately return their results to the invoker and v)
		  compensate each party for use of their data, storage,
		  communication or computing resources via tracking and
		  accountability mechanisms. We use an emergency evacuation
		  application to motivate the need for NFE and demonstrate
		  its potential.},
  language	= {en},
  author	= {Xylomenos, George and Pavlou, George and Psaras, Ioannis
		  and Karakonstantis, Ioannis},
  year		= {2019},
  pages		= {6},
  file		= {Xylomenos et al. - 2019 - Named Functions at the
		  Edge.pdf:/home/prateeks/Zotero/storage/2W7DJBP3/Xylomenos
		  et al. - 2019 - Named Functions at the
		  Edge.pdf:application/pdf}
}

@InProceedings{	  yan2020hermes,
  title		= {Hermes: Efficient Cache Management for Container-based
		  Serverless Computing},
  author	= {Yan, Bowen and Gao, Heran and Wu, Heng and Zhang, Wenbo
		  and Hua, Lei and Huang, Tao},
  booktitle	= {12th Asia-Pacific Symposium on Internetware},
  pages		= {136--145},
  year		= {2020}
}

@InProceedings{	  yang2000general,
  title		= {General AIMD congestion control},
  author	= {Yang, Yang Richard and Lam, Simon S},
  booktitle	= {Proceedings 2000 International Conference on Network
		  Protocols},
  pages		= {187--198},
  year		= {2000},
  organization	= {IEEE}
}

@InProceedings{	  yang2020large,
  title		= {A large scale analysis of hundreds of in-memory cache
		  clusters at Twitter},
  author	= {Yang, Juncheng and Yue, Yao and Rashmi, KV},
  booktitle	= {14th USENIX Symposium on Operating Systems Design and
		  Implementation (OSDI 20)},
  pages		= {191--208},
  year		= {2020}
}

@InProceedings{	  yank,
  author	= {R. Singh and D. Irwin and P. Shenoy and K.K.
		  Ramakrishnan},
  title		= {Yank: Enabling {G}reen {D}ata {C}enters to {P}ull the
		  {P}lug},
  booktitle	= {NSDI},
  year		= {2013},
  month		= {April}
}

@InProceedings{	  ycsb-socc2010,
  title		= {Benchmarking cloud serving systems with YCSB},
  author	= {Cooper, Brian F and Silberstein, Adam and Tam, Erwin and
		  Ramakrishnan, Raghu and Sears, Russell},
  booktitle	= {Proceedings of the 1st ACM symposium on Cloud computing},
  pages		= {143--154},
  year		= {2010}
}

###InProceedings{ ycsb-socc2010,
  title		= {Benchmarking cloud serving systems with YCSB},
  author	= {Cooper, Brian F and Silberstein, Adam and Tam, Erwin and
		  Ramakrishnan, Raghu and Sears, Russell},
  booktitle	= {Proceedings of the 1st ACM symposium on Cloud computing},
  pages		= {143--154},
  year		= {2010}
}

@Article{	  yoon_appscope_2012,
  title		= {{AppScope}: {Application} {Energy} {Metering} {Framework}
		  for {Android} {Smartphones} using {Kernel} {Activity}
		  {Monitoring}},
  abstract	= {Understanding the energy consumption of a smartphone
		  application is a key area of interest for end users, as
		  well as application and system software developers.
		  Previous work has only been able to provide limited
		  information concerning the energy consumption of individual
		  applications because of limited access to underlying
		  hardware and system software. The energy consumption of a
		  smartphone application is, therefore, often estimated with
		  low accuracy and granularity. In this paper, we propose
		  AppScope, an Android-based energy metering system. This
		  system monitors application’s hardware usage at the
		  kernel level and accurately estimates energy consumption.
		  AppScope is implemented as a kernel module and uses an
		  event-driven monitoring method that generates low overhead
		  and provides high accuracy. The evaluation results indicate
		  that AppScope accurately estimates the energy consumption
		  of Android applications expending approximately 35mW and
		  2.1\% in power consumption and CPU utilization overhead,
		  respectively.},
  language	= {en},
  journal	= {USENIX ATC},
  author	= {Yoon, Chanmin and Kim, Dongwon and Jung, Wonwoo and Kang,
		  Chulkoo and Cha, Hojung},
  year		= {2012},
  pages		= {14},
  file		= {Yoon et al. - AppScope Application Energy Metering
		  Framework
		  fo.pdf:/home/prateeks/Zotero/storage/LLCXFHSM/Yoon et al. -
		  AppScope Application Energy Metering Framework
		  fo.pdf:application/pdf}
}

@Article{	  young2002line,
  title		= {On-line file caching},
  author	= {Young, Neal E},
  journal	= {Algorithmica},
  volume	= {33},
  number	= {3},
  pages		= {371--383},
  year		= {2002},
  publisher	= {Springer}
}

@Article{	  young_gd_orig_94,
  title		= {The {K}-server dual and loose competitiveness for paging},
  volume	= {11},
  issn		= {0178-4617, 1432-0541},
  url		= {http://link.springer.com/10.1007/BF01189992},
  doi		= {10.1007/BF01189992},
  abstract	= {Weighted caching is a generalization of paging in which
		  the cost to evict an item depends on the item. We give two
		  results concerning strategies for these problems that incur
		  a cost within a factor of the minimum possible on each
		  input.},
  language	= {en},
  number	= {6},
  urldate	= {2020-08-17},
  journal	= {Algorithmica},
  author	= {Young, N.},
  month		= jun,
  year		= {1994},
  pages		= {525--541},
  file		= {Young - 1994 - Thek-server dual and loose competitiveness
		  for pag.pdf:/home/prateeks/Zotero/storage/SEV3HAD8/Young -
		  1994 - Thek-server dual and loose competitiveness for
		  pag.pdf:application/pdf}
}

@InProceedings{	  yu2021faasrank,
  title		= {FaaSRank: Learning to Schedule Functions in Serverless
		  Platforms},
  author	= {Yu, Hanfei and Irissappane, Athirai A and Wang, Hao and
		  Lloyd, Wes J},
  booktitle	= {2021 IEEE International Conference on Autonomic Computing
		  and Self-Organizing Systems (ACSOS)},
  pages		= {31--40},
  year		= {2021},
  organization	= {IEEE}
}

###InProceedings{ yu2021faasrank,
  title		= {FaaSRank: Learning to Schedule Functions in Serverless
		  Platforms},
  author	= {Yu, Hanfei and Irissappane, Athirai A and Wang, Hao and
		  Lloyd, Wes J},
  booktitle	= {2021 IEEE International Conference on Autonomic Computing
		  and Self-Organizing Systems (ACSOS)},
  pages		= {31--40},
  year		= {2021},
  organization	= {IEEE}
}

###PhDThesis{	  yu2021faasrank,
  title		= {FaaSRank: A Reinforcement Learning Scheduler for
		  Serverless Function-as-a-Service Platforms},
  author	= {Yu, Hanfei},
  year		= {2021},
  school	= {University of Washington}
}

@InProceedings{	  zaharia2010delay,
  title		= {Delay scheduling: a simple technique for achieving
		  locality and fairness in cluster scheduling},
  author	= {Zaharia, Matei and Borthakur, Dhruba and Sen Sarma,
		  Joydeep and Elmeleegy, Khaled and Shenker, Scott and
		  Stoica, Ion},
  booktitle	= {Proceedings of the 5th European conference on Computer
		  systems},
  pages		= {265--278},
  year		= {2010}
}

@Article{	  zaharia2018accelerating,
  title		= {Accelerating the machine learning lifecycle with MLflow.},
  author	= {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and
		  Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and
		  Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul
		  and Parkhe, Mani and others},
  journal	= {IEEE Data Eng. Bull.},
  volume	= {41},
  number	= {4},
  pages		= {39--45},
  year		= {2018}
}

@Article{	  zeng_currentcy_2003,
  title		= {Currentcy: {A} {Unifying} {Abstraction} for {Expressing}
		  {Energy} {Management} {Policies}},
  abstract	= {The global nature of energy creates challenges and
		  opportunities for developing operating system policies to
		  effectively manage energy consumption in batterypowered
		  mobile/wireless devices. The proposed currentcy model
		  creates the framework for the operating system to manage
		  energy as a ﬁrst-class resource. Furthermore, currentcy
		  provides a powerful mechanism to formulate energy goals and
		  to unify resource management policies across diverse
		  competing applications and spanning device components with
		  very different power characteristics.},
  language	= {en},
  journal	= {USENIX ATC},
  author	= {Zeng, Heng and Ellis, Carla S and Lebeck, Alvin R and
		  Vahdat, Amin},
  year		= {2003},
  pages		= {14},
  file		= {Zeng et al. - Currentcy A Unifying Abstraction for
		  Expressing
		  E.pdf:/home/prateeks/Zotero/storage/YABHPL5B/Zeng et al. -
		  Currentcy A Unifying Abstraction for Expressing
		  E.pdf:application/pdf}
}

@Article{	  zeng_ecosystem_2002,
  title		= {{ECOSystem}: {Managing} {Energy} as a {First} {Class}
		  {Operating} {System} {Resource}.},
  abstract	= {Energy consumption has recently been widely recognized as
		  a major challenge of computer systems design. This paper
		  explores how to support energy as a first-class operating
		  system resource. Energy, because of its global system
		  nature, presents challenges beyond those of conventional
		  resource management. To"meet these challenges we propose
		  the Currentcy Model that unifies energy accounting over
		  diverse hardware components and enables fair allocation of
		  available energy among applications. Our particular goal is
		  to extend battery lifetime by limiting the average
		  discharge rate and to share this limited resource among
		  competing task according to user preferences. To
		  demonstrate how our framework supports explicit control
		  over the battery resource we implemented ECOSystem, a
		  modified Linux, that incorporates our currentcy model.
		  Experimental results show that ECOSystem accurately
		  accounts for the energy consumed by asynchronous device
		  operation, can achieve a target battery lifetime, and
		  proportionally shares the, limited energy resource among
		  competing tasks.},
  language	= {en},
  journal	= {ASPLOS},
  author	= {Zeng, Heng and Ellis, Carla S and Lebeck, Alvin R and
		  Vahdat, Amin},
  year		= {2002},
  pages		= {10},
  file		= {Zeng et al. - ECOSystem Managing Energy as a First Class
		  Operat.pdf:/home/prateeks/Zotero/storage/CHKGBWF6/Zeng et
		  al. - ECOSystem Managing Energy as a First Class
		  Operat.pdf:application/pdf}
}

@InProceedings{	  zhai2014happy,
  title		= {$\{$HaPPy$\}$: Hyperthread-aware Power Profiling
		  Dynamically},
  author	= {Zhai, Yan and Zhang, Xiao and Eranian, Stephane and Tang,
		  Lingjia and Mars, Jason},
  booktitle	= {2014 USENIX annual technical conference (USENIX ATC 14)},
  pages		= {211--217},
  year		= {2014}
}

@InProceedings{	  zhang_estimating_2020,
  title		= {Estimating {Power} {Consumption} of {Containers} and
		  {Virtual} {Machines} in {Data} {Centers}},
  doi		= {10.1109/CLUSTER49012.2020.00039},
  abstract	= {Virtualization technologies provide solutions of cloud
		  computing. Virtual resource scheduling is a crucial task in
		  data centers, and the power consumption of virtual
		  resources is a critical foundation of virtualization
		  scheduling. Containers are the smallest unit of virtual
		  resource scheduling and migration. Although many effective
		  models for estimating power consumption of virtual machines
		  (VM) have been proposed, few power estimation models of
		  containers have been put forth. In this paper, we offer a
		  fast-training piecewise regression model based on decision
		  tree to build a VM power estimation model and estimate the
		  containers' power by treating the container as a group of
		  processes on the VM. In our model, we characterize the
		  nonlinear relationship between power and features and
		  realize the effective estimation of the containers on the
		  VM. We evaluate the proposed model on 13 workloads in
		  PARSEC and compare it with several models. The experimental
		  results prove the effectiveness of our proposed model on
		  most workloads. Moreover, the estimated power of the
		  containers is in line with expectations.},
  booktitle	= {2020 {IEEE} {International} {Conference} on {Cluster}
		  {Computing} ({CLUSTER})},
  author	= {Zhang, Xusheng and Shen, Ziyu and Xia, Bin and Liu, Zheng
		  and Li, Yun},
  month		= sep,
  year		= {2020},
  note		= {ISSN: 2168-9253},
  keywords	= {Data models, virtual machines, Virtualization, Virtual
		  machining, Estimation, Containers, Processor scheduling,
		  containers, Power demand, data centers, power estimation},
  pages		= {288--293},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/QYUIRLW9/9229581.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/6G9MX5XF/Zhang et al. -
		  2020 - Estimating Power Consumption of Containers and
		  Vir.pdf:application/pdf}
}

@InProceedings{	  zhang_flex_2021,
  title		= {Flex: {High}-{Availability} {Datacenters} {With} {Zero}
		  {Reserved} {Power}},
  shorttitle	= {Flex},
  doi		= {10.1109/ISCA52012.2021.00033},
  abstract	= {Cloud providers, like Amazon and Microsoft, must guarantee
		  high availability for a large fraction of their workloads.
		  For this reason, they build datacenters with redundant
		  infrastructures for power delivery and cooling. Typically,
		  the redundant resources are reserved for use only during
		  infrastructure failure or maintenance events, so that
		  workload performance and availability do not suffer.
		  Unfortunately, the reserved resources also produce lower
		  power utilization and, consequently, require more
		  datacenters to be built. To address these problems, in this
		  paper we propose "zero-reserved-power" datacenters and the
		  Flex system to ensure that workloads still receive their
		  desired performance and availability. Flex leverages the
		  existence of software-redundant workloads that can tolerate
		  lower infrastructure availability, while imposing minimal
		  (if any) performance degradation for those that require
		  high infrastructure availability. Flex mainly comprises (1)
		  a new offline workload placement policy that reduces
		  stranded power while ensuring safety during failure or
		  maintenance events, and (2) a distributed system that
		  monitors for failures and quickly reduces the power draw
		  while respecting the workloads’ requirements, when it
		  detects a failure. Our evaluation shows that Flex produces
		  less than 5\% stranded power and increases the number of
		  deployed servers by up to 33\%, which translates to
		  hundreds of millions of dollars in construction cost
		  savings per datacenter site. We end the paper with lessons
		  from our experience bringing Flex to production in
		  Microsoft’s datacenters.},
  booktitle	= {2021 {ACM}/{IEEE} 48th {Annual} {International}
		  {Symposium} on {Computer} {Architecture} ({ISCA})},
  author	= {Zhang, Chaojie and Kumbhare, Alok Gautam and Manousakis,
		  Ioannis and Zhang, Deli and Misra, Pulkit A. and Assis, Rod
		  and Woolcock, Kyle and Mahalingam, Nithish and Warrier,
		  Brijesh and Gauthier, David and Kunnath, Lalu and Solomon,
		  Steve and Morales, Osvaldo and Fontoura, Marcus and
		  Bianchini, Ricardo},
  month		= jun,
  year		= {2021},
  note		= {ISSN: 2575-713X},
  keywords	= {Maintenance engineering, Production, Dynamic scheduling,
		  Flexible printed circuits, Index Terms—Datacenter power
		  management, power capping, Power system dynamics, Power
		  system management, redundant power, Safety, workload
		  availability},
  pages		= {319--332},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/Z7EX7AF9/9499818.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/2W543J83/Zhang et al. -
		  2021 - Flex High-Availability Datacenters With Zero
		  Rese.pdf:application/pdf}
}

@Article{	  zhang_hyperfaas_2019,
  title		= {{HyperFaaS}: {A} {Truly} {Elastic} {Serverless}
		  {Computing} {Framework}},
  language	= {en},
  journal	= {USENIX NSDI},
  author	= {Zhang, Jingyuan and Wang, Ao and Li, Min and Chen, Yuan
		  and Cheng, Yue},
  year		= {2019},
  pages		= {2},
  file		= {Zhang et al. - HyperFaaS A Truly Elastic Serverless
		  Computing
		  Fr.pdf:/home/prateeks/Zotero/storage/QR8A2WT8/Zhang et al.
		  - HyperFaaS A Truly Elastic Serverless Computing
		  Fr.pdf:application/pdf}
}

###Article{	  zhang_hyperfaas_2019,
  title		= {{HyperFaaS}: {A} {Truly} {Elastic} {Serverless}
		  {Computing} {Framework}},
  language	= {en},
  journal	= {USENIX NSDI},
  author	= {Zhang, Jingyuan and Wang, Ao and Li, Min and Chen, Yuan
		  and Cheng, Yue},
  year		= {2019},
  pages		= {2},
  abstract	= {Poster.},
  file		= {Zhang et al. - HyperFaaS A Truly Elastic Serverless
		  Computing
		  Fr.pdf:/home/prateeks/Zotero/storage/QR8A2WT8/Zhang et al.
		  - HyperFaaS A Truly Elastic Serverless Computing
		  Fr.pdf:application/pdf}
}

@Article{	  zhang_maximizing_2016,
  title		= {Maximizing {Performance} {Under} a {Power} {Cap}: {A}
		  {Comparison} of {Hardware}, {Software}, and {Hybrid}
		  {Techniques}},
  abstract	= {Power and thermal dissipation constrain multicore
		  performance scaling. Modern processors are built such that
		  they could sustain damaging levels of power dissipation,
		  creating a need for systems that can implement processor
		  power caps. A particular challenge is developing systems
		  that can maximize performance within a power cap, and
		  approaches have been proposed in both software and
		  hardware. Software approaches are ﬂexible, allowing
		  multiple hardware resources to be coordinated for maximum
		  performance, but software is slow, requiring a long time to
		  converge to the power target. In contrast, hardware power
		  capping quickly converges to the the power cap, but only
		  manages voltage and frequency, limiting its potential
		  performance.},
  language	= {en},
  journal	= {ASPLOS},
  author	= {Zhang, Huazhe and Hoffmann, Henry},
  year		= {2016},
  pages		= {15},
  file		= {Zhang and Hoffmann - Maximizing Performance Under a Power
		  Cap A
		  Compar.pdf:/home/prateeks/Zotero/storage/9T7MWA9H/Zhang and
		  Hoffmann - Maximizing Performance Under a Power Cap A
		  Compar.pdf:application/pdf}
}

###Article{	  zhang_maximizing_2016,
  title		= {Maximizing {Performance} {Under} a {Power} {Cap}: {A}
		  {Comparison} of {Hardware}, {Software}, and {Hybrid}
		  {Techniques}},
  abstract	= {Power and thermal dissipation constrain multicore
		  performance scaling. Modern processors are built such that
		  they could sustain damaging levels of power dissipation,
		  creating a need for systems that can implement processor
		  power caps. A particular challenge is developing systems
		  that can maximize performance within a power cap, and
		  approaches have been proposed in both software and
		  hardware. Software approaches are ﬂexible, allowing
		  multiple hardware resources to be coordinated for maximum
		  performance, but software is slow, requiring a long time to
		  converge to the power target. In contrast, hardware power
		  capping quickly converges to the the power cap, but only
		  manages voltage and frequency, limiting its potential
		  performance.},
  language	= {en},
  journal	= {ASPLOS},
  author	= {Zhang, Huazhe and Hoffmann, Henry},
  year		= {2016},
  pages		= {15},
  file		= {Zhang and Hoffmann - Maximizing Performance Under a Power
		  Cap A
		  Compar.pdf:/home/prateeks/Zotero/storage/9T7MWA9H/Zhang and
		  Hoffmann - Maximizing Performance Under a Power Cap A
		  Compar.pdf:application/pdf}
}

@InProceedings{	  zhang_narrowing_2019,
  address	= {Santa Cruz, CA, USA},
  title		= {Narrowing the {Gap} {Between} {Serverless} and its {State}
		  with {Storage} {Functions}},
  isbn		= {978-1-4503-6973-2},
  url		= {http://dl.acm.org/citation.cfm?doid=3357223.3362723},
  doi		= {10.1145/3357223.3362723},
  abstract	= {Serverless computing has gained attention due to its
		  fine-grained provisioning, large-scale multi-tenancy, and
		  on-demand scaling. However, it also forces applications to
		  externalize state in remote storage, adding substantial
		  overheads. To fix this “data shipping problem” we built
		  Shredder, a low-latency multi-tenant cloud store that
		  allows small units of computation to be performed directly
		  within storage nodes. Storage tenants provide Shredder with
		  JavaScript functions (or WebAssembly programs), which can
		  interact directly with data without moving them over the
		  network. The key challenge in Shredder is safely isolating
		  thousands of tenant storage functions while minimizing data
		  interaction costs. Shredder uses a unique approach where
		  its data store and networking paths are implemented in
		  native code to ensure performance, while isolated tenant
		  functions interact with data using a V8-specific
		  intermediate representation that avoids expensive
		  cross-protectiondomain calls and data copying. As a result,
		  Shredder can execute 4 million remotely-invoked tenant
		  functions per second spread over thousands of tenants with
		  median and 99th-percentile response latencies of less than
		  50 µs and 500 µs, respectively. Our evaluation shows that
		  Shredder achieves a 14\% to 78\% speedup against
		  conventional remote storage when fetching items with just
		  one to three data dependencies between them. We also
		  demonstrate Shredder’s effectiveness in accelerating
		  data-intensive applications, including a k-hop query on
		  social graphs that shows orders of magnitude gain.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {ACM} {Symposium} on {Cloud}
		  {Computing} - {SoCC} '19},
  publisher	= {ACM Press},
  author	= {Zhang, Tian and Xie, Dong and Li, Feifei and Stutsman,
		  Ryan},
  year		= {2019},
  pages		= {1--12},
  file		= {Zhang et al. - 2019 - Narrowing the Gap Between Serverless
		  and its
		  State.pdf:/home/prateeks/Zotero/storage/JUAWARDJ/Zhang et
		  al. - 2019 - Narrowing the Gap Between Serverless and its
		  State.pdf:application/pdf}
}

###InProceedings{ zhang_narrowing_2019,
  address	= {Santa Cruz, CA, USA},
  title		= {Narrowing the {Gap} {Between} {Serverless} and its {State}
		  with {Storage} {Functions}},
  isbn		= {978-1-4503-6973-2},
  url		= {http://dl.acm.org/citation.cfm?doid=3357223.3362723},
  doi		= {10.1145/3357223.3362723},
  abstract	= {Serverless computing has gained attention due to its
		  fine-grained provisioning, large-scale multi-tenancy, and
		  on-demand scaling. However, it also forces applications to
		  externalize state in remote storage, adding substantial
		  overheads. To fix this “data shipping problem” we built
		  Shredder, a low-latency multi-tenant cloud store that
		  allows small units of computation to be performed directly
		  within storage nodes. Storage tenants provide Shredder with
		  JavaScript functions (or WebAssembly programs), which can
		  interact directly with data without moving them over the
		  network. The key challenge in Shredder is safely isolating
		  thousands of tenant storage functions while minimizing data
		  interaction costs. Shredder uses a unique approach where
		  its data store and networking paths are implemented in
		  native code to ensure performance, while isolated tenant
		  functions interact with data using a V8-specific
		  intermediate representation that avoids expensive
		  cross-protectiondomain calls and data copying. As a result,
		  Shredder can execute 4 million remotely-invoked tenant
		  functions per second spread over thousands of tenants with
		  median and 99th-percentile response latencies of less than
		  50 µs and 500 µs, respectively. Our evaluation shows that
		  Shredder achieves a 14\% to 78\% speedup against
		  conventional remote storage when fetching items with just
		  one to three data dependencies between them. We also
		  demonstrate Shredder’s effectiveness in accelerating
		  data-intensive applications, including a k-hop query on
		  social graphs that shows orders of magnitude gain.},
  language	= {en},
  urldate	= {2020-01-10},
  booktitle	= {Proceedings of the {ACM} {Symposium} on {Cloud}
		  {Computing} - {SoCC} '19},
  publisher	= {ACM Press},
  author	= {Zhang, Tian and Xie, Dong and Li, Feifei and Stutsman,
		  Ryan},
  year		= {2019},
  pages		= {1--12},
  file		= {Zhang et al. - 2019 - Narrowing the Gap Between Serverless
		  and its
		  State.pdf:/home/prateeks/Zotero/storage/JUAWARDJ/Zhang et
		  al. - 2019 - Narrowing the Gap Between Serverless and its
		  State.pdf:application/pdf}
}

@InProceedings{	  zhang_podd_2019,
  address	= {Denver Colorado},
  title		= {\textit{{PoDD}}: power-capping dependent distributed
		  applications},
  isbn		= {978-1-4503-6229-0},
  shorttitle	= {\textit{{PoDD}}},
  url		= {https://dl.acm.org/doi/10.1145/3295500.3356174},
  doi		= {10.1145/3295500.3356174},
  abstract	= {Power budgeting (or capping) has become essential for
		  large-scale computing installations. Meanwhile, as these
		  systems scale out, they can concurrently execute dependent
		  applications that were previously processed serially. Such
		  application coupling reduces IO traffic and overall time to
		  completion as the applications now communicate at runtime
		  instead of through disk. Coupled applications are predicted
		  to be a major workload for future exascale supercomputers;
		  e.g., scientific simulations will execute concurrently with
		  in situ analysis. One critical challenge for power
		  budgeting systems is implementing power capping for coupled
		  applications while still achieving high performance.
		  Existing approaches on power capping coupled workloads,
		  however, have major limitations including: (1) poor
		  practicality, due to dependence on offline application
		  profiling; and (2) limited optimization opportunity, as
		  they consider power reallocation on a strictly global level
		  (from node-to-node), without considering node-level
		  optimization opportunities.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the {International} {Conference} for {High}
		  {Performance} {Computing}, {Networking}, {Storage} and
		  {Analysis}},
  publisher	= {ACM},
  author	= {Zhang, Huazhe and Hoffmann, Henry},
  month		= nov,
  year		= {2019},
  pages		= {1--23},
  file		= {Zhang and Hoffmann - 2019 - PoDD power-capping dependent
		  distributed
		  a.pdf:/home/prateeks/Zotero/storage/H8P8B7UU/Zhang and
		  Hoffmann - 2019 - PoDD power-capping dependent distributed
		  a.pdf:application/pdf}
}

@Article{	  zhang_quantitative_2015,
  title		= {A {Quantitative} {Evaluation} of the {RAPL} {Power}
		  {Control} {System}},
  abstract	= {We evaluate Intel’s RAPL power control system, which
		  allows users to set a power limit and then tunes processor
		  behavior to respect that limit. We evaluate RAPL by setting
		  power limits and running a number of standard benchmarks.
		  We quantify RAPL along ﬁve metrics: stability, accuracy,
		  settling time, overshoot, and efﬁciency. The ﬁrst four
		  are standard measures for evaluating control systems. The
		  last recognizes that any power control approach should
		  deliver the highest possible performance achievable within
		  the power limit. Our results show that RAPL performs well
		  on the four standard metrics, but some benchmarks fail to
		  achieve maximum performance. At high power limits, the
		  average performance is within 90\% of optimal. At middle
		  power limits, it is 86\% of optimal. At low power limits,
		  the average performance is less than 65\% of optimal.},
  language	= {en},
  journal	= {Feedback computing},
  author	= {Zhang, Huazhe and Hoffmann, Henry},
  year		= {2015},
  pages		= {6},
  file		= {Zhang and Hoffmann - A Quantitative Evaluation of the RAPL
		  Power
		  Contro.pdf:/home/prateeks/Zotero/storage/ENYXQDU9/Zhang and
		  Hoffmann - A Quantitative Evaluation of the RAPL Power
		  Contro.pdf:application/pdf}
}

@InProceedings{	  zhang_red_2021,
  address	= {Virtual Event Hong Kong},
  title		= {Red {Alert} for {Power} {Leakage}: {Exploiting} {Intel}
		  {RAPL}-{Induced} {Side} {Channels}},
  isbn		= {978-1-4503-8287-8},
  shorttitle	= {Red {Alert} for {Power} {Leakage}},
  url		= {https://dl.acm.org/doi/10.1145/3433210.3437517},
  doi		= {10.1145/3433210.3437517},
  abstract	= {RAPL (Running Average Power Limit) is a hardware feature
		  introduced by Intel to facilitate power management. Even
		  though RAPL and its supporting software interfaces can
		  benefit power management significantly, they are
		  unfortunately designed without taking certain security
		  issues into careful consideration. In this paper, we
		  demonstrate that information leaked through RAPL-induced
		  side channels can be exploited to mount realistic attacks.
		  Specifically, we have constructed a new RAPL-based covert
		  channel using a single AVX instruction, which can
		  exfiltrate data across different boundaries (e.g., those
		  established by containers in software or even CPUs in
		  hardware); and, we have investigated the first RAPL-based
		  website fingerprinting technique that can identify visited
		  webpages with a high accuracy (up to 99\% in the case of
		  the regular network using a browser like Chrome or Safari,
		  and up to 81\% in the case of the anonymity network using
		  Tor). These two studies form a preliminary examination into
		  RAPL-imposed security implications. In addition, we discuss
		  some possible countermeasures.},
  language	= {en},
  urldate	= {2022-04-11},
  booktitle	= {Proceedings of the 2021 {ACM} {Asia} {Conference} on
		  {Computer} and {Communications} {Security}},
  publisher	= {ACM},
  author	= {Zhang, Zhenkai and Liang, Sisheng and Yao, Fan and Gao,
		  Xing},
  month		= may,
  year		= {2021},
  pages		= {162--175},
  file		= {Zhang et al. - 2021 - Red Alert for Power Leakage
		  Exploiting Intel
		  RAPL.pdf:/home/prateeks/Zotero/storage/NZNL3ARR/Zhang et
		  al. - 2021 - Red Alert for Power Leakage Exploiting Intel
		  RAPL.pdf:application/pdf}
}

@InProceedings{	  zhao2021understanding,
  title		= {Understanding, predicting and scheduling serverless
		  workloads under partial interference},
  author	= {Zhao, Laiping and Yang, Yanan and Li, Yiming and Zhou,
		  Xian and Li, Keqiu},
  booktitle	= {Proceedings of the International Conference for High
		  Performance Computing, Networking, Storage and Analysis},
  pages		= {1--15},
  year		= {2021}
}

@InProceedings{	  zhou2022aquatope,
  title		= {AQUATOPE: QoS-and-Uncertainty-Aware Resource Management
		  for Multi-stage Serverless Workflows},
  author	= {Zhou, Zhuangzhuang and Zhang, Yanqi and Delimitrou,
		  Christina},
  booktitle	= {Proceedings of the 28th ACM International Conference on
		  Architectural Support for Programming Languages and
		  Operating Systems, Volume 1},
  pages		= {1--14},
  year		= {2022}
}

@Misc{		  zhou_qos-aware_2022,
  title		= {{QoS}-{Aware} {Resource} {Management} for {Multi}-phase
		  {Serverless} {Workflows} with {Aquatope}},
  url		= {http://arxiv.org/abs/2212.13882},
  abstract	= {Multi-stage serverless applications, i.e., workflows with
		  many computation and I/O stages, are becoming increasingly
		  representative of FaaS platforms. Despite their advantages
		  in terms of fine-grained scalability and modular
		  development, these applications are subject to suboptimal
		  performance, resource inefficiency, and high costs to a
		  larger degree than previous simple serverless functions.},
  language	= {en},
  urldate	= {2023-01-09},
  publisher	= {arXiv},
  author	= {Zhou, Zhuangzhuang and Zhang, Yanqi and Delimitrou,
		  Christina},
  month		= dec,
  year		= {2022},
  note		= {arXiv:2212.13882 [cs]},
  keywords	= {Computer Science - Distributed, Parallel, and Cluster
		  Computing, Computer Science - Networking and Internet
		  Architecture},
  file		= {Zhou et al. - 2022 - QoS-Aware Resource Management for
		  Multi-phase
		  Serv.pdf:/home/prateeks/Zotero/storage/GGITJQFN/Zhou et al.
		  - 2022 - QoS-Aware Resource Management for Multi-phase
		  Serv.pdf:application/pdf}
}

@Article{	  zhuang2020comprehensive,
  title		= {A comprehensive survey on transfer learning},
  author	= {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi,
		  Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui
		  and He, Qing},
  journal	= {Proceedings of the IEEE},
  volume	= {109},
  number	= {1},
  pages		= {43--76},
  year		= {2020},
  publisher	= {IEEE}
}

@Article{	  zoha_non-intrusive_2012,
  title		= {Non-{Intrusive} {Load} {Monitoring} {Approaches} for
		  {Disaggregated} {Energy} {Sensing}: {A} {Survey}},
  volume	= {12},
  issn		= {1424-8220},
  shorttitle	= {Non-{Intrusive} {Load} {Monitoring} {Approaches} for
		  {Disaggregated} {Energy} {Sensing}},
  url		= {http://www.mdpi.com/1424-8220/12/12/16838},
  doi		= {10.3390/s121216838},
  abstract	= {Appliance Load Monitoring (ALM) is essential for energy
		  management solutions, allowing them to obtain
		  appliance-speciﬁc energy consumption statistics that can
		  further be used to devise load scheduling strategies for
		  optimal energy utilization. Fine-grained energy monitoring
		  can be achieved by deploying smart power outlets on every
		  device of interest; however it incurs extra hardware cost
		  and installation complexity. Non-Intrusive Load Monitoring
		  (NILM) is an attractive method for energy disaggregation,
		  as it can discern devices from the aggregated data acquired
		  from a single point of measurement. This paper provides a
		  comprehensive overview of NILM system and its associated
		  methods and techniques used for disaggregated energy
		  sensing. We review the state-of-the art load signatures and
		  disaggregation algorithms used for appliance recognition
		  and highlight challenges and future research directions.},
  language	= {en},
  number	= {12},
  urldate	= {2022-09-30},
  journal	= {Sensors},
  author	= {Zoha, Ahmed and Gluhak, Alexander and Imran, Muhammad and
		  Rajasegarar, Sutharshan},
  month		= dec,
  year		= {2012},
  pages		= {16838--16866},
  file		= {Zoha et al. - 2012 - Non-Intrusive Load Monitoring
		  Approaches for
		  Disag.pdf:/home/prateeks/Zotero/storage/K77DEA5G/Zoha et
		  al. - 2012 - Non-Intrusive Load Monitoring Approaches for
		  Disag.pdf:application/pdf}
}

@InProceedings{	  zuk_call_2022,
  title		= {Call {Scheduling} to {Reduce} {Response} {Time} of a
		  {FaaS} {System}},
  doi		= {10.1109/CLUSTER51413.2022.00031},
  abstract	= {In an overloaded FaaS cluster, individual worker nodes
		  strain under lengthening queues of requests. Although the
		  cluster might be eventually horizontally-scaled, adding a
		  new node takes dozens of seconds. As serving applications
		  are tuned for tail serving latencies, and these greatly
		  increase under heavier loads, the current workaround is
		  resource over-provisioning. In fact, even though a service
		  can withstand a steady load of, e.g., 70\% CPU utilization,
		  the autoscaler is triggered at, e.g., 30–40\% (thus the
		  service uses twice as many nodes as it would be needed). We
		  propose an alternative: a worker-level method handling
		  heavy load without increasing the number of nodes. FaaS
		  executions are not interactive, compared to, e.g., text
		  editors: end-users do not benefit from the CPU allocated to
		  processes often, yet for short periods. Inspired by
		  scheduling methods for High Performance Computing, we take
		  a radical step of replacing the classic OS preemption by
		  (1) queuing requests based on their historical
		  characteristics; (2) once a request is being processed,
		  setting its CPU limit to exactly one core (with no CPU
		  oversubscription). We extend OpenWhisk and measure the
		  efficiency of the proposed solutions using the SeBS
		  benchmark. In a loaded system, our method decreases the
		  average response time by a factor of 4. The improvement is
		  even higher for shorter requests, as the average stretch is
		  decreased by a factor of 18. This leads us to show that we
		  can provide better response-time statistics with 3 machines
		  compared to a 4-machine baseline.},
  booktitle	= {2022 {IEEE} {International} {Conference} on {Cluster}
		  {Computing} ({CLUSTER})},
  author	= {Zuk, Paweł and Przybylski, Bartłomiej and Rzadca,
		  Krzysztof},
  month		= sep,
  year		= {2022},
  note		= {ISSN: 2168-9253},
  keywords	= {cloud, FaaS, Function as a Service, High performance
		  computing, Measurement, Open Whisk, Processor scheduling,
		  Production, response time, scheduling, Sequential analysis,
		  server-less, stretch, Tail, Time-frequency analysis},
  pages		= {172--182},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/4NESPHFL/stamp.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/4WM5EUE7/Zuk et al. -
		  2022 - Call Scheduling to Reduce Response Time of a FaaS
		  .pdf:application/pdf}
}

###InProceedings{ zuk_call_2022,
  title		= {Call {Scheduling} to {Reduce} {Response} {Time} of a
		  {FaaS} {System}},
  doi		= {10.1109/CLUSTER51413.2022.00031},
  abstract	= {In an overloaded FaaS cluster, individual worker nodes
		  strain under lengthening queues of requests. Although the
		  cluster might be eventually horizontally-scaled, adding a
		  new node takes dozens of seconds. As serving applications
		  are tuned for tail serving latencies, and these greatly
		  increase under heavier loads, the current workaround is
		  resource over-provisioning. In fact, even though a service
		  can withstand a steady load of, e.g., 70\% CPU utilization,
		  the autoscaler is triggered at, e.g., 30–40\% (thus the
		  service uses twice as many nodes as it would be needed). We
		  propose an alternative: a worker-level method handling
		  heavy load without increasing the number of nodes. FaaS
		  executions are not interactive, compared to, e.g., text
		  editors: end-users do not benefit from the CPU allocated to
		  processes often, yet for short periods. Inspired by
		  scheduling methods for High Performance Computing, we take
		  a radical step of replacing the classic OS preemption by
		  (1) queuing requests based on their historical
		  characteristics; (2) once a request is being processed,
		  setting its CPU limit to exactly one core (with no CPU
		  oversubscription). We extend OpenWhisk and measure the
		  efficiency of the proposed solutions using the SeBS
		  benchmark. In a loaded system, our method decreases the
		  average response time by a factor of 4. The improvement is
		  even higher for shorter requests, as the average stretch is
		  decreased by a factor of 18. This leads us to show that we
		  can provide better response-time statistics with 3 machines
		  compared to a 4-machine baseline.},
  booktitle	= {2022 {IEEE} {International} {Conference} on {Cluster}
		  {Computing} ({CLUSTER})},
  author	= {Zuk, Paweł and Przybylski, Bartłomiej and Rzadca,
		  Krzysztof},
  month		= sep,
  year		= {2022},
  note		= {ISSN: 2168-9253},
  keywords	= {cloud, FaaS, Function as a Service, High performance
		  computing, Measurement, Open Whisk, Processor scheduling,
		  Production, response time, scheduling, Sequential analysis,
		  server-less, stretch, Tail, Time-frequency analysis},
  pages		= {172--182},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/4NESPHFL/stamp.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/4WM5EUE7/Zuk et al. -
		  2022 - Call Scheduling to Reduce Response Time of a FaaS
		  .pdf:application/pdf}
}

###InProceedings{ zuk_call_2022,
  title		= {Call {Scheduling} to {Reduce} {Response} {Time} of a
		  {FaaS} {System}},
  doi		= {10.1109/CLUSTER51413.2022.00031},
  abstract	= {In an overloaded FaaS cluster, individual worker nodes
		  strain under lengthening queues of requests. Although the
		  cluster might be eventually horizontally-scaled, adding a
		  new node takes dozens of seconds. As serving applications
		  are tuned for tail serving latencies, and these greatly
		  increase under heavier loads, the current workaround is
		  resource over-provisioning. In fact, even though a service
		  can withstand a steady load of, e.g., 70\% CPU utilization,
		  the autoscaler is triggered at, e.g., 30–40\% (thus the
		  service uses twice as many nodes as it would be needed). We
		  propose an alternative: a worker-level method handling
		  heavy load without increasing the number of nodes. FaaS
		  executions are not interactive, compared to, e.g., text
		  editors: end-users do not benefit from the CPU allocated to
		  processes often, yet for short periods. Inspired by
		  scheduling methods for High Performance Computing, we take
		  a radical step of replacing the classic OS preemption by
		  (1) queuing requests based on their historical
		  characteristics; (2) once a request is being processed,
		  setting its CPU limit to exactly one core (with no CPU
		  oversubscription). We extend OpenWhisk and measure the
		  efficiency of the proposed solutions using the SeBS
		  benchmark. In a loaded system, our method decreases the
		  average response time by a factor of 4. The improvement is
		  even higher for shorter requests, as the average stretch is
		  decreased by a factor of 18. This leads us to show that we
		  can provide better response-time statistics with 3 machines
		  compared to a 4-machine baseline.},
  booktitle	= {2022 {IEEE} {International} {Conference} on {Cluster}
		  {Computing} ({CLUSTER})},
  author	= {Zuk, Paweł and Przybylski, Bartłomiej and Rzadca,
		  Krzysztof},
  month		= sep,
  year		= {2022},
  note		= {ISSN: 2168-9253},
  keywords	= {cloud, FaaS, Function as a Service, High performance
		  computing, Measurement, Open Whisk, Processor scheduling,
		  Production, response time, scheduling, Sequential analysis,
		  server-less, stretch, Tail, Time-frequency analysis},
  pages		= {172--182},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/4NESPHFL/stamp.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/4WM5EUE7/Zuk et al. -
		  2022 - Call Scheduling to Reduce Response Time of a FaaS
		  .pdf:application/pdf}
}

@InProceedings{	  zuk_scheduling_2020,
  title		= {Scheduling {Methods} to {Reduce} {Response} {Latency} of
		  {Function} as a {Service}},
  doi		= {10.1109/SBAC-PAD49847.2020.00028},
  abstract	= {Function as a Service (FaaS) permits cloud customers to
		  deploy to cloud individual functions, in contrast to
		  complete virtual machines or Linux containers. All major
		  cloud providers offer FaaS products (Amazon Lambda, Google
		  Cloud Functions, Azure Serverless); there are also popular
		  open-source implementations (Apache OpenWhisk) with
		  commercial offerings (Adobe I/O Runtime, IBM Cloud
		  Functions). A new feature of FaaS is function composition:
		  a function may (sequentially) call another function, which,
		  in turn, may call yet another function - forming a chain of
		  invocations. From the perspective of the infrastructure, a
		  composed FaaS is less opaque than a virtual machine or a
		  container. We show that this additional information enables
		  the infrastructure to reduce the response latency. In
		  particular, knowing the sequence of future invocations, the
		  infrastructure can schedule these invocations along with
		  environment preparation. We model resource management in
		  FaaS as a scheduling problem combining (1) sequencing of
		  invocations, (2) deploying execution environments on
		  machines, and (3) allocating invocations to deployed
		  environments. For each aspect, we propose heuristics. We
		  explore their performance by simulation on a range of
		  synthetic workloads. Our results show that if the setup
		  times are long compared to invocation times, algorithms
		  that use information about the composition of functions
		  consistently outperform greedy, myopic algorithms, leading
		  to significant decrease in response latency.},
  booktitle	= {2020 {IEEE} 32nd {International} {Symposium} on {Computer}
		  {Architecture} and {High} {Performance} {Computing}
		  ({SBAC}-{PAD})},
  author	= {Zuk, Pawel and Rzadca, Krzysztof},
  month		= sep,
  year		= {2020},
  note		= {ISSN: 2643-3001},
  keywords	= {Cloud computing, Containers, FAA, Resource management,
		  Schedules, scheduling, workflow, setup time,
		  function-as-a-service, serverless, Standards, Task
		  analysis},
  pages		= {132--140},
  file		= {IEEE Xplore Abstract
		  Record:/home/prateeks/Zotero/storage/QVPC82P3/stamp.html:text/html;IEEE
		  Xplore Full Text
		  PDF:/home/prateeks/Zotero/storage/2WVAPFPD/Zuk and Rzadca -
		  2020 - Scheduling Methods to Reduce Response Latency of
		  F.pdf:application/pdf}
}


@inproceedings{juan_reducing_2023,
	address = {Atlanta, GA, USA},
	title = {Reducing the {Cost} of {GPU} {Cold} {Starts} in {Serverless} {Deep} {Learning} {Inference} {Serving}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66545-381-3},
	url = {https://ieeexplore.ieee.org/document/10150381/},
	doi = {10.1109/PerComWorkshops56833.2023.10150381},
	abstract = {The rapid growth of Deep Learning (DL) has led to increasing demand for DL-as-a-Service. In this paradigm, DL inferences are served on-demand through a serverless cloud provider, which manages the scaling of hardware resources to satisfy dynamic workloads. This is enticing to businesses due to lower infrastructure management costs compared to dedicated on-site hosting. However, current serverless systems suffer from long cold starts where requests are queued until a server can be initialized with the DL model, which is especially problematic due to large DL model sizes. In addition, low-latency demands such as in real-time fraud detection and algorithmic trading cause long inferences in CPU-only systems to violate deadlines. To tackle this, current systems rely on over-provisioning expensive GPU resources to meet low-latency requirements, thus increasing the total cost of ownership for cloud service providers.},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {2023 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} and other {Affiliated} {Events} ({PerCom} {Workshops})},
	publisher = {IEEE},
	author = {Juan, Justin San and Wong, Bernard},
	month = mar,
	year = {2023},
	pages = {225--230},
	file = {Juan and Wong - 2023 - Reducing the Cost of GPU Cold Starts in Serverless.pdf:/home/prateeks/Zotero/storage/WTJ484WS/Juan and Wong - 2023 - Reducing the Cost of GPU Cold Starts in Serverless.pdf:application/pdf},
}


@inproceedings{satzke_efficient_2020,
	address = {Virtual Event Sweden},
	title = {Efficient {GPU} {Sharing} for {Serverless} {Workflows}},
	isbn = {978-1-4503-8388-2},
	url = {https://dl.acm.org/doi/10.1145/3452413.3464785},
	doi = {10.1145/3452413.3464785},
	abstract = {Serverless computing has emerged as a new cloud computing paradigm, where an application consists of individual functions that can be separately managed and executed. However, the function development environment of all serverless computing frameworks at present is CPU-based. In this paper, we propose to extend the open-sourced KNIX high-performance serverless framework so that it can execute functions on shared GPU cluster resources. We have evaluated the performance impacts on the extended KNIX system by measuring overheads and penalties incurred using different deep learning frameworks.},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {Proceedings of the 1st {Workshop} on {High} {Performance} {Serverless} {Computing}},
	publisher = {ACM},
	author = {Satzke, Klaus and Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Beck, Andre and Aditya, Paarijaat and Vanga, Manohar and Hilt, Volker},
	month = jun,
	year = {2020},
	pages = {17--24},
	file = {Satzke et al. - 2020 - Efficient GPU Sharing for Serverless Workflows.pdf:/home/prateeks/Zotero/storage/B7IM62P2/Satzke et al. - 2020 - Efficient GPU Sharing for Serverless Workflows.pdf:application/pdf},
}


@inproceedings{kim_gpu_2018,
	address = {Cambridge},
	title = {{GPU} {Enabled} {Serverless} {Computing} {Framework}},
	isbn = {978-1-5386-4975-6},
	url = {https://ieeexplore.ieee.org/document/8374513/},
	doi = {10.1109/PDP2018.2018.00090},
	abstract = {A new form of cloud computing, serverless computing, is drawing attention as a new way to design micro-services architectures. In a serverless computing environment, services are developed as service functional units. The function development environment of all serverless computing framework at present is CPU based. In this paper, we propose a GPU-supported serverless computing framework that can deploy services faster than existing serverless computing framework using CPU. Our core approach is to integrate the open source serverless computing framework with NVIDIA-Docker and deploy services based on the GPU support container. We have developed an API that connects the open source framework to the NVIDIA-Docker and commands that enable GPU programming. In our experiments, we measured the performance of the framework in various environments. As a result, developers who want to develop services through the framework can deploy high-performance micro services and developers who want to run deep learning programs without a GPU environment can run code on remote GPUs with little performance degradation.},
	language = {en},
	urldate = {2024-05-17},
	booktitle = {2018 26th {Euromicro} {International} {Conference} on {Parallel}, {Distributed} and {Network}-based {Processing} ({PDP})},
	publisher = {IEEE},
	author = {Kim, Jaewook and Jun, Tae Joon and Kang, Daeyoun and Kim, Dohyeun and Kim, Daeyoung},
	month = mar,
	year = {2018},
	pages = {533--540},
	file = {Kim et al. - 2018 - GPU Enabled Serverless Computing Framework.pdf:/home/prateeks/Zotero/storage/24TF22S8/Kim et al. - 2018 - GPU Enabled Serverless Computing Framework.pdf:application/pdf},
}


@misc{sage_zhao_towards_2024,
	title = {Towards {Fast} {Setup} and {High} {Throughput} of {GPU} {Serverless} {Computing}},
	url = {http://arxiv.org/abs/2404.14691},
	abstract = {Integrating GPUs into serverless computing platforms is crucial for improving efficiency. However, existing solutions for GPU-enabled serverless computing platforms face two significant problems due to coarse-grained GPU management: long setup time and low function throughput. To address these issues, we propose SAGE, a GPU serverless framework with fast setup and high throughput. First, based on the data knowability of GPU function ahead of actual execution, SAGE first devises the parallelized function setup mechanism, which parallelizes the data preparation and context creation. In this way, SAGE achieves fast setup of GPU function invocations.Second, SAGE further proposes the sharing-based memory management mechanism, which shares the read-only memory and context memory across multiple invocations of the same function. The memory sharing mechanism avoids repeated data preparation and then unnecessary data-loading contention. As a consequence, the function throughput could be improved. Our experimental results show that SAGE reduces function duration by 11.3× and improves function density by 1.22× compared to the state-of-the-art serverless platform.},
	language = {en},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Zhao, Han and Cui, Weihao and Chen, Quan and Zhang, Shulai and Li, Zijun and Leng, Jingwen and Li, Chao and Zeng, Deze and Guo, Minyi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14691 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Zhao et al. - 2024 - Towards Fast Setup and High Throughput of GPU Serv.pdf:/home/prateeks/Zotero/storage/RWSUULVE/Zhao et al. - 2024 - Towards Fast Setup and High Throughput of GPU Serv.pdf:application/pdf},
}

@inproceedings{tgs_wu2023transparent,
  title={Transparent $\{$GPU$\}$ sharing in container clouds for deep learning workloads},
  author={Wu, Bingyang and Zhang, Zili and Bai, Zhihao and Liu, Xuanzhe and Jin, Xin},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={69--85},
  year={2023}
}

@inproceedings{gujarati2020serving,
  title={Serving $\{$DNNs$\}$ like clockwork: Performance predictability from the bottom up},
  author={Gujarati, Arpan and Karimi, Reza and Alzayat, Safya and Hao, Wei and Kaufmann, Antoine and Vigfusson, Ymir and Mace, Jonathan},
  booktitle={14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages={443--462},
  year={2020}
}

@misc{mo_optimal_2024,
	title = {Optimal {Resource} {Efficiency} with {Fairness} in {Heterogeneous} {GPU} {Clusters}},
	url = {http://arxiv.org/abs/2403.18545},
	abstract = {Ensuring the highest training throughput to maximize resource efficiency, while maintaining fairness among users, is critical for deep learning (DL) training in heterogeneous GPU clusters. However, current DL schedulers provide only limited fairness properties and suboptimal training throughput, impeding tenants from effectively leveraging heterogeneous resources. The underlying design challenge stems from inherent conflicts between efficiency and fairness properties. In this paper, we introduce OEF, a new resource allocation framework specifically developed for achieving optimal resource efficiency and ensuring diverse fairness properties in heterogeneous GPU clusters. By integrating resource efficiency and fairness within a global optimization framework, OEF is capable of providing users with maximized overall efficiency, as well as various guarantees of fairness, in both cooperative and non-cooperative environments. We have implemented OEF in a cluster resource manager and conducted large-scale experiments, showing that OEF can improve the overall training throughput by up to 32\% while improving fairness compared to state-of-the-art heterogeneity-aware schedulers.},
	language = {en},
	urldate = {2024-05-17},
	publisher = {arXiv},
	author = {Mo, Zizhao and Xu, Huanle and Lau, Wing Cheong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18545 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Mo et al. - 2024 - Optimal Resource Efficiency with Fairness in Heter.pdf:/home/prateeks/Zotero/storage/HKGF35D6/Mo et al. - 2024 - Optimal Resource Efficiency with Fairness in Heter.pdf:application/pdf},
}

@inproceedings{wang2021faasnet,
  title={$\{$FaaSNet$\}$: Scalable and fast provisioning of custom serverless container runtimes at alibaba cloud function compute},
  author={Wang, Ao and Chang, Shuai and Tian, Huangshi and Wang, Hongqi and Yang, Haoran and Li, Huiba and Du, Rui and Cheng, Yue},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={443--457},
  year={2021}
}
