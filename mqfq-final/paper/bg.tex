\vspace*{\subsecspace}
\section{Background}
\vspace*{\subsecspace}
\label{sec:bg}

\subsection{Functions as a Service}

Serverless control planes execute snippets of user code inside isolated sandboxes, handling data movement and resource allocation needs to service them.
The creation of such function-specific containers costs several seconds on the critical execution path, and is known as a \quotes{cold start}.
To amortize the sandbox startup time, they are kept resident in memory to execute subsequent invocations in a \quotes{container pool}.
A simple one-sized fits all design of assigning an entire GPU to each container breaks down when trying to handle heterogeneous FaaS~\cite{shahrad2020serverless} workloads.
% It results in high container churn and cold starts, constantly serving invocations with no resident container increases latency significantly. 
One cannot have a viable container pool when the limited GPUs on a machine are each assigned to a single container.
This results in constant removal of containers to serve new invocations, causing high numbers of cold starts.
Any idle function would also result in significant underutilization of resources -- a major waste of hardware from the provider's perspective.

\begin{comment}
Serverless providers have users upload their function code to their systems, and execute that code on demand when a trigger or event occurs~\cite{serverless-cacm-21, aws-lambda, google-functions,azure-functions}.
The FaaS system routes the invocation to a worker node and creates a sandbox execution environment for it.
These isolation mechanisms can vary from OS containers like Docker~\cite{docker-main}, VMs, or even protected language runtimes~\cite{shillaker2020faasm}. 
This creation also incorporates significant setup including networking, code/library initialization, and function data dependencies.
Such initialization has been named a \quotes{cold start}, and can take several hundred milliseconds or more.
Systems mitigate this overhead by re-using sandboxes for future executions, leaving them resident in memory for some period of time called a \quotes{keep-alive} time~\cite{du2020catalyzer, faascache-asplos21}.

% They can also be passed arguments either through the provider, or downloaded during execution via cloud storage, causing more variance in characteristics.
% Chains of functions can be created in the FaaS system to form larger applications with either direct links between functions or allow several to run in parallel.

A key feature of FaaS is its ability to scale up and down with demand, where providers create more containers to execute concurrent invocations if they have large numbers of requests.
Inversely, if the requests decrease, containers will be removed to not waste resources, even scaling to zero should a function become idle.
Functions hold arbitrary user code, and naturally exhibit highly heterogeneous runtime characteristics such as execution time and memory usage~\cite{shahrad2020serverless}.
Users are only billed for execution time, so providers must choose between keeping containers available to reduce latency or remove them to reduce costs.
The one-sized fits all design of assigning an entire GPU to each container fails when trying to handle the complex workload.
It results in high container churn and cold starts, constantly serving invocations with no resident container increases latency significantly. 
% Supporting these unique aspects of FaaS workloads requires us to move away from assigning an entire GPU to a container
% Assigning an entire GPU to a container
% This scalability does not fit with the limited resources available per-GPU when a single function can hog the entire device.
\end{comment}

\subsection{GPU Support for Functions}

\begin{table}
  \caption{The functions in Tables~\ref{tab:gpu-attatch} and~\ref{tab:gpu-cpu} come from several sources. 
  They are a subset of the ones we ported to \sysname.}
  \label{tab:fun-list}
  \vspace{\captionspace}
  \begin{tabular}{c|p{6cm}}
    \hline
    Collection & Functions \\
    \hline
    ML~\cite{kim2019functionbench} & Imagenet, Roberta \\ %  RNN, Squeezenet
    PyHPC~\cite{pyhpc-bench} & FFT, Isoneural \\ % Eos, 
    Rodinia~\cite{che2009rodinia} & Lud, Myocyte, Needle, Pathfinder \\ % , Srad
    Other & Ffmpeg \\
  \end{tabular}
  \vspace{-0.4cm}
\end{table}

\begin{table}
  \caption{Functions' get great performance benefits from running on GPU over CPU. All times are in seconds.}
  \label{tab:gpu-cpu}
  \vspace{\captionspace}
  % \begin{center}
  \begin{tabular}{lccc}
    \hline
    Function & GPU & CPU & GPU Speedup \\
    \hline
  Imagenet & 2.253 & 5.477 & 2.44x \\
  % Squeezenet & 1.168 & 1.099 & 0.95x \\
  % RNN & 0.366 & 0.048 & 0.14x \\
  Roberta & 0.268 & 5.162 & 19.26x \\
  Ffmpeg & 4.483 & 32.997 & 7.37x \\
  FFT & 0.897 & 11.584 & 12.92x \\
  % Eos & 0.017 & 0.045 & 2.66x \\
  Isoneural & 0.026 & 0.501 & 19.57x \\
  % Lavamd & 1.989 & 15.199 & 7.64x \\
  Lud & 2.050 & 70.915 & 34.6x \\
  Myocyte & 2.784 & 39.277 & 14.11x \\
  Needle & 1.979 & 144.639 & 73.09x \\
  Pathfinder & 1.472 & 134.358 & 91.3x \\
  % Srad & 3.893 & 119.759 & 30.77x \\
  \end{tabular}
% \end{center}
  \vspace{-0.4cm}
\end{table}

% One of the first works to integrate GPUs is~\cite{naranjo2020accelerated} using rCUDA~\cite{duato2010rcuda} to connect disaggregated GPUs in a cluster to containers.
% It only looks at the performance affect on individual function invocations, not exploring the resource management, queuing, or heterogeneous load issues in FaaS.
% DGSF~\cite{fingler2022dgsf} combines disaggregation and API remoting to improve utilization, and does load-balancing between GPUs on the compute node.

Many applications that have adopted FaaS for its on-demand scaling will also benefit from acceleration, as shown by Table~\ref{tab:gpu-cpu}.
Machine learning inference requires iterations of matrix multiplication and transformations, common tasks for which modern GPUs are designed around.
Imagenet and Roberta respectively see a 3x and 20x reduction in latency thanks to acceleration, common numbers for ML improvements.
Video encoding via \emph{ffmpeg} has become the a popular task on AWS Lambda, which can leverage specialized hardware found in most GPUs to get a minimum 7x speedup.
Scientific computing has started to be used in FaaS~\cite{john_sweep_2019,mocskos_faaster_2018,werner2018serverless,shankar2020serverless}, but needs acceleration to quickly run expensive algorithms.
One such ubiquitious algorithm, Fixed-Fourier-Transform (FFT), sees 13x improvement, and larger, complete applications (Needle, Pathfinder, etc.) are 80-90x faster with acceleration.
These are performance gains and application opportunities which are currently being left on the table, and which we are the first to enable.

\begin{comment}
Using disaggregated GPUs~\cite{naranjo2020accelerated,fingler2022dgsf} is a common tactic to maximize device utilization from bursty FaaS functions.
The interposition layer virtualizes the device and forwards requests to the remote GPU, timesharing resources between clients.
These works fail to enable device concurrency between several functions, and do not consider queuing for either fairness between functions or encouraging device locality.

The common approach for ML inference~\cite{pemberton2022kernel,ng2023paella} has the control plane break apart tasks into GPU and CPU-specific sections, and schedules them internally.
While achieving high utilization and throughput, they abandon the black-box nature of serverless and do not work with different workload classes.
Other work~\cite{gu2023fast} has profiled inference tasks to learn their compute usage to schedule concurrent invocations without interference.
It does not resolve the memory exhaustion problem, and cannot predict compute usage of other functions which can vary with each input.
% The latter style violates our intended black-boxprinciples for orchestration, and the former breaks down with the new workloads who's compute and memory usage can vary with each input.
\end{comment}

% Software level scheduling of GPU kernels (device compute requests from programs) has been explored by several works, and requires GPU code be extracted separately and not be in a function with CPU code.
% Kernel-as-a-Service~\cite{pemberton2022kernel} treats individual kernels as first-class serverless functions, managing kernel scheduling and memory allocations directly from the platform.
% %  and uses DAGs to intersperse them with host code
% Paella~\cite{ng2023paella} also breaks apart model inference tasks into CUDA kernel launches to enable control plane software scheduling of GPUs and their resource management.
% Both of these automatically move memory off of the device when kernels are done, and don't have to worry about applications holding onto device memory when idle.

% FaST-GShare~\cite{gu2023fast} profiles ML workloads to monitor how much of the GPU it utilizes, then uses this information to schedule inference tasks on GPUs to maximize utilization.
% ML inference tasks have fixed sized memory and kernel usages (known tensor sizes, etc.) and this is an effective approach.
% Other applications can have arbitrary and changing requirements, especially when one considers that function arguments are the main determiner of resource usage, so this idea breaks down when shifting to a black-box application approach. 


\subsection{GPU Sharing Mechanisms}
%TODO: Integrate with previous (gpu support for functions)

\mhead{Spatial Sharing}
Nvidia has created several technologies for sharing their GPUs that split compute and memory between clients.
% via virtualization that vary with different hardware generations.
% Virtualization passthrough of vGPUs~\cite{nvidia-passthrough} uses direct device assignment common to hardware virtualization, where a guest VM has total control of the device with no sharing.
Nvidia MIG~\cite{nvidia-mig} (Multi-Instance GPU) pre-partitions device resources at  manufacturing time, and one or more of these virtualized GPU partitions can be assigned to a VM or container via direct device assignment.
Once slice(s) of the GPU are assigned, the only way to adjust allocations is to cold start a new container.
Fixed sizes place arbitrary restrictions on the dynamic resource allocations we want to be able to give to functions.
Over-allocation causes fragmentation, and the opposite approach will reduce performance or could break functions due to insufficient resources.
% Assigned partitions can lead to underutilization due to fragmentation from poor placement and function idle periods.
Idle functions cause immediate underutilization, which could be alleviated via temporal sharing complicating fixed hardware partition management even further.
%  and fragmentation when a function does not use all the resources it has been asigned.
%  and place arbitrary limits on resource allocations we can make to functions.
% Multiple containers could temporally share GPU partitions to improve utilization, but this is an unnecessary complication of userspace temporal sharing.
% The partitions sizes are fixed as manufacturing time, and lead to resource fragmentation and underutilization.

Multi-Process Service~\cite{nvidia-mps} (MPS) allows multiple processes to make share the device concurrently and has been proposed for FaaS~\cite{gu2023fast}.
An MPS server and the hardware device perform resource partitioning based on configuration at application start time.
MPS is explicitly designed to let cooperative processes share GPU resources, and documentation specifies that it is intended to work with OpenMP/MPI applications.
If any process encounters a critical error, all processes connected to the MPS server will crash, meaning one faulty serverless function will break all functions using that GPU.%, regardless of the function it is for.
% The amount of GPU memory and compute usable by a process can be specified at process launch time, and is fixed thereafter.
% Importantly, support for MPS features varies between hardware versions, and only the newest devices have resource management features.
% Both spatial approaches have drawbacks that render them unsuitable for FaaS.

% Resource partitioning, a new container must be spun up with the large overhead that entails.
% MPS does not enable oversubscribing limited device memory, and the inability to truly isolate processes from one another makes it a poor candidate for FaaS.
% Both spatial sharing schemes do not allow variable resource allocations, requiring a container cold start to change allocations.
% Neither enable oversubscription of limited device memory, 

\mhead{Temporal Sharing}
% Several software things.
There has been much work by others in the virtualization of accelerators~\cite{duato2010rcuda, yu2019automatic, hong2017gpu} to allow for temporal sharing.
Virtualization allows total control over device resources and scheduling, but imposes significant performance overheads~\cite{yu2017full}.
These approaches~\cite{yu2019automatic, hong2017gpu} must duplicate application state in host memory, and allocate/de-allocate all resources when switching between applications.
% , and most target disaggregation~\cite{duato2010rcuda,fingler2022dgsf} or API imposition~\cite{yu2019automatic}.
% These support memory and compute sharing 


% Actual scheduling of code on GPUs happens at several levels of the orchestration stack.
The GPU driver itself accepts individual compute \emph{kernels} from applications and has a device-internal scheduler which maps them to available compute blocks as they arrive.
% Replacement schedulers~\cite{mccoy2024gallatin} have been devised, as well as systems that schedule kernel launches from inside an application~\cite{strati2024orion}.
Kernel schedulers for domain-specific optimizations~\cite{strati2024orion,chen2017effisha,kim2020navigator,gu2023fast} have been designed to coordinate kernels from several applications to improve on device scheduler performance.
These operate only on compute kernels to prevent contention and assume active concurrent workloads are coordinated elsewhere to not exceed the device memory.
% At the highest level, a process or job can be scheduled on a specific GPU and allowed to run to completion, accepting the application's arbitrary kernel launches. 
% As many workloads can't consume all available GPU resources, some schedulers control both kernel launches and device memory.
Other schedulers~\cite{ng2023paella,pemberton2022kernel,strati2024orion} include manipulating and oversubscribing memory as they schedule kernel launches, but they violate the black-box principles we target by breaking apart the application to have tighter control.
Interposition and disaggregation techniques~\cite{fingler2022dgsf,duato2010rcuda} allow for multiplexing memory and compute similar to our design, thanks to the level of control gained by replacing the GPU driver, and are black-box because they do not replace the actual application.


\begin{comment}
\mhead{Memory Multiplexing}
Limitations caused by requiring driver interaction to control the device makes it a challenge to multiplex memory.
Virtualization approaches~\cite{yu2019automatic, hong2017gpu} must duplicate application state in host memory, and allocate/de-allocate all resources when switching between applications.
Kernel level schedulers~\cite{ng2023paella,pemberton2022kernel,strati2024orion} are able to manipulate and oversubscribe memory as any GPU application would, but violate the black-box principles we target.
Interposition and disaggregation techniques~\cite{duato2010rcuda,fingler2022dgsf} allow for true multiplexing similar to our design, thanks to their level of control equal to that of a kernel scheduler while not being part of the actual application.
\end{comment}
% \mhead{Usage}
% GPUs are throughput-oriented architectures, executing a single instruction across multiple threads (SIMT).
% They are designed to execute thousands of threads in parallel on large datasets coming from a single application.
% Not designed for switching between several small applications or smaller datasets.


\begin{comment}
% All of these techniques give total control to one function, or allows it to allocate physical memory on a device and leave insufficient memory for other functions to run.
% The only way to adjust resource imbalances is with removal and creation of new containers, a high-overhead scenario with many function cold-starts.
Applications cannot manipulate GPUs directly, they must use a manufacturer-provided driver for all operations.
GPU compute is launched by host programs using \emph{kernels} which execute a code block with given memory inputs and a number of parallel threads to use.
Multiple kernels can be launched concurrently, with the device handling scheduling of kernels and threads internally.
Kernels run until completion, with execution time varying widely depending on the number of threads, input size, and complexity of the code being run.
Nvidia introduced Unified Virtual Memory (UVM)~\cite{nvidia-uvm} to allow applications to both simplify memory management and be a workaround to insufficient device memory.
Traditionally, \funcname{cuMemAlloc} is used for device-only memory allocations, and the program manually moves data between the device and host.
% Programs allocate memory using \funcname{cuMemAllocManaged} instead of \funcname{cuMemAlloc}, and the device driver moves memory between the device and host in response to usage and memory pressure.
When using \funcname{cuMemAllocManaged}, the same memory pointer can be used on both host and device, and the driver is responsible for data migration and coherency.
This capability is also used by UVM to allow over-subscription of memory, as it will move data pages around on-demand, using host memory as \quotes{swap} space.
We integrate our control plane with UVM to actively oversubscribe memory, allowing many function containers to remain warm on a device without interference, dramatically reducing cold starts.
% Each device generation has a variable number of streaming multiprocessors (SMs) that can support blocks of threads at a time.
% Devices report utilzation in 
\end{comment}

% \subsection{GPU Scheduling}
